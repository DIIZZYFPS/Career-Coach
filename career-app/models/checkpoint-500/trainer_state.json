{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 4.467415730337079,
  "eval_steps": 500,
  "global_step": 500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.008988764044943821,
      "grad_norm": 5.867749214172363,
      "learning_rate": 0.0,
      "loss": 2.9327,
      "step": 1
    },
    {
      "epoch": 0.017977528089887642,
      "grad_norm": 6.089925289154053,
      "learning_rate": 4e-05,
      "loss": 2.8975,
      "step": 2
    },
    {
      "epoch": 0.02696629213483146,
      "grad_norm": 5.607311725616455,
      "learning_rate": 8e-05,
      "loss": 2.8929,
      "step": 3
    },
    {
      "epoch": 0.035955056179775284,
      "grad_norm": 3.70684552192688,
      "learning_rate": 0.00012,
      "loss": 2.4366,
      "step": 4
    },
    {
      "epoch": 0.0449438202247191,
      "grad_norm": 2.045083522796631,
      "learning_rate": 0.00016,
      "loss": 2.0695,
      "step": 5
    },
    {
      "epoch": 0.05393258426966292,
      "grad_norm": 1.2802430391311646,
      "learning_rate": 0.0002,
      "loss": 1.891,
      "step": 6
    },
    {
      "epoch": 0.06292134831460675,
      "grad_norm": 0.898371160030365,
      "learning_rate": 0.00019979899497487438,
      "loss": 1.6193,
      "step": 7
    },
    {
      "epoch": 0.07191011235955057,
      "grad_norm": 0.9284430146217346,
      "learning_rate": 0.00019959798994974876,
      "loss": 1.6732,
      "step": 8
    },
    {
      "epoch": 0.08089887640449438,
      "grad_norm": 0.856439471244812,
      "learning_rate": 0.00019939698492462313,
      "loss": 1.541,
      "step": 9
    },
    {
      "epoch": 0.0898876404494382,
      "grad_norm": 0.7945054769515991,
      "learning_rate": 0.0001991959798994975,
      "loss": 1.4773,
      "step": 10
    },
    {
      "epoch": 0.09887640449438202,
      "grad_norm": 0.7183399200439453,
      "learning_rate": 0.00019899497487437187,
      "loss": 1.4706,
      "step": 11
    },
    {
      "epoch": 0.10786516853932585,
      "grad_norm": 0.6312628984451294,
      "learning_rate": 0.00019879396984924622,
      "loss": 1.2916,
      "step": 12
    },
    {
      "epoch": 0.11685393258426967,
      "grad_norm": 0.582293689250946,
      "learning_rate": 0.00019859296482412062,
      "loss": 1.2124,
      "step": 13
    },
    {
      "epoch": 0.1258426966292135,
      "grad_norm": 0.5891973972320557,
      "learning_rate": 0.000198391959798995,
      "loss": 1.2031,
      "step": 14
    },
    {
      "epoch": 0.1348314606741573,
      "grad_norm": 0.4892641603946686,
      "learning_rate": 0.00019819095477386937,
      "loss": 1.2475,
      "step": 15
    },
    {
      "epoch": 0.14382022471910114,
      "grad_norm": 0.4891263544559479,
      "learning_rate": 0.0001979899497487437,
      "loss": 1.1212,
      "step": 16
    },
    {
      "epoch": 0.15280898876404495,
      "grad_norm": 0.5373388528823853,
      "learning_rate": 0.0001977889447236181,
      "loss": 1.075,
      "step": 17
    },
    {
      "epoch": 0.16179775280898875,
      "grad_norm": 0.49342024326324463,
      "learning_rate": 0.00019758793969849249,
      "loss": 1.0955,
      "step": 18
    },
    {
      "epoch": 0.1707865168539326,
      "grad_norm": 0.5118899345397949,
      "learning_rate": 0.00019738693467336683,
      "loss": 1.094,
      "step": 19
    },
    {
      "epoch": 0.1797752808988764,
      "grad_norm": 0.5248528122901917,
      "learning_rate": 0.0001971859296482412,
      "loss": 1.0609,
      "step": 20
    },
    {
      "epoch": 0.18876404494382024,
      "grad_norm": 0.46604856848716736,
      "learning_rate": 0.0001969849246231156,
      "loss": 1.0628,
      "step": 21
    },
    {
      "epoch": 0.19775280898876405,
      "grad_norm": 0.4173665940761566,
      "learning_rate": 0.00019678391959798995,
      "loss": 0.9104,
      "step": 22
    },
    {
      "epoch": 0.20674157303370785,
      "grad_norm": 0.42213624715805054,
      "learning_rate": 0.00019658291457286432,
      "loss": 1.0498,
      "step": 23
    },
    {
      "epoch": 0.2157303370786517,
      "grad_norm": 0.4096555709838867,
      "learning_rate": 0.0001963819095477387,
      "loss": 0.9594,
      "step": 24
    },
    {
      "epoch": 0.2247191011235955,
      "grad_norm": 0.377143919467926,
      "learning_rate": 0.0001961809045226131,
      "loss": 0.9617,
      "step": 25
    },
    {
      "epoch": 0.23370786516853934,
      "grad_norm": 0.40291813015937805,
      "learning_rate": 0.00019597989949748744,
      "loss": 0.9425,
      "step": 26
    },
    {
      "epoch": 0.24269662921348314,
      "grad_norm": 0.4336307942867279,
      "learning_rate": 0.00019577889447236181,
      "loss": 1.0204,
      "step": 27
    },
    {
      "epoch": 0.251685393258427,
      "grad_norm": 0.402761846780777,
      "learning_rate": 0.0001955778894472362,
      "loss": 0.9487,
      "step": 28
    },
    {
      "epoch": 0.2606741573033708,
      "grad_norm": 0.4199514091014862,
      "learning_rate": 0.00019537688442211056,
      "loss": 1.0548,
      "step": 29
    },
    {
      "epoch": 0.2696629213483146,
      "grad_norm": 0.37908244132995605,
      "learning_rate": 0.00019517587939698493,
      "loss": 0.9881,
      "step": 30
    },
    {
      "epoch": 0.2786516853932584,
      "grad_norm": 0.4199800193309784,
      "learning_rate": 0.0001949748743718593,
      "loss": 0.9603,
      "step": 31
    },
    {
      "epoch": 0.2876404494382023,
      "grad_norm": 0.3951322138309479,
      "learning_rate": 0.00019477386934673368,
      "loss": 0.9788,
      "step": 32
    },
    {
      "epoch": 0.2966292134831461,
      "grad_norm": 0.41350582242012024,
      "learning_rate": 0.00019457286432160805,
      "loss": 0.982,
      "step": 33
    },
    {
      "epoch": 0.3056179775280899,
      "grad_norm": 0.39367079734802246,
      "learning_rate": 0.00019437185929648243,
      "loss": 0.9235,
      "step": 34
    },
    {
      "epoch": 0.3146067415730337,
      "grad_norm": 0.40450218319892883,
      "learning_rate": 0.0001941708542713568,
      "loss": 0.9477,
      "step": 35
    },
    {
      "epoch": 0.3235955056179775,
      "grad_norm": 0.3980608880519867,
      "learning_rate": 0.00019396984924623117,
      "loss": 0.9309,
      "step": 36
    },
    {
      "epoch": 0.3325842696629214,
      "grad_norm": 0.3938213288784027,
      "learning_rate": 0.00019376884422110552,
      "loss": 0.9232,
      "step": 37
    },
    {
      "epoch": 0.3415730337078652,
      "grad_norm": 0.3818555176258087,
      "learning_rate": 0.00019356783919597992,
      "loss": 0.9612,
      "step": 38
    },
    {
      "epoch": 0.350561797752809,
      "grad_norm": 0.39738729596138,
      "learning_rate": 0.0001933668341708543,
      "loss": 0.9214,
      "step": 39
    },
    {
      "epoch": 0.3595505617977528,
      "grad_norm": 0.4112373888492584,
      "learning_rate": 0.00019316582914572864,
      "loss": 0.9443,
      "step": 40
    },
    {
      "epoch": 0.3685393258426966,
      "grad_norm": 0.44761043787002563,
      "learning_rate": 0.000192964824120603,
      "loss": 0.9779,
      "step": 41
    },
    {
      "epoch": 0.3775280898876405,
      "grad_norm": 0.3748815953731537,
      "learning_rate": 0.0001927638190954774,
      "loss": 0.9403,
      "step": 42
    },
    {
      "epoch": 0.3865168539325843,
      "grad_norm": 0.38828393816947937,
      "learning_rate": 0.00019256281407035178,
      "loss": 0.8938,
      "step": 43
    },
    {
      "epoch": 0.3955056179775281,
      "grad_norm": 0.3828636407852173,
      "learning_rate": 0.00019236180904522613,
      "loss": 0.9022,
      "step": 44
    },
    {
      "epoch": 0.4044943820224719,
      "grad_norm": 0.3481641709804535,
      "learning_rate": 0.0001921608040201005,
      "loss": 0.8908,
      "step": 45
    },
    {
      "epoch": 0.4134831460674157,
      "grad_norm": 0.3703015148639679,
      "learning_rate": 0.0001919597989949749,
      "loss": 0.9164,
      "step": 46
    },
    {
      "epoch": 0.42247191011235957,
      "grad_norm": 0.3773420751094818,
      "learning_rate": 0.00019175879396984925,
      "loss": 0.9058,
      "step": 47
    },
    {
      "epoch": 0.4314606741573034,
      "grad_norm": 0.38058748841285706,
      "learning_rate": 0.00019155778894472362,
      "loss": 1.0132,
      "step": 48
    },
    {
      "epoch": 0.4404494382022472,
      "grad_norm": 0.3675095736980438,
      "learning_rate": 0.000191356783919598,
      "loss": 0.9217,
      "step": 49
    },
    {
      "epoch": 0.449438202247191,
      "grad_norm": 0.35707536339759827,
      "learning_rate": 0.0001911557788944724,
      "loss": 0.8823,
      "step": 50
    },
    {
      "epoch": 0.4584269662921348,
      "grad_norm": 0.35422247648239136,
      "learning_rate": 0.00019095477386934674,
      "loss": 0.8278,
      "step": 51
    },
    {
      "epoch": 0.46741573033707867,
      "grad_norm": 0.36722618341445923,
      "learning_rate": 0.0001907537688442211,
      "loss": 0.8718,
      "step": 52
    },
    {
      "epoch": 0.4764044943820225,
      "grad_norm": 0.38193291425704956,
      "learning_rate": 0.00019055276381909548,
      "loss": 0.9399,
      "step": 53
    },
    {
      "epoch": 0.4853932584269663,
      "grad_norm": 0.390215128660202,
      "learning_rate": 0.00019035175879396986,
      "loss": 0.921,
      "step": 54
    },
    {
      "epoch": 0.4943820224719101,
      "grad_norm": 0.3703664243221283,
      "learning_rate": 0.00019015075376884423,
      "loss": 0.9191,
      "step": 55
    },
    {
      "epoch": 0.503370786516854,
      "grad_norm": 0.431052029132843,
      "learning_rate": 0.0001899497487437186,
      "loss": 0.8556,
      "step": 56
    },
    {
      "epoch": 0.5123595505617977,
      "grad_norm": 0.37735891342163086,
      "learning_rate": 0.00018974874371859298,
      "loss": 0.9284,
      "step": 57
    },
    {
      "epoch": 0.5213483146067416,
      "grad_norm": 0.39499998092651367,
      "learning_rate": 0.00018954773869346732,
      "loss": 0.9808,
      "step": 58
    },
    {
      "epoch": 0.5303370786516854,
      "grad_norm": 0.41476577520370483,
      "learning_rate": 0.00018934673366834172,
      "loss": 0.9974,
      "step": 59
    },
    {
      "epoch": 0.5393258426966292,
      "grad_norm": 0.3912324905395508,
      "learning_rate": 0.0001891457286432161,
      "loss": 0.8562,
      "step": 60
    },
    {
      "epoch": 0.5483146067415731,
      "grad_norm": 0.3886553645133972,
      "learning_rate": 0.00018894472361809047,
      "loss": 0.9381,
      "step": 61
    },
    {
      "epoch": 0.5573033707865168,
      "grad_norm": 0.3735424876213074,
      "learning_rate": 0.00018874371859296481,
      "loss": 0.8479,
      "step": 62
    },
    {
      "epoch": 0.5662921348314607,
      "grad_norm": 0.3583531975746155,
      "learning_rate": 0.00018854271356783921,
      "loss": 0.9041,
      "step": 63
    },
    {
      "epoch": 0.5752808988764045,
      "grad_norm": 0.3895391523838043,
      "learning_rate": 0.0001883417085427136,
      "loss": 0.905,
      "step": 64
    },
    {
      "epoch": 0.5842696629213483,
      "grad_norm": 0.348271906375885,
      "learning_rate": 0.00018814070351758793,
      "loss": 0.8225,
      "step": 65
    },
    {
      "epoch": 0.5932584269662922,
      "grad_norm": 0.33687055110931396,
      "learning_rate": 0.0001879396984924623,
      "loss": 0.8673,
      "step": 66
    },
    {
      "epoch": 0.6022471910112359,
      "grad_norm": 0.34555262327194214,
      "learning_rate": 0.0001877386934673367,
      "loss": 0.7744,
      "step": 67
    },
    {
      "epoch": 0.6112359550561798,
      "grad_norm": 0.34469354152679443,
      "learning_rate": 0.00018753768844221108,
      "loss": 0.831,
      "step": 68
    },
    {
      "epoch": 0.6202247191011236,
      "grad_norm": 0.38777756690979004,
      "learning_rate": 0.00018733668341708543,
      "loss": 0.8816,
      "step": 69
    },
    {
      "epoch": 0.6292134831460674,
      "grad_norm": 0.39915797114372253,
      "learning_rate": 0.0001871356783919598,
      "loss": 0.903,
      "step": 70
    },
    {
      "epoch": 0.6382022471910113,
      "grad_norm": 0.415497362613678,
      "learning_rate": 0.0001869346733668342,
      "loss": 0.8148,
      "step": 71
    },
    {
      "epoch": 0.647191011235955,
      "grad_norm": 0.4267079532146454,
      "learning_rate": 0.00018673366834170854,
      "loss": 0.9171,
      "step": 72
    },
    {
      "epoch": 0.6561797752808989,
      "grad_norm": 0.38350701332092285,
      "learning_rate": 0.00018653266331658292,
      "loss": 0.9319,
      "step": 73
    },
    {
      "epoch": 0.6651685393258427,
      "grad_norm": 0.3482752740383148,
      "learning_rate": 0.0001863316582914573,
      "loss": 0.7812,
      "step": 74
    },
    {
      "epoch": 0.6741573033707865,
      "grad_norm": 0.37183454632759094,
      "learning_rate": 0.0001861306532663317,
      "loss": 0.8417,
      "step": 75
    },
    {
      "epoch": 0.6831460674157304,
      "grad_norm": 0.4105025827884674,
      "learning_rate": 0.00018592964824120604,
      "loss": 0.8403,
      "step": 76
    },
    {
      "epoch": 0.6921348314606741,
      "grad_norm": 0.3946227431297302,
      "learning_rate": 0.0001857286432160804,
      "loss": 0.8547,
      "step": 77
    },
    {
      "epoch": 0.701123595505618,
      "grad_norm": 0.4017118811607361,
      "learning_rate": 0.00018552763819095478,
      "loss": 0.8397,
      "step": 78
    },
    {
      "epoch": 0.7101123595505618,
      "grad_norm": 0.40974161028862,
      "learning_rate": 0.00018532663316582915,
      "loss": 0.8774,
      "step": 79
    },
    {
      "epoch": 0.7191011235955056,
      "grad_norm": 0.35914596915245056,
      "learning_rate": 0.00018512562814070353,
      "loss": 0.8316,
      "step": 80
    },
    {
      "epoch": 0.7280898876404495,
      "grad_norm": 0.3894263803958893,
      "learning_rate": 0.0001849246231155779,
      "loss": 0.8834,
      "step": 81
    },
    {
      "epoch": 0.7370786516853932,
      "grad_norm": 0.3998435139656067,
      "learning_rate": 0.00018472361809045227,
      "loss": 0.8497,
      "step": 82
    },
    {
      "epoch": 0.7460674157303371,
      "grad_norm": 0.3712238669395447,
      "learning_rate": 0.00018452261306532662,
      "loss": 0.8256,
      "step": 83
    },
    {
      "epoch": 0.755056179775281,
      "grad_norm": 0.37852635979652405,
      "learning_rate": 0.00018432160804020102,
      "loss": 0.8428,
      "step": 84
    },
    {
      "epoch": 0.7640449438202247,
      "grad_norm": 0.41039490699768066,
      "learning_rate": 0.0001841206030150754,
      "loss": 0.8384,
      "step": 85
    },
    {
      "epoch": 0.7730337078651686,
      "grad_norm": 0.36587080359458923,
      "learning_rate": 0.00018391959798994977,
      "loss": 0.805,
      "step": 86
    },
    {
      "epoch": 0.7820224719101123,
      "grad_norm": 0.3999593257904053,
      "learning_rate": 0.0001837185929648241,
      "loss": 0.8175,
      "step": 87
    },
    {
      "epoch": 0.7910112359550562,
      "grad_norm": 0.3999386429786682,
      "learning_rate": 0.0001835175879396985,
      "loss": 0.8624,
      "step": 88
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.3902212679386139,
      "learning_rate": 0.00018331658291457288,
      "loss": 0.7897,
      "step": 89
    },
    {
      "epoch": 0.8089887640449438,
      "grad_norm": 0.38353726267814636,
      "learning_rate": 0.00018311557788944723,
      "loss": 0.8559,
      "step": 90
    },
    {
      "epoch": 0.8179775280898877,
      "grad_norm": 0.42697617411613464,
      "learning_rate": 0.0001829145728643216,
      "loss": 0.8541,
      "step": 91
    },
    {
      "epoch": 0.8269662921348314,
      "grad_norm": 0.42537394165992737,
      "learning_rate": 0.000182713567839196,
      "loss": 0.8613,
      "step": 92
    },
    {
      "epoch": 0.8359550561797753,
      "grad_norm": 0.3900884687900543,
      "learning_rate": 0.00018251256281407038,
      "loss": 0.8603,
      "step": 93
    },
    {
      "epoch": 0.8449438202247191,
      "grad_norm": 0.40019869804382324,
      "learning_rate": 0.00018231155778894472,
      "loss": 0.8292,
      "step": 94
    },
    {
      "epoch": 0.8539325842696629,
      "grad_norm": 0.43005529046058655,
      "learning_rate": 0.0001821105527638191,
      "loss": 0.8261,
      "step": 95
    },
    {
      "epoch": 0.8629213483146068,
      "grad_norm": 0.3859888017177582,
      "learning_rate": 0.0001819095477386935,
      "loss": 0.7778,
      "step": 96
    },
    {
      "epoch": 0.8719101123595505,
      "grad_norm": 0.36568424105644226,
      "learning_rate": 0.00018170854271356784,
      "loss": 0.8256,
      "step": 97
    },
    {
      "epoch": 0.8808988764044944,
      "grad_norm": 0.39068710803985596,
      "learning_rate": 0.00018150753768844221,
      "loss": 0.8632,
      "step": 98
    },
    {
      "epoch": 0.8898876404494382,
      "grad_norm": 0.3732796609401703,
      "learning_rate": 0.0001813065326633166,
      "loss": 0.7997,
      "step": 99
    },
    {
      "epoch": 0.898876404494382,
      "grad_norm": 0.40858006477355957,
      "learning_rate": 0.00018110552763819096,
      "loss": 0.8876,
      "step": 100
    },
    {
      "epoch": 0.9078651685393259,
      "grad_norm": 0.3990350663661957,
      "learning_rate": 0.00018090452261306533,
      "loss": 0.8671,
      "step": 101
    },
    {
      "epoch": 0.9168539325842696,
      "grad_norm": 0.36895596981048584,
      "learning_rate": 0.0001807035175879397,
      "loss": 0.8447,
      "step": 102
    },
    {
      "epoch": 0.9258426966292135,
      "grad_norm": 0.39416226744651794,
      "learning_rate": 0.00018050251256281408,
      "loss": 0.8825,
      "step": 103
    },
    {
      "epoch": 0.9348314606741573,
      "grad_norm": 0.38766059279441833,
      "learning_rate": 0.00018030150753768845,
      "loss": 0.8204,
      "step": 104
    },
    {
      "epoch": 0.9438202247191011,
      "grad_norm": 0.42018699645996094,
      "learning_rate": 0.00018010050251256282,
      "loss": 0.8231,
      "step": 105
    },
    {
      "epoch": 0.952808988764045,
      "grad_norm": 0.39098498225212097,
      "learning_rate": 0.0001798994974874372,
      "loss": 0.8061,
      "step": 106
    },
    {
      "epoch": 0.9617977528089887,
      "grad_norm": 0.36769038438796997,
      "learning_rate": 0.00017969849246231157,
      "loss": 0.7754,
      "step": 107
    },
    {
      "epoch": 0.9707865168539326,
      "grad_norm": 0.4232609272003174,
      "learning_rate": 0.00017949748743718592,
      "loss": 0.8302,
      "step": 108
    },
    {
      "epoch": 0.9797752808988764,
      "grad_norm": 0.3972019851207733,
      "learning_rate": 0.00017929648241206032,
      "loss": 0.7942,
      "step": 109
    },
    {
      "epoch": 0.9887640449438202,
      "grad_norm": 0.39545804262161255,
      "learning_rate": 0.0001790954773869347,
      "loss": 0.8044,
      "step": 110
    },
    {
      "epoch": 0.9977528089887641,
      "grad_norm": 0.417758971452713,
      "learning_rate": 0.00017889447236180906,
      "loss": 0.8142,
      "step": 111
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.7638589143753052,
      "learning_rate": 0.0001786934673366834,
      "loss": 0.8826,
      "step": 112
    },
    {
      "epoch": 1.0089887640449438,
      "grad_norm": 0.3843201994895935,
      "learning_rate": 0.0001784924623115578,
      "loss": 0.7894,
      "step": 113
    },
    {
      "epoch": 1.0179775280898877,
      "grad_norm": 0.3837498724460602,
      "learning_rate": 0.00017829145728643218,
      "loss": 0.7617,
      "step": 114
    },
    {
      "epoch": 1.0269662921348315,
      "grad_norm": 0.3774866759777069,
      "learning_rate": 0.00017809045226130653,
      "loss": 0.7437,
      "step": 115
    },
    {
      "epoch": 1.0359550561797752,
      "grad_norm": 0.39342543482780457,
      "learning_rate": 0.0001778894472361809,
      "loss": 0.6878,
      "step": 116
    },
    {
      "epoch": 1.0449438202247192,
      "grad_norm": 0.40413787961006165,
      "learning_rate": 0.0001776884422110553,
      "loss": 0.714,
      "step": 117
    },
    {
      "epoch": 1.053932584269663,
      "grad_norm": 0.43508851528167725,
      "learning_rate": 0.00017748743718592967,
      "loss": 0.7655,
      "step": 118
    },
    {
      "epoch": 1.0629213483146067,
      "grad_norm": 0.42643389105796814,
      "learning_rate": 0.00017728643216080402,
      "loss": 0.7436,
      "step": 119
    },
    {
      "epoch": 1.0719101123595505,
      "grad_norm": 0.42035067081451416,
      "learning_rate": 0.0001770854271356784,
      "loss": 0.7224,
      "step": 120
    },
    {
      "epoch": 1.0808988764044944,
      "grad_norm": 0.4430274963378906,
      "learning_rate": 0.0001768844221105528,
      "loss": 0.7331,
      "step": 121
    },
    {
      "epoch": 1.0898876404494382,
      "grad_norm": 0.46041616797447205,
      "learning_rate": 0.00017668341708542714,
      "loss": 0.7634,
      "step": 122
    },
    {
      "epoch": 1.098876404494382,
      "grad_norm": 0.44022336602211,
      "learning_rate": 0.0001764824120603015,
      "loss": 0.7226,
      "step": 123
    },
    {
      "epoch": 1.107865168539326,
      "grad_norm": 0.4280224144458771,
      "learning_rate": 0.00017628140703517588,
      "loss": 0.6994,
      "step": 124
    },
    {
      "epoch": 1.1168539325842697,
      "grad_norm": 0.4560234546661377,
      "learning_rate": 0.00017608040201005026,
      "loss": 0.6701,
      "step": 125
    },
    {
      "epoch": 1.1258426966292134,
      "grad_norm": 0.43843650817871094,
      "learning_rate": 0.00017587939698492463,
      "loss": 0.7225,
      "step": 126
    },
    {
      "epoch": 1.1348314606741572,
      "grad_norm": 0.4603751003742218,
      "learning_rate": 0.000175678391959799,
      "loss": 0.6977,
      "step": 127
    },
    {
      "epoch": 1.1438202247191012,
      "grad_norm": 0.488694965839386,
      "learning_rate": 0.00017547738693467338,
      "loss": 0.7399,
      "step": 128
    },
    {
      "epoch": 1.152808988764045,
      "grad_norm": 0.5058482885360718,
      "learning_rate": 0.00017527638190954775,
      "loss": 0.822,
      "step": 129
    },
    {
      "epoch": 1.1617977528089887,
      "grad_norm": 0.42756450176239014,
      "learning_rate": 0.00017507537688442212,
      "loss": 0.6806,
      "step": 130
    },
    {
      "epoch": 1.1707865168539326,
      "grad_norm": 0.4557887315750122,
      "learning_rate": 0.0001748743718592965,
      "loss": 0.7495,
      "step": 131
    },
    {
      "epoch": 1.1797752808988764,
      "grad_norm": 0.4222213625907898,
      "learning_rate": 0.00017467336683417087,
      "loss": 0.7032,
      "step": 132
    },
    {
      "epoch": 1.1887640449438202,
      "grad_norm": 0.4623889923095703,
      "learning_rate": 0.00017447236180904521,
      "loss": 0.7604,
      "step": 133
    },
    {
      "epoch": 1.1977528089887641,
      "grad_norm": 0.46354904770851135,
      "learning_rate": 0.00017427135678391961,
      "loss": 0.7059,
      "step": 134
    },
    {
      "epoch": 1.2067415730337079,
      "grad_norm": 0.4317735731601715,
      "learning_rate": 0.000174070351758794,
      "loss": 0.684,
      "step": 135
    },
    {
      "epoch": 1.2157303370786516,
      "grad_norm": 0.45820531249046326,
      "learning_rate": 0.00017386934673366836,
      "loss": 0.751,
      "step": 136
    },
    {
      "epoch": 1.2247191011235956,
      "grad_norm": 0.44425931572914124,
      "learning_rate": 0.0001736683417085427,
      "loss": 0.6861,
      "step": 137
    },
    {
      "epoch": 1.2337078651685394,
      "grad_norm": 0.48116546869277954,
      "learning_rate": 0.0001734673366834171,
      "loss": 0.7022,
      "step": 138
    },
    {
      "epoch": 1.2426966292134831,
      "grad_norm": 0.4522624909877777,
      "learning_rate": 0.00017326633165829148,
      "loss": 0.7369,
      "step": 139
    },
    {
      "epoch": 1.2516853932584269,
      "grad_norm": 0.4743351638317108,
      "learning_rate": 0.00017306532663316582,
      "loss": 0.6894,
      "step": 140
    },
    {
      "epoch": 1.2606741573033708,
      "grad_norm": 0.47365182638168335,
      "learning_rate": 0.0001728643216080402,
      "loss": 0.6986,
      "step": 141
    },
    {
      "epoch": 1.2696629213483146,
      "grad_norm": 0.4866678714752197,
      "learning_rate": 0.0001726633165829146,
      "loss": 0.6603,
      "step": 142
    },
    {
      "epoch": 1.2786516853932584,
      "grad_norm": 0.4980037808418274,
      "learning_rate": 0.00017246231155778897,
      "loss": 0.6927,
      "step": 143
    },
    {
      "epoch": 1.2876404494382023,
      "grad_norm": 0.5299847722053528,
      "learning_rate": 0.00017226130653266332,
      "loss": 0.6806,
      "step": 144
    },
    {
      "epoch": 1.296629213483146,
      "grad_norm": 0.4967336058616638,
      "learning_rate": 0.0001720603015075377,
      "loss": 0.7083,
      "step": 145
    },
    {
      "epoch": 1.3056179775280898,
      "grad_norm": 0.4745408296585083,
      "learning_rate": 0.00017185929648241206,
      "loss": 0.6671,
      "step": 146
    },
    {
      "epoch": 1.3146067415730336,
      "grad_norm": 0.5049158334732056,
      "learning_rate": 0.00017165829145728644,
      "loss": 0.718,
      "step": 147
    },
    {
      "epoch": 1.3235955056179776,
      "grad_norm": 0.4823002517223358,
      "learning_rate": 0.0001714572864321608,
      "loss": 0.7348,
      "step": 148
    },
    {
      "epoch": 1.3325842696629213,
      "grad_norm": 0.48722559213638306,
      "learning_rate": 0.00017125628140703518,
      "loss": 0.7277,
      "step": 149
    },
    {
      "epoch": 1.3415730337078653,
      "grad_norm": 0.45071959495544434,
      "learning_rate": 0.00017105527638190955,
      "loss": 0.6373,
      "step": 150
    },
    {
      "epoch": 1.350561797752809,
      "grad_norm": 0.4802282452583313,
      "learning_rate": 0.00017085427135678393,
      "loss": 0.6944,
      "step": 151
    },
    {
      "epoch": 1.3595505617977528,
      "grad_norm": 0.47302353382110596,
      "learning_rate": 0.0001706532663316583,
      "loss": 0.6844,
      "step": 152
    },
    {
      "epoch": 1.3685393258426966,
      "grad_norm": 0.5128833651542664,
      "learning_rate": 0.00017045226130653267,
      "loss": 0.7695,
      "step": 153
    },
    {
      "epoch": 1.3775280898876405,
      "grad_norm": 0.4940306842327118,
      "learning_rate": 0.00017025125628140705,
      "loss": 0.6969,
      "step": 154
    },
    {
      "epoch": 1.3865168539325843,
      "grad_norm": 0.4941319227218628,
      "learning_rate": 0.00017005025125628142,
      "loss": 0.7141,
      "step": 155
    },
    {
      "epoch": 1.395505617977528,
      "grad_norm": 0.4479774236679077,
      "learning_rate": 0.0001698492462311558,
      "loss": 0.5948,
      "step": 156
    },
    {
      "epoch": 1.404494382022472,
      "grad_norm": 0.4846738576889038,
      "learning_rate": 0.00016964824120603016,
      "loss": 0.6525,
      "step": 157
    },
    {
      "epoch": 1.4134831460674158,
      "grad_norm": 0.5554487705230713,
      "learning_rate": 0.0001694472361809045,
      "loss": 0.7579,
      "step": 158
    },
    {
      "epoch": 1.4224719101123595,
      "grad_norm": 0.5085360407829285,
      "learning_rate": 0.0001692462311557789,
      "loss": 0.7527,
      "step": 159
    },
    {
      "epoch": 1.4314606741573033,
      "grad_norm": 0.5170609951019287,
      "learning_rate": 0.00016904522613065328,
      "loss": 0.7776,
      "step": 160
    },
    {
      "epoch": 1.4404494382022472,
      "grad_norm": 0.5048285722732544,
      "learning_rate": 0.00016884422110552766,
      "loss": 0.7281,
      "step": 161
    },
    {
      "epoch": 1.449438202247191,
      "grad_norm": 0.507969081401825,
      "learning_rate": 0.000168643216080402,
      "loss": 0.6509,
      "step": 162
    },
    {
      "epoch": 1.4584269662921348,
      "grad_norm": 0.5137774348258972,
      "learning_rate": 0.0001684422110552764,
      "loss": 0.766,
      "step": 163
    },
    {
      "epoch": 1.4674157303370787,
      "grad_norm": 0.5057291388511658,
      "learning_rate": 0.00016824120603015078,
      "loss": 0.6835,
      "step": 164
    },
    {
      "epoch": 1.4764044943820225,
      "grad_norm": 0.4965907037258148,
      "learning_rate": 0.00016804020100502512,
      "loss": 0.6177,
      "step": 165
    },
    {
      "epoch": 1.4853932584269662,
      "grad_norm": 0.5021895170211792,
      "learning_rate": 0.0001678391959798995,
      "loss": 0.6733,
      "step": 166
    },
    {
      "epoch": 1.49438202247191,
      "grad_norm": 0.5060408711433411,
      "learning_rate": 0.0001676381909547739,
      "loss": 0.5612,
      "step": 167
    },
    {
      "epoch": 1.503370786516854,
      "grad_norm": 0.5448092818260193,
      "learning_rate": 0.00016743718592964827,
      "loss": 0.6875,
      "step": 168
    },
    {
      "epoch": 1.5123595505617977,
      "grad_norm": 0.5072811245918274,
      "learning_rate": 0.0001672361809045226,
      "loss": 0.6463,
      "step": 169
    },
    {
      "epoch": 1.5213483146067417,
      "grad_norm": 0.518211305141449,
      "learning_rate": 0.00016703517587939699,
      "loss": 0.6449,
      "step": 170
    },
    {
      "epoch": 1.5303370786516854,
      "grad_norm": 0.4931667745113373,
      "learning_rate": 0.00016683417085427136,
      "loss": 0.6912,
      "step": 171
    },
    {
      "epoch": 1.5393258426966292,
      "grad_norm": 0.49399125576019287,
      "learning_rate": 0.00016663316582914573,
      "loss": 0.7422,
      "step": 172
    },
    {
      "epoch": 1.548314606741573,
      "grad_norm": 0.4899473488330841,
      "learning_rate": 0.0001664321608040201,
      "loss": 0.8077,
      "step": 173
    },
    {
      "epoch": 1.5573033707865167,
      "grad_norm": 0.5195055603981018,
      "learning_rate": 0.00016623115577889448,
      "loss": 0.6577,
      "step": 174
    },
    {
      "epoch": 1.5662921348314607,
      "grad_norm": 0.5048450231552124,
      "learning_rate": 0.00016603015075376885,
      "loss": 0.687,
      "step": 175
    },
    {
      "epoch": 1.5752808988764047,
      "grad_norm": 0.48674046993255615,
      "learning_rate": 0.00016582914572864322,
      "loss": 0.7065,
      "step": 176
    },
    {
      "epoch": 1.5842696629213484,
      "grad_norm": 0.5055518746376038,
      "learning_rate": 0.0001656281407035176,
      "loss": 0.7426,
      "step": 177
    },
    {
      "epoch": 1.5932584269662922,
      "grad_norm": 0.4986704885959625,
      "learning_rate": 0.00016542713567839197,
      "loss": 0.6988,
      "step": 178
    },
    {
      "epoch": 1.602247191011236,
      "grad_norm": 0.5376096963882446,
      "learning_rate": 0.00016522613065326634,
      "loss": 0.6533,
      "step": 179
    },
    {
      "epoch": 1.6112359550561797,
      "grad_norm": 0.48395565152168274,
      "learning_rate": 0.00016502512562814072,
      "loss": 0.6976,
      "step": 180
    },
    {
      "epoch": 1.6202247191011236,
      "grad_norm": 0.5291516780853271,
      "learning_rate": 0.0001648241206030151,
      "loss": 0.7121,
      "step": 181
    },
    {
      "epoch": 1.6292134831460674,
      "grad_norm": 0.4831722378730774,
      "learning_rate": 0.00016462311557788946,
      "loss": 0.7012,
      "step": 182
    },
    {
      "epoch": 1.6382022471910114,
      "grad_norm": 0.5175377130508423,
      "learning_rate": 0.0001644221105527638,
      "loss": 0.7017,
      "step": 183
    },
    {
      "epoch": 1.6471910112359551,
      "grad_norm": 0.5111058950424194,
      "learning_rate": 0.0001642211055276382,
      "loss": 0.704,
      "step": 184
    },
    {
      "epoch": 1.6561797752808989,
      "grad_norm": 0.4826604127883911,
      "learning_rate": 0.00016402010050251258,
      "loss": 0.6078,
      "step": 185
    },
    {
      "epoch": 1.6651685393258426,
      "grad_norm": 0.5259034633636475,
      "learning_rate": 0.00016381909547738695,
      "loss": 0.7103,
      "step": 186
    },
    {
      "epoch": 1.6741573033707864,
      "grad_norm": 0.5101408362388611,
      "learning_rate": 0.0001636180904522613,
      "loss": 0.7293,
      "step": 187
    },
    {
      "epoch": 1.6831460674157304,
      "grad_norm": 0.5450435280799866,
      "learning_rate": 0.0001634170854271357,
      "loss": 0.5243,
      "step": 188
    },
    {
      "epoch": 1.6921348314606741,
      "grad_norm": 0.5203365087509155,
      "learning_rate": 0.00016321608040201007,
      "loss": 0.6445,
      "step": 189
    },
    {
      "epoch": 1.701123595505618,
      "grad_norm": 0.527142345905304,
      "learning_rate": 0.00016301507537688442,
      "loss": 0.6936,
      "step": 190
    },
    {
      "epoch": 1.7101123595505618,
      "grad_norm": 0.49742498993873596,
      "learning_rate": 0.0001628140703517588,
      "loss": 0.6219,
      "step": 191
    },
    {
      "epoch": 1.7191011235955056,
      "grad_norm": 0.5218608379364014,
      "learning_rate": 0.00016261306532663316,
      "loss": 0.6898,
      "step": 192
    },
    {
      "epoch": 1.7280898876404494,
      "grad_norm": 0.5369536280632019,
      "learning_rate": 0.00016241206030150756,
      "loss": 0.6533,
      "step": 193
    },
    {
      "epoch": 1.737078651685393,
      "grad_norm": 0.5223860740661621,
      "learning_rate": 0.0001622110552763819,
      "loss": 0.6607,
      "step": 194
    },
    {
      "epoch": 1.746067415730337,
      "grad_norm": 0.5324874520301819,
      "learning_rate": 0.00016201005025125628,
      "loss": 0.7002,
      "step": 195
    },
    {
      "epoch": 1.755056179775281,
      "grad_norm": 0.5073167681694031,
      "learning_rate": 0.00016180904522613066,
      "loss": 0.7446,
      "step": 196
    },
    {
      "epoch": 1.7640449438202248,
      "grad_norm": 0.4847022294998169,
      "learning_rate": 0.00016160804020100503,
      "loss": 0.7033,
      "step": 197
    },
    {
      "epoch": 1.7730337078651686,
      "grad_norm": 0.4975110590457916,
      "learning_rate": 0.0001614070351758794,
      "loss": 0.705,
      "step": 198
    },
    {
      "epoch": 1.7820224719101123,
      "grad_norm": 0.49392595887184143,
      "learning_rate": 0.00016120603015075378,
      "loss": 0.7073,
      "step": 199
    },
    {
      "epoch": 1.791011235955056,
      "grad_norm": 0.4920112192630768,
      "learning_rate": 0.00016100502512562815,
      "loss": 0.644,
      "step": 200
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.5274104475975037,
      "learning_rate": 0.00016080402010050252,
      "loss": 0.6477,
      "step": 201
    },
    {
      "epoch": 1.8089887640449438,
      "grad_norm": 0.5122180581092834,
      "learning_rate": 0.0001606030150753769,
      "loss": 0.6666,
      "step": 202
    },
    {
      "epoch": 1.8179775280898878,
      "grad_norm": 0.44857776165008545,
      "learning_rate": 0.00016040201005025127,
      "loss": 0.5581,
      "step": 203
    },
    {
      "epoch": 1.8269662921348315,
      "grad_norm": 0.49341297149658203,
      "learning_rate": 0.00016020100502512564,
      "loss": 0.5232,
      "step": 204
    },
    {
      "epoch": 1.8359550561797753,
      "grad_norm": 0.5167354345321655,
      "learning_rate": 0.00016,
      "loss": 0.5907,
      "step": 205
    },
    {
      "epoch": 1.844943820224719,
      "grad_norm": 0.5707966685295105,
      "learning_rate": 0.00015979899497487439,
      "loss": 0.6816,
      "step": 206
    },
    {
      "epoch": 1.8539325842696628,
      "grad_norm": 0.5250276327133179,
      "learning_rate": 0.00015959798994974876,
      "loss": 0.6869,
      "step": 207
    },
    {
      "epoch": 1.8629213483146068,
      "grad_norm": 0.5712555050849915,
      "learning_rate": 0.0001593969849246231,
      "loss": 0.7121,
      "step": 208
    },
    {
      "epoch": 1.8719101123595505,
      "grad_norm": 0.550301730632782,
      "learning_rate": 0.0001591959798994975,
      "loss": 0.6475,
      "step": 209
    },
    {
      "epoch": 1.8808988764044945,
      "grad_norm": 0.4825449585914612,
      "learning_rate": 0.00015899497487437188,
      "loss": 0.6321,
      "step": 210
    },
    {
      "epoch": 1.8898876404494382,
      "grad_norm": 0.5767778158187866,
      "learning_rate": 0.00015879396984924625,
      "loss": 0.5749,
      "step": 211
    },
    {
      "epoch": 1.898876404494382,
      "grad_norm": 0.5126257538795471,
      "learning_rate": 0.0001585929648241206,
      "loss": 0.734,
      "step": 212
    },
    {
      "epoch": 1.9078651685393258,
      "grad_norm": 0.5008631944656372,
      "learning_rate": 0.000158391959798995,
      "loss": 0.6667,
      "step": 213
    },
    {
      "epoch": 1.9168539325842695,
      "grad_norm": 0.5043883323669434,
      "learning_rate": 0.00015819095477386937,
      "loss": 0.6787,
      "step": 214
    },
    {
      "epoch": 1.9258426966292135,
      "grad_norm": 0.4949117600917816,
      "learning_rate": 0.00015798994974874372,
      "loss": 0.6328,
      "step": 215
    },
    {
      "epoch": 1.9348314606741575,
      "grad_norm": 0.5707808136940002,
      "learning_rate": 0.0001577889447236181,
      "loss": 0.616,
      "step": 216
    },
    {
      "epoch": 1.9438202247191012,
      "grad_norm": 0.7143571972846985,
      "learning_rate": 0.00015758793969849246,
      "loss": 0.6714,
      "step": 217
    },
    {
      "epoch": 1.952808988764045,
      "grad_norm": 0.6252564191818237,
      "learning_rate": 0.00015738693467336686,
      "loss": 0.5917,
      "step": 218
    },
    {
      "epoch": 1.9617977528089887,
      "grad_norm": 0.5254539251327515,
      "learning_rate": 0.0001571859296482412,
      "loss": 0.6649,
      "step": 219
    },
    {
      "epoch": 1.9707865168539325,
      "grad_norm": 0.5377323627471924,
      "learning_rate": 0.00015698492462311558,
      "loss": 0.6644,
      "step": 220
    },
    {
      "epoch": 1.9797752808988764,
      "grad_norm": 0.5130908489227295,
      "learning_rate": 0.00015678391959798995,
      "loss": 0.5696,
      "step": 221
    },
    {
      "epoch": 1.9887640449438202,
      "grad_norm": 0.5565310716629028,
      "learning_rate": 0.00015658291457286433,
      "loss": 0.5736,
      "step": 222
    },
    {
      "epoch": 1.9977528089887642,
      "grad_norm": 0.4949910044670105,
      "learning_rate": 0.0001563819095477387,
      "loss": 0.6281,
      "step": 223
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.0195002555847168,
      "learning_rate": 0.00015618090452261307,
      "loss": 0.5102,
      "step": 224
    },
    {
      "epoch": 2.0089887640449438,
      "grad_norm": 0.4660240113735199,
      "learning_rate": 0.00015597989949748745,
      "loss": 0.5222,
      "step": 225
    },
    {
      "epoch": 2.0179775280898875,
      "grad_norm": 0.5553634762763977,
      "learning_rate": 0.00015577889447236182,
      "loss": 0.4074,
      "step": 226
    },
    {
      "epoch": 2.0269662921348313,
      "grad_norm": 0.5150858163833618,
      "learning_rate": 0.0001555778894472362,
      "loss": 0.5437,
      "step": 227
    },
    {
      "epoch": 2.0359550561797755,
      "grad_norm": 0.5600498914718628,
      "learning_rate": 0.00015537688442211056,
      "loss": 0.521,
      "step": 228
    },
    {
      "epoch": 2.044943820224719,
      "grad_norm": 0.5323230028152466,
      "learning_rate": 0.00015517587939698494,
      "loss": 0.5334,
      "step": 229
    },
    {
      "epoch": 2.053932584269663,
      "grad_norm": 0.5573216080665588,
      "learning_rate": 0.0001549748743718593,
      "loss": 0.5199,
      "step": 230
    },
    {
      "epoch": 2.0629213483146067,
      "grad_norm": 0.6677011847496033,
      "learning_rate": 0.00015477386934673368,
      "loss": 0.5506,
      "step": 231
    },
    {
      "epoch": 2.0719101123595505,
      "grad_norm": 0.6032688617706299,
      "learning_rate": 0.00015457286432160806,
      "loss": 0.6113,
      "step": 232
    },
    {
      "epoch": 2.0808988764044942,
      "grad_norm": 0.7408851981163025,
      "learning_rate": 0.0001543718592964824,
      "loss": 0.4487,
      "step": 233
    },
    {
      "epoch": 2.0898876404494384,
      "grad_norm": 0.8626886010169983,
      "learning_rate": 0.0001541708542713568,
      "loss": 0.6083,
      "step": 234
    },
    {
      "epoch": 2.098876404494382,
      "grad_norm": 0.6213219165802002,
      "learning_rate": 0.00015396984924623117,
      "loss": 0.5407,
      "step": 235
    },
    {
      "epoch": 2.107865168539326,
      "grad_norm": 0.6023510098457336,
      "learning_rate": 0.00015376884422110555,
      "loss": 0.5464,
      "step": 236
    },
    {
      "epoch": 2.1168539325842697,
      "grad_norm": 0.5395693778991699,
      "learning_rate": 0.0001535678391959799,
      "loss": 0.4975,
      "step": 237
    },
    {
      "epoch": 2.1258426966292134,
      "grad_norm": 0.5109840631484985,
      "learning_rate": 0.00015336683417085427,
      "loss": 0.4496,
      "step": 238
    },
    {
      "epoch": 2.134831460674157,
      "grad_norm": 0.5461961627006531,
      "learning_rate": 0.00015316582914572867,
      "loss": 0.4442,
      "step": 239
    },
    {
      "epoch": 2.143820224719101,
      "grad_norm": 0.5748690366744995,
      "learning_rate": 0.000152964824120603,
      "loss": 0.5014,
      "step": 240
    },
    {
      "epoch": 2.152808988764045,
      "grad_norm": 0.5407891273498535,
      "learning_rate": 0.00015276381909547739,
      "loss": 0.4825,
      "step": 241
    },
    {
      "epoch": 2.161797752808989,
      "grad_norm": 0.5855742692947388,
      "learning_rate": 0.00015256281407035176,
      "loss": 0.4442,
      "step": 242
    },
    {
      "epoch": 2.1707865168539326,
      "grad_norm": 0.5812053084373474,
      "learning_rate": 0.00015236180904522613,
      "loss": 0.5011,
      "step": 243
    },
    {
      "epoch": 2.1797752808988764,
      "grad_norm": 0.6219434142112732,
      "learning_rate": 0.0001521608040201005,
      "loss": 0.5654,
      "step": 244
    },
    {
      "epoch": 2.18876404494382,
      "grad_norm": 0.6690217852592468,
      "learning_rate": 0.00015195979899497488,
      "loss": 0.5959,
      "step": 245
    },
    {
      "epoch": 2.197752808988764,
      "grad_norm": 0.615625262260437,
      "learning_rate": 0.00015175879396984925,
      "loss": 0.5846,
      "step": 246
    },
    {
      "epoch": 2.2067415730337077,
      "grad_norm": 0.5975291728973389,
      "learning_rate": 0.00015155778894472362,
      "loss": 0.4663,
      "step": 247
    },
    {
      "epoch": 2.215730337078652,
      "grad_norm": 0.5664569139480591,
      "learning_rate": 0.000151356783919598,
      "loss": 0.4738,
      "step": 248
    },
    {
      "epoch": 2.2247191011235956,
      "grad_norm": 0.6067225933074951,
      "learning_rate": 0.00015115577889447237,
      "loss": 0.545,
      "step": 249
    },
    {
      "epoch": 2.2337078651685394,
      "grad_norm": 0.6892182230949402,
      "learning_rate": 0.00015095477386934674,
      "loss": 0.4804,
      "step": 250
    },
    {
      "epoch": 2.242696629213483,
      "grad_norm": 0.654926061630249,
      "learning_rate": 0.00015075376884422112,
      "loss": 0.4978,
      "step": 251
    },
    {
      "epoch": 2.251685393258427,
      "grad_norm": 0.5852486491203308,
      "learning_rate": 0.0001505527638190955,
      "loss": 0.5754,
      "step": 252
    },
    {
      "epoch": 2.2606741573033706,
      "grad_norm": 0.5744386315345764,
      "learning_rate": 0.00015035175879396986,
      "loss": 0.4953,
      "step": 253
    },
    {
      "epoch": 2.2696629213483144,
      "grad_norm": 0.5505176186561584,
      "learning_rate": 0.00015015075376884423,
      "loss": 0.4115,
      "step": 254
    },
    {
      "epoch": 2.2786516853932586,
      "grad_norm": 0.6617196202278137,
      "learning_rate": 0.0001499497487437186,
      "loss": 0.4522,
      "step": 255
    },
    {
      "epoch": 2.2876404494382023,
      "grad_norm": 0.5835797190666199,
      "learning_rate": 0.00014974874371859298,
      "loss": 0.5154,
      "step": 256
    },
    {
      "epoch": 2.296629213483146,
      "grad_norm": 0.629218578338623,
      "learning_rate": 0.00014954773869346735,
      "loss": 0.5049,
      "step": 257
    },
    {
      "epoch": 2.30561797752809,
      "grad_norm": 0.6005010008811951,
      "learning_rate": 0.0001493467336683417,
      "loss": 0.515,
      "step": 258
    },
    {
      "epoch": 2.3146067415730336,
      "grad_norm": 0.7216350436210632,
      "learning_rate": 0.0001491457286432161,
      "loss": 0.6126,
      "step": 259
    },
    {
      "epoch": 2.3235955056179773,
      "grad_norm": 0.6677060723304749,
      "learning_rate": 0.00014894472361809047,
      "loss": 0.5323,
      "step": 260
    },
    {
      "epoch": 2.3325842696629215,
      "grad_norm": 0.5940092206001282,
      "learning_rate": 0.00014874371859296482,
      "loss": 0.4089,
      "step": 261
    },
    {
      "epoch": 2.3415730337078653,
      "grad_norm": 0.599693238735199,
      "learning_rate": 0.0001485427135678392,
      "loss": 0.5056,
      "step": 262
    },
    {
      "epoch": 2.350561797752809,
      "grad_norm": 0.6471900343894958,
      "learning_rate": 0.00014834170854271356,
      "loss": 0.56,
      "step": 263
    },
    {
      "epoch": 2.359550561797753,
      "grad_norm": 0.6030563116073608,
      "learning_rate": 0.00014814070351758796,
      "loss": 0.5402,
      "step": 264
    },
    {
      "epoch": 2.3685393258426966,
      "grad_norm": 0.6223505735397339,
      "learning_rate": 0.0001479396984924623,
      "loss": 0.4698,
      "step": 265
    },
    {
      "epoch": 2.3775280898876403,
      "grad_norm": 0.6378539800643921,
      "learning_rate": 0.00014773869346733668,
      "loss": 0.619,
      "step": 266
    },
    {
      "epoch": 2.3865168539325845,
      "grad_norm": 0.6360874176025391,
      "learning_rate": 0.00014753768844221106,
      "loss": 0.478,
      "step": 267
    },
    {
      "epoch": 2.3955056179775283,
      "grad_norm": 0.6217352747917175,
      "learning_rate": 0.00014733668341708543,
      "loss": 0.541,
      "step": 268
    },
    {
      "epoch": 2.404494382022472,
      "grad_norm": 0.587964653968811,
      "learning_rate": 0.0001471356783919598,
      "loss": 0.5041,
      "step": 269
    },
    {
      "epoch": 2.4134831460674158,
      "grad_norm": 0.5437304377555847,
      "learning_rate": 0.00014693467336683417,
      "loss": 0.4398,
      "step": 270
    },
    {
      "epoch": 2.4224719101123595,
      "grad_norm": 0.6016120910644531,
      "learning_rate": 0.00014673366834170855,
      "loss": 0.4713,
      "step": 271
    },
    {
      "epoch": 2.4314606741573033,
      "grad_norm": 0.6375266909599304,
      "learning_rate": 0.00014653266331658292,
      "loss": 0.4592,
      "step": 272
    },
    {
      "epoch": 2.440449438202247,
      "grad_norm": 0.6722357273101807,
      "learning_rate": 0.0001463316582914573,
      "loss": 0.5549,
      "step": 273
    },
    {
      "epoch": 2.449438202247191,
      "grad_norm": 0.6131983399391174,
      "learning_rate": 0.00014613065326633167,
      "loss": 0.567,
      "step": 274
    },
    {
      "epoch": 2.458426966292135,
      "grad_norm": 0.6151455044746399,
      "learning_rate": 0.00014592964824120604,
      "loss": 0.5816,
      "step": 275
    },
    {
      "epoch": 2.4674157303370787,
      "grad_norm": 0.6787477135658264,
      "learning_rate": 0.0001457286432160804,
      "loss": 0.4874,
      "step": 276
    },
    {
      "epoch": 2.4764044943820225,
      "grad_norm": 0.5861737132072449,
      "learning_rate": 0.00014552763819095479,
      "loss": 0.515,
      "step": 277
    },
    {
      "epoch": 2.4853932584269662,
      "grad_norm": 0.6954892873764038,
      "learning_rate": 0.00014532663316582916,
      "loss": 0.4164,
      "step": 278
    },
    {
      "epoch": 2.49438202247191,
      "grad_norm": 0.5795653462409973,
      "learning_rate": 0.00014512562814070353,
      "loss": 0.3879,
      "step": 279
    },
    {
      "epoch": 2.5033707865168537,
      "grad_norm": 0.6603276133537292,
      "learning_rate": 0.0001449246231155779,
      "loss": 0.5487,
      "step": 280
    },
    {
      "epoch": 2.512359550561798,
      "grad_norm": 0.5875048637390137,
      "learning_rate": 0.00014472361809045228,
      "loss": 0.4917,
      "step": 281
    },
    {
      "epoch": 2.5213483146067417,
      "grad_norm": 0.6392641067504883,
      "learning_rate": 0.00014452261306532665,
      "loss": 0.568,
      "step": 282
    },
    {
      "epoch": 2.5303370786516854,
      "grad_norm": 0.6484583020210266,
      "learning_rate": 0.000144321608040201,
      "loss": 0.4412,
      "step": 283
    },
    {
      "epoch": 2.539325842696629,
      "grad_norm": 0.6449112892150879,
      "learning_rate": 0.00014412060301507537,
      "loss": 0.5526,
      "step": 284
    },
    {
      "epoch": 2.548314606741573,
      "grad_norm": 0.6698688864707947,
      "learning_rate": 0.00014391959798994977,
      "loss": 0.5832,
      "step": 285
    },
    {
      "epoch": 2.5573033707865167,
      "grad_norm": 0.6654890775680542,
      "learning_rate": 0.00014371859296482411,
      "loss": 0.4937,
      "step": 286
    },
    {
      "epoch": 2.5662921348314605,
      "grad_norm": 0.6368288397789001,
      "learning_rate": 0.0001435175879396985,
      "loss": 0.6144,
      "step": 287
    },
    {
      "epoch": 2.5752808988764047,
      "grad_norm": 0.5983182191848755,
      "learning_rate": 0.00014331658291457286,
      "loss": 0.5085,
      "step": 288
    },
    {
      "epoch": 2.5842696629213484,
      "grad_norm": 0.580386221408844,
      "learning_rate": 0.00014311557788944726,
      "loss": 0.4726,
      "step": 289
    },
    {
      "epoch": 2.593258426966292,
      "grad_norm": 0.7215433716773987,
      "learning_rate": 0.0001429145728643216,
      "loss": 0.4915,
      "step": 290
    },
    {
      "epoch": 2.602247191011236,
      "grad_norm": 0.5611022114753723,
      "learning_rate": 0.00014271356783919598,
      "loss": 0.4294,
      "step": 291
    },
    {
      "epoch": 2.6112359550561797,
      "grad_norm": 0.5867642760276794,
      "learning_rate": 0.00014251256281407035,
      "loss": 0.5665,
      "step": 292
    },
    {
      "epoch": 2.620224719101124,
      "grad_norm": 0.6344872713088989,
      "learning_rate": 0.00014231155778894473,
      "loss": 0.4829,
      "step": 293
    },
    {
      "epoch": 2.629213483146067,
      "grad_norm": 0.6000683903694153,
      "learning_rate": 0.0001421105527638191,
      "loss": 0.3992,
      "step": 294
    },
    {
      "epoch": 2.6382022471910114,
      "grad_norm": 0.6512176394462585,
      "learning_rate": 0.00014190954773869347,
      "loss": 0.5714,
      "step": 295
    },
    {
      "epoch": 2.647191011235955,
      "grad_norm": 0.7028009295463562,
      "learning_rate": 0.00014170854271356784,
      "loss": 0.5783,
      "step": 296
    },
    {
      "epoch": 2.656179775280899,
      "grad_norm": 0.6494923233985901,
      "learning_rate": 0.00014150753768844222,
      "loss": 0.5988,
      "step": 297
    },
    {
      "epoch": 2.6651685393258426,
      "grad_norm": 0.5978695750236511,
      "learning_rate": 0.0001413065326633166,
      "loss": 0.5985,
      "step": 298
    },
    {
      "epoch": 2.6741573033707864,
      "grad_norm": 0.5788876414299011,
      "learning_rate": 0.00014110552763819096,
      "loss": 0.4572,
      "step": 299
    },
    {
      "epoch": 2.6831460674157306,
      "grad_norm": 0.6566181778907776,
      "learning_rate": 0.00014090452261306534,
      "loss": 0.5156,
      "step": 300
    },
    {
      "epoch": 2.692134831460674,
      "grad_norm": 0.6133995056152344,
      "learning_rate": 0.0001407035175879397,
      "loss": 0.3775,
      "step": 301
    },
    {
      "epoch": 2.701123595505618,
      "grad_norm": 0.7124189734458923,
      "learning_rate": 0.00014050251256281408,
      "loss": 0.5253,
      "step": 302
    },
    {
      "epoch": 2.710112359550562,
      "grad_norm": 0.6259135603904724,
      "learning_rate": 0.00014030150753768846,
      "loss": 0.4731,
      "step": 303
    },
    {
      "epoch": 2.7191011235955056,
      "grad_norm": 0.623723030090332,
      "learning_rate": 0.0001401005025125628,
      "loss": 0.5035,
      "step": 304
    },
    {
      "epoch": 2.7280898876404494,
      "grad_norm": 0.6508894562721252,
      "learning_rate": 0.0001398994974874372,
      "loss": 0.4888,
      "step": 305
    },
    {
      "epoch": 2.737078651685393,
      "grad_norm": 0.8689988851547241,
      "learning_rate": 0.00013969849246231157,
      "loss": 0.5793,
      "step": 306
    },
    {
      "epoch": 2.7460674157303373,
      "grad_norm": 0.684623122215271,
      "learning_rate": 0.00013949748743718595,
      "loss": 0.4543,
      "step": 307
    },
    {
      "epoch": 2.755056179775281,
      "grad_norm": 0.6090950965881348,
      "learning_rate": 0.0001392964824120603,
      "loss": 0.5538,
      "step": 308
    },
    {
      "epoch": 2.764044943820225,
      "grad_norm": 0.5619356632232666,
      "learning_rate": 0.00013909547738693467,
      "loss": 0.4644,
      "step": 309
    },
    {
      "epoch": 2.7730337078651686,
      "grad_norm": 0.6681782603263855,
      "learning_rate": 0.00013889447236180907,
      "loss": 0.548,
      "step": 310
    },
    {
      "epoch": 2.7820224719101123,
      "grad_norm": 0.6274324059486389,
      "learning_rate": 0.0001386934673366834,
      "loss": 0.5981,
      "step": 311
    },
    {
      "epoch": 2.791011235955056,
      "grad_norm": 0.5922242403030396,
      "learning_rate": 0.00013849246231155778,
      "loss": 0.5413,
      "step": 312
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.6123243570327759,
      "learning_rate": 0.00013829145728643216,
      "loss": 0.5823,
      "step": 313
    },
    {
      "epoch": 2.808988764044944,
      "grad_norm": 0.5891364812850952,
      "learning_rate": 0.00013809045226130656,
      "loss": 0.5406,
      "step": 314
    },
    {
      "epoch": 2.8179775280898878,
      "grad_norm": 0.5824383497238159,
      "learning_rate": 0.0001378894472361809,
      "loss": 0.5467,
      "step": 315
    },
    {
      "epoch": 2.8269662921348315,
      "grad_norm": 0.6133068799972534,
      "learning_rate": 0.00013768844221105528,
      "loss": 0.4834,
      "step": 316
    },
    {
      "epoch": 2.8359550561797753,
      "grad_norm": 0.6003114581108093,
      "learning_rate": 0.00013748743718592965,
      "loss": 0.4825,
      "step": 317
    },
    {
      "epoch": 2.844943820224719,
      "grad_norm": 0.6441670656204224,
      "learning_rate": 0.00013728643216080402,
      "loss": 0.5443,
      "step": 318
    },
    {
      "epoch": 2.853932584269663,
      "grad_norm": 0.5923592448234558,
      "learning_rate": 0.0001370854271356784,
      "loss": 0.3885,
      "step": 319
    },
    {
      "epoch": 2.8629213483146065,
      "grad_norm": 0.6361895799636841,
      "learning_rate": 0.00013688442211055277,
      "loss": 0.5519,
      "step": 320
    },
    {
      "epoch": 2.8719101123595507,
      "grad_norm": 0.6282822489738464,
      "learning_rate": 0.00013668341708542714,
      "loss": 0.4749,
      "step": 321
    },
    {
      "epoch": 2.8808988764044945,
      "grad_norm": 0.6911002993583679,
      "learning_rate": 0.00013648241206030151,
      "loss": 0.5507,
      "step": 322
    },
    {
      "epoch": 2.8898876404494382,
      "grad_norm": 0.6519534587860107,
      "learning_rate": 0.0001362814070351759,
      "loss": 0.5251,
      "step": 323
    },
    {
      "epoch": 2.898876404494382,
      "grad_norm": 0.568443775177002,
      "learning_rate": 0.00013608040201005026,
      "loss": 0.4177,
      "step": 324
    },
    {
      "epoch": 2.9078651685393258,
      "grad_norm": 0.6429846882820129,
      "learning_rate": 0.00013587939698492463,
      "loss": 0.578,
      "step": 325
    },
    {
      "epoch": 2.9168539325842695,
      "grad_norm": 0.6556768417358398,
      "learning_rate": 0.000135678391959799,
      "loss": 0.5041,
      "step": 326
    },
    {
      "epoch": 2.9258426966292133,
      "grad_norm": 0.6875482797622681,
      "learning_rate": 0.00013547738693467338,
      "loss": 0.5296,
      "step": 327
    },
    {
      "epoch": 2.9348314606741575,
      "grad_norm": 0.6077708601951599,
      "learning_rate": 0.00013527638190954775,
      "loss": 0.4493,
      "step": 328
    },
    {
      "epoch": 2.943820224719101,
      "grad_norm": 0.6266283392906189,
      "learning_rate": 0.0001350753768844221,
      "loss": 0.5076,
      "step": 329
    },
    {
      "epoch": 2.952808988764045,
      "grad_norm": 0.5939067602157593,
      "learning_rate": 0.00013487437185929647,
      "loss": 0.4287,
      "step": 330
    },
    {
      "epoch": 2.9617977528089887,
      "grad_norm": 0.6024183630943298,
      "learning_rate": 0.00013467336683417087,
      "loss": 0.5725,
      "step": 331
    },
    {
      "epoch": 2.9707865168539325,
      "grad_norm": 0.6141772866249084,
      "learning_rate": 0.00013447236180904524,
      "loss": 0.6012,
      "step": 332
    },
    {
      "epoch": 2.9797752808988767,
      "grad_norm": 0.6132792830467224,
      "learning_rate": 0.0001342713567839196,
      "loss": 0.5303,
      "step": 333
    },
    {
      "epoch": 2.98876404494382,
      "grad_norm": 0.6210426092147827,
      "learning_rate": 0.00013407035175879396,
      "loss": 0.5591,
      "step": 334
    },
    {
      "epoch": 2.997752808988764,
      "grad_norm": 0.604884147644043,
      "learning_rate": 0.00013386934673366836,
      "loss": 0.4974,
      "step": 335
    },
    {
      "epoch": 3.0,
      "grad_norm": 1.146360993385315,
      "learning_rate": 0.0001336683417085427,
      "loss": 0.4953,
      "step": 336
    },
    {
      "epoch": 3.0089887640449438,
      "grad_norm": 0.5554021000862122,
      "learning_rate": 0.00013346733668341708,
      "loss": 0.3507,
      "step": 337
    },
    {
      "epoch": 3.0179775280898875,
      "grad_norm": 0.5963609218597412,
      "learning_rate": 0.00013326633165829146,
      "loss": 0.3894,
      "step": 338
    },
    {
      "epoch": 3.0269662921348313,
      "grad_norm": 0.6020979285240173,
      "learning_rate": 0.00013306532663316586,
      "loss": 0.3324,
      "step": 339
    },
    {
      "epoch": 3.0359550561797755,
      "grad_norm": 0.6343191862106323,
      "learning_rate": 0.0001328643216080402,
      "loss": 0.3224,
      "step": 340
    },
    {
      "epoch": 3.044943820224719,
      "grad_norm": 0.6305267214775085,
      "learning_rate": 0.00013266331658291457,
      "loss": 0.3868,
      "step": 341
    },
    {
      "epoch": 3.053932584269663,
      "grad_norm": 0.7155238389968872,
      "learning_rate": 0.00013246231155778895,
      "loss": 0.3228,
      "step": 342
    },
    {
      "epoch": 3.0629213483146067,
      "grad_norm": 0.7738904356956482,
      "learning_rate": 0.00013226130653266332,
      "loss": 0.3558,
      "step": 343
    },
    {
      "epoch": 3.0719101123595505,
      "grad_norm": 0.7337636351585388,
      "learning_rate": 0.0001320603015075377,
      "loss": 0.4103,
      "step": 344
    },
    {
      "epoch": 3.0808988764044942,
      "grad_norm": 0.638300359249115,
      "learning_rate": 0.00013185929648241207,
      "loss": 0.2855,
      "step": 345
    },
    {
      "epoch": 3.0898876404494384,
      "grad_norm": 0.7031756639480591,
      "learning_rate": 0.00013165829145728644,
      "loss": 0.2787,
      "step": 346
    },
    {
      "epoch": 3.098876404494382,
      "grad_norm": 0.6939449310302734,
      "learning_rate": 0.0001314572864321608,
      "loss": 0.3155,
      "step": 347
    },
    {
      "epoch": 3.107865168539326,
      "grad_norm": 0.7217143177986145,
      "learning_rate": 0.00013125628140703518,
      "loss": 0.3563,
      "step": 348
    },
    {
      "epoch": 3.1168539325842697,
      "grad_norm": 0.7956618666648865,
      "learning_rate": 0.00013105527638190956,
      "loss": 0.3679,
      "step": 349
    },
    {
      "epoch": 3.1258426966292134,
      "grad_norm": 0.7286489605903625,
      "learning_rate": 0.00013085427135678393,
      "loss": 0.3831,
      "step": 350
    },
    {
      "epoch": 3.134831460674157,
      "grad_norm": 0.8053686022758484,
      "learning_rate": 0.0001306532663316583,
      "loss": 0.3743,
      "step": 351
    },
    {
      "epoch": 3.143820224719101,
      "grad_norm": 0.7931649684906006,
      "learning_rate": 0.00013045226130653268,
      "loss": 0.4057,
      "step": 352
    },
    {
      "epoch": 3.152808988764045,
      "grad_norm": 0.6974821090698242,
      "learning_rate": 0.00013025125628140705,
      "loss": 0.3598,
      "step": 353
    },
    {
      "epoch": 3.161797752808989,
      "grad_norm": 0.7493677139282227,
      "learning_rate": 0.0001300502512562814,
      "loss": 0.3893,
      "step": 354
    },
    {
      "epoch": 3.1707865168539326,
      "grad_norm": 0.7561050653457642,
      "learning_rate": 0.00012984924623115577,
      "loss": 0.3687,
      "step": 355
    },
    {
      "epoch": 3.1797752808988764,
      "grad_norm": 0.7245007157325745,
      "learning_rate": 0.00012964824120603017,
      "loss": 0.3343,
      "step": 356
    },
    {
      "epoch": 3.18876404494382,
      "grad_norm": 0.6741995811462402,
      "learning_rate": 0.00012944723618090454,
      "loss": 0.3753,
      "step": 357
    },
    {
      "epoch": 3.197752808988764,
      "grad_norm": 0.7612851858139038,
      "learning_rate": 0.0001292462311557789,
      "loss": 0.3518,
      "step": 358
    },
    {
      "epoch": 3.2067415730337077,
      "grad_norm": 0.7604615688323975,
      "learning_rate": 0.00012904522613065326,
      "loss": 0.3425,
      "step": 359
    },
    {
      "epoch": 3.215730337078652,
      "grad_norm": 0.7310434579849243,
      "learning_rate": 0.00012884422110552766,
      "loss": 0.3441,
      "step": 360
    },
    {
      "epoch": 3.2247191011235956,
      "grad_norm": 0.7486586570739746,
      "learning_rate": 0.000128643216080402,
      "loss": 0.3041,
      "step": 361
    },
    {
      "epoch": 3.2337078651685394,
      "grad_norm": 0.7074959874153137,
      "learning_rate": 0.00012844221105527638,
      "loss": 0.3567,
      "step": 362
    },
    {
      "epoch": 3.242696629213483,
      "grad_norm": 0.7554347515106201,
      "learning_rate": 0.00012824120603015075,
      "loss": 0.2938,
      "step": 363
    },
    {
      "epoch": 3.251685393258427,
      "grad_norm": 0.8878810405731201,
      "learning_rate": 0.00012804020100502515,
      "loss": 0.4509,
      "step": 364
    },
    {
      "epoch": 3.2606741573033706,
      "grad_norm": 0.849636435508728,
      "learning_rate": 0.0001278391959798995,
      "loss": 0.4112,
      "step": 365
    },
    {
      "epoch": 3.2696629213483144,
      "grad_norm": 0.7759038209915161,
      "learning_rate": 0.00012763819095477387,
      "loss": 0.3778,
      "step": 366
    },
    {
      "epoch": 3.2786516853932586,
      "grad_norm": 0.7177848815917969,
      "learning_rate": 0.00012743718592964824,
      "loss": 0.3334,
      "step": 367
    },
    {
      "epoch": 3.2876404494382023,
      "grad_norm": 0.7122929096221924,
      "learning_rate": 0.00012723618090452262,
      "loss": 0.3364,
      "step": 368
    },
    {
      "epoch": 3.296629213483146,
      "grad_norm": 0.8124998807907104,
      "learning_rate": 0.000127035175879397,
      "loss": 0.3188,
      "step": 369
    },
    {
      "epoch": 3.30561797752809,
      "grad_norm": 0.7098675966262817,
      "learning_rate": 0.00012683417085427136,
      "loss": 0.369,
      "step": 370
    },
    {
      "epoch": 3.3146067415730336,
      "grad_norm": 0.74213707447052,
      "learning_rate": 0.00012663316582914574,
      "loss": 0.4122,
      "step": 371
    },
    {
      "epoch": 3.3235955056179773,
      "grad_norm": 0.6893321871757507,
      "learning_rate": 0.0001264321608040201,
      "loss": 0.2779,
      "step": 372
    },
    {
      "epoch": 3.3325842696629215,
      "grad_norm": 0.719463050365448,
      "learning_rate": 0.00012623115577889448,
      "loss": 0.3726,
      "step": 373
    },
    {
      "epoch": 3.3415730337078653,
      "grad_norm": 0.7136383652687073,
      "learning_rate": 0.00012603015075376885,
      "loss": 0.3683,
      "step": 374
    },
    {
      "epoch": 3.350561797752809,
      "grad_norm": 0.743434727191925,
      "learning_rate": 0.00012582914572864323,
      "loss": 0.3665,
      "step": 375
    },
    {
      "epoch": 3.359550561797753,
      "grad_norm": 0.7832537293434143,
      "learning_rate": 0.0001256281407035176,
      "loss": 0.3686,
      "step": 376
    },
    {
      "epoch": 3.3685393258426966,
      "grad_norm": 0.7466504573822021,
      "learning_rate": 0.00012542713567839197,
      "loss": 0.3677,
      "step": 377
    },
    {
      "epoch": 3.3775280898876403,
      "grad_norm": 0.6690831184387207,
      "learning_rate": 0.00012522613065326635,
      "loss": 0.3013,
      "step": 378
    },
    {
      "epoch": 3.3865168539325845,
      "grad_norm": 0.756691575050354,
      "learning_rate": 0.0001250251256281407,
      "loss": 0.385,
      "step": 379
    },
    {
      "epoch": 3.3955056179775283,
      "grad_norm": 0.6948524117469788,
      "learning_rate": 0.00012482412060301507,
      "loss": 0.3517,
      "step": 380
    },
    {
      "epoch": 3.404494382022472,
      "grad_norm": 0.7330809235572815,
      "learning_rate": 0.00012462311557788947,
      "loss": 0.3494,
      "step": 381
    },
    {
      "epoch": 3.4134831460674158,
      "grad_norm": 0.7869780659675598,
      "learning_rate": 0.00012442211055276384,
      "loss": 0.4464,
      "step": 382
    },
    {
      "epoch": 3.4224719101123595,
      "grad_norm": 0.8178461194038391,
      "learning_rate": 0.00012422110552763818,
      "loss": 0.3412,
      "step": 383
    },
    {
      "epoch": 3.4314606741573033,
      "grad_norm": 0.7086721658706665,
      "learning_rate": 0.00012402010050251256,
      "loss": 0.3528,
      "step": 384
    },
    {
      "epoch": 3.440449438202247,
      "grad_norm": 0.7312350273132324,
      "learning_rate": 0.00012381909547738696,
      "loss": 0.3328,
      "step": 385
    },
    {
      "epoch": 3.449438202247191,
      "grad_norm": 0.7593942880630493,
      "learning_rate": 0.0001236180904522613,
      "loss": 0.3281,
      "step": 386
    },
    {
      "epoch": 3.458426966292135,
      "grad_norm": 0.7736111879348755,
      "learning_rate": 0.00012341708542713568,
      "loss": 0.4143,
      "step": 387
    },
    {
      "epoch": 3.4674157303370787,
      "grad_norm": 0.7553831934928894,
      "learning_rate": 0.00012321608040201005,
      "loss": 0.3764,
      "step": 388
    },
    {
      "epoch": 3.4764044943820225,
      "grad_norm": 0.6346025466918945,
      "learning_rate": 0.00012301507537688445,
      "loss": 0.3063,
      "step": 389
    },
    {
      "epoch": 3.4853932584269662,
      "grad_norm": 0.7386281490325928,
      "learning_rate": 0.0001228140703517588,
      "loss": 0.3583,
      "step": 390
    },
    {
      "epoch": 3.49438202247191,
      "grad_norm": 0.8871465921401978,
      "learning_rate": 0.00012261306532663317,
      "loss": 0.3872,
      "step": 391
    },
    {
      "epoch": 3.5033707865168537,
      "grad_norm": 0.7815625071525574,
      "learning_rate": 0.00012241206030150754,
      "loss": 0.4208,
      "step": 392
    },
    {
      "epoch": 3.512359550561798,
      "grad_norm": 0.6490662097930908,
      "learning_rate": 0.00012221105527638191,
      "loss": 0.3336,
      "step": 393
    },
    {
      "epoch": 3.5213483146067417,
      "grad_norm": 0.8376544117927551,
      "learning_rate": 0.00012201005025125629,
      "loss": 0.406,
      "step": 394
    },
    {
      "epoch": 3.5303370786516854,
      "grad_norm": 0.6983129978179932,
      "learning_rate": 0.00012180904522613066,
      "loss": 0.2934,
      "step": 395
    },
    {
      "epoch": 3.539325842696629,
      "grad_norm": 0.7467079758644104,
      "learning_rate": 0.00012160804020100502,
      "loss": 0.4012,
      "step": 396
    },
    {
      "epoch": 3.548314606741573,
      "grad_norm": 0.7544340491294861,
      "learning_rate": 0.00012140703517587942,
      "loss": 0.4022,
      "step": 397
    },
    {
      "epoch": 3.5573033707865167,
      "grad_norm": 0.6978884339332581,
      "learning_rate": 0.00012120603015075378,
      "loss": 0.3595,
      "step": 398
    },
    {
      "epoch": 3.5662921348314605,
      "grad_norm": 0.7388366460800171,
      "learning_rate": 0.00012100502512562815,
      "loss": 0.3647,
      "step": 399
    },
    {
      "epoch": 3.5752808988764047,
      "grad_norm": 0.8385876417160034,
      "learning_rate": 0.00012080402010050251,
      "loss": 0.3812,
      "step": 400
    },
    {
      "epoch": 3.5842696629213484,
      "grad_norm": 0.792842447757721,
      "learning_rate": 0.00012060301507537688,
      "loss": 0.4102,
      "step": 401
    },
    {
      "epoch": 3.593258426966292,
      "grad_norm": 0.6842297911643982,
      "learning_rate": 0.00012040201005025127,
      "loss": 0.351,
      "step": 402
    },
    {
      "epoch": 3.602247191011236,
      "grad_norm": 0.6840287446975708,
      "learning_rate": 0.00012020100502512563,
      "loss": 0.2561,
      "step": 403
    },
    {
      "epoch": 3.6112359550561797,
      "grad_norm": 0.749516487121582,
      "learning_rate": 0.00012,
      "loss": 0.3617,
      "step": 404
    },
    {
      "epoch": 3.620224719101124,
      "grad_norm": 0.7854650616645813,
      "learning_rate": 0.00011979899497487436,
      "loss": 0.4217,
      "step": 405
    },
    {
      "epoch": 3.629213483146067,
      "grad_norm": 0.7130045890808105,
      "learning_rate": 0.00011959798994974876,
      "loss": 0.3679,
      "step": 406
    },
    {
      "epoch": 3.6382022471910114,
      "grad_norm": 0.6821588277816772,
      "learning_rate": 0.00011939698492462312,
      "loss": 0.3038,
      "step": 407
    },
    {
      "epoch": 3.647191011235955,
      "grad_norm": 0.6782889366149902,
      "learning_rate": 0.0001191959798994975,
      "loss": 0.335,
      "step": 408
    },
    {
      "epoch": 3.656179775280899,
      "grad_norm": 0.842370331287384,
      "learning_rate": 0.00011899497487437185,
      "loss": 0.4232,
      "step": 409
    },
    {
      "epoch": 3.6651685393258426,
      "grad_norm": 0.8541638851165771,
      "learning_rate": 0.00011879396984924624,
      "loss": 0.4059,
      "step": 410
    },
    {
      "epoch": 3.6741573033707864,
      "grad_norm": 0.7281871438026428,
      "learning_rate": 0.00011859296482412061,
      "loss": 0.3201,
      "step": 411
    },
    {
      "epoch": 3.6831460674157306,
      "grad_norm": 0.8146765232086182,
      "learning_rate": 0.00011839195979899497,
      "loss": 0.3779,
      "step": 412
    },
    {
      "epoch": 3.692134831460674,
      "grad_norm": 0.8430759310722351,
      "learning_rate": 0.00011819095477386935,
      "loss": 0.4202,
      "step": 413
    },
    {
      "epoch": 3.701123595505618,
      "grad_norm": 0.787123441696167,
      "learning_rate": 0.00011798994974874373,
      "loss": 0.3185,
      "step": 414
    },
    {
      "epoch": 3.710112359550562,
      "grad_norm": 0.6547075510025024,
      "learning_rate": 0.0001177889447236181,
      "loss": 0.2879,
      "step": 415
    },
    {
      "epoch": 3.7191011235955056,
      "grad_norm": 0.6983742117881775,
      "learning_rate": 0.00011758793969849247,
      "loss": 0.3542,
      "step": 416
    },
    {
      "epoch": 3.7280898876404494,
      "grad_norm": 0.7807478904724121,
      "learning_rate": 0.00011738693467336684,
      "loss": 0.3881,
      "step": 417
    },
    {
      "epoch": 3.737078651685393,
      "grad_norm": 0.7677443027496338,
      "learning_rate": 0.00011718592964824122,
      "loss": 0.3875,
      "step": 418
    },
    {
      "epoch": 3.7460674157303373,
      "grad_norm": 0.794285237789154,
      "learning_rate": 0.00011698492462311558,
      "loss": 0.3679,
      "step": 419
    },
    {
      "epoch": 3.755056179775281,
      "grad_norm": 0.7937585115432739,
      "learning_rate": 0.00011678391959798996,
      "loss": 0.4337,
      "step": 420
    },
    {
      "epoch": 3.764044943820225,
      "grad_norm": 0.6814838647842407,
      "learning_rate": 0.00011658291457286432,
      "loss": 0.3486,
      "step": 421
    },
    {
      "epoch": 3.7730337078651686,
      "grad_norm": 0.6709129810333252,
      "learning_rate": 0.00011638190954773872,
      "loss": 0.2449,
      "step": 422
    },
    {
      "epoch": 3.7820224719101123,
      "grad_norm": 0.6068003177642822,
      "learning_rate": 0.00011618090452261308,
      "loss": 0.2801,
      "step": 423
    },
    {
      "epoch": 3.791011235955056,
      "grad_norm": 0.771164059638977,
      "learning_rate": 0.00011597989949748745,
      "loss": 0.344,
      "step": 424
    },
    {
      "epoch": 3.8,
      "grad_norm": 0.7781122922897339,
      "learning_rate": 0.00011577889447236181,
      "loss": 0.4293,
      "step": 425
    },
    {
      "epoch": 3.808988764044944,
      "grad_norm": 0.760399580001831,
      "learning_rate": 0.00011557788944723618,
      "loss": 0.3243,
      "step": 426
    },
    {
      "epoch": 3.8179775280898878,
      "grad_norm": 0.6893835663795471,
      "learning_rate": 0.00011537688442211057,
      "loss": 0.3106,
      "step": 427
    },
    {
      "epoch": 3.8269662921348315,
      "grad_norm": 0.7480776906013489,
      "learning_rate": 0.00011517587939698493,
      "loss": 0.3949,
      "step": 428
    },
    {
      "epoch": 3.8359550561797753,
      "grad_norm": 0.8022905588150024,
      "learning_rate": 0.0001149748743718593,
      "loss": 0.4187,
      "step": 429
    },
    {
      "epoch": 3.844943820224719,
      "grad_norm": 0.7737365365028381,
      "learning_rate": 0.00011477386934673366,
      "loss": 0.3538,
      "step": 430
    },
    {
      "epoch": 3.853932584269663,
      "grad_norm": 0.7838185429573059,
      "learning_rate": 0.00011457286432160806,
      "loss": 0.4341,
      "step": 431
    },
    {
      "epoch": 3.8629213483146065,
      "grad_norm": 0.7684136033058167,
      "learning_rate": 0.00011437185929648242,
      "loss": 0.3644,
      "step": 432
    },
    {
      "epoch": 3.8719101123595507,
      "grad_norm": 0.7471869587898254,
      "learning_rate": 0.00011417085427135679,
      "loss": 0.3982,
      "step": 433
    },
    {
      "epoch": 3.8808988764044945,
      "grad_norm": 0.6247967481613159,
      "learning_rate": 0.00011396984924623115,
      "loss": 0.2802,
      "step": 434
    },
    {
      "epoch": 3.8898876404494382,
      "grad_norm": 0.7592265009880066,
      "learning_rate": 0.00011376884422110554,
      "loss": 0.4051,
      "step": 435
    },
    {
      "epoch": 3.898876404494382,
      "grad_norm": 0.8130335807800293,
      "learning_rate": 0.00011356783919597991,
      "loss": 0.4261,
      "step": 436
    },
    {
      "epoch": 3.9078651685393258,
      "grad_norm": 0.8373491764068604,
      "learning_rate": 0.00011336683417085427,
      "loss": 0.3824,
      "step": 437
    },
    {
      "epoch": 3.9168539325842695,
      "grad_norm": 0.6816343069076538,
      "learning_rate": 0.00011316582914572864,
      "loss": 0.3138,
      "step": 438
    },
    {
      "epoch": 3.9258426966292133,
      "grad_norm": 0.7390284538269043,
      "learning_rate": 0.00011296482412060303,
      "loss": 0.3722,
      "step": 439
    },
    {
      "epoch": 3.9348314606741575,
      "grad_norm": 0.6764691472053528,
      "learning_rate": 0.0001127638190954774,
      "loss": 0.3041,
      "step": 440
    },
    {
      "epoch": 3.943820224719101,
      "grad_norm": 0.7113064527511597,
      "learning_rate": 0.00011256281407035176,
      "loss": 0.2926,
      "step": 441
    },
    {
      "epoch": 3.952808988764045,
      "grad_norm": 0.6337546706199646,
      "learning_rate": 0.00011236180904522614,
      "loss": 0.3036,
      "step": 442
    },
    {
      "epoch": 3.9617977528089887,
      "grad_norm": 0.7553158402442932,
      "learning_rate": 0.00011216080402010052,
      "loss": 0.3973,
      "step": 443
    },
    {
      "epoch": 3.9707865168539325,
      "grad_norm": 0.83677077293396,
      "learning_rate": 0.00011195979899497488,
      "loss": 0.4605,
      "step": 444
    },
    {
      "epoch": 3.9797752808988767,
      "grad_norm": 0.7215544581413269,
      "learning_rate": 0.00011175879396984925,
      "loss": 0.3487,
      "step": 445
    },
    {
      "epoch": 3.98876404494382,
      "grad_norm": 0.6975643038749695,
      "learning_rate": 0.00011155778894472361,
      "loss": 0.3027,
      "step": 446
    },
    {
      "epoch": 3.997752808988764,
      "grad_norm": 0.5573864579200745,
      "learning_rate": 0.00011135678391959799,
      "loss": 0.282,
      "step": 447
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.9003021717071533,
      "learning_rate": 0.00011115577889447237,
      "loss": 0.2126,
      "step": 448
    },
    {
      "epoch": 4.008988764044944,
      "grad_norm": 0.6495447158813477,
      "learning_rate": 0.00011095477386934675,
      "loss": 0.2508,
      "step": 449
    },
    {
      "epoch": 4.0179775280898875,
      "grad_norm": 0.6412861943244934,
      "learning_rate": 0.0001107537688442211,
      "loss": 0.2376,
      "step": 450
    },
    {
      "epoch": 4.026966292134832,
      "grad_norm": 0.4923248291015625,
      "learning_rate": 0.00011055276381909548,
      "loss": 0.1681,
      "step": 451
    },
    {
      "epoch": 4.035955056179775,
      "grad_norm": 0.6236695051193237,
      "learning_rate": 0.00011035175879396986,
      "loss": 0.1787,
      "step": 452
    },
    {
      "epoch": 4.044943820224719,
      "grad_norm": 0.7589457631111145,
      "learning_rate": 0.00011015075376884422,
      "loss": 0.2579,
      "step": 453
    },
    {
      "epoch": 4.0539325842696625,
      "grad_norm": 0.8254818916320801,
      "learning_rate": 0.0001099497487437186,
      "loss": 0.2637,
      "step": 454
    },
    {
      "epoch": 4.062921348314607,
      "grad_norm": 0.8028696775436401,
      "learning_rate": 0.00010974874371859296,
      "loss": 0.2462,
      "step": 455
    },
    {
      "epoch": 4.071910112359551,
      "grad_norm": 0.8442559242248535,
      "learning_rate": 0.00010954773869346736,
      "loss": 0.2409,
      "step": 456
    },
    {
      "epoch": 4.080898876404494,
      "grad_norm": 0.817123293876648,
      "learning_rate": 0.00010934673366834172,
      "loss": 0.1833,
      "step": 457
    },
    {
      "epoch": 4.089887640449438,
      "grad_norm": 1.053604245185852,
      "learning_rate": 0.00010914572864321609,
      "loss": 0.2499,
      "step": 458
    },
    {
      "epoch": 4.098876404494382,
      "grad_norm": 0.721295952796936,
      "learning_rate": 0.00010894472361809045,
      "loss": 0.1359,
      "step": 459
    },
    {
      "epoch": 4.107865168539326,
      "grad_norm": 1.2230534553527832,
      "learning_rate": 0.00010874371859296483,
      "loss": 0.2709,
      "step": 460
    },
    {
      "epoch": 4.116853932584269,
      "grad_norm": 0.9722910523414612,
      "learning_rate": 0.00010854271356783921,
      "loss": 0.2126,
      "step": 461
    },
    {
      "epoch": 4.125842696629213,
      "grad_norm": 0.8737917542457581,
      "learning_rate": 0.00010834170854271357,
      "loss": 0.1852,
      "step": 462
    },
    {
      "epoch": 4.134831460674158,
      "grad_norm": 0.9618750214576721,
      "learning_rate": 0.00010814070351758794,
      "loss": 0.2582,
      "step": 463
    },
    {
      "epoch": 4.143820224719101,
      "grad_norm": 0.8098375201225281,
      "learning_rate": 0.00010793969849246233,
      "loss": 0.2365,
      "step": 464
    },
    {
      "epoch": 4.152808988764045,
      "grad_norm": 0.8762398362159729,
      "learning_rate": 0.0001077386934673367,
      "loss": 0.2572,
      "step": 465
    },
    {
      "epoch": 4.1617977528089884,
      "grad_norm": 0.7826884984970093,
      "learning_rate": 0.00010753768844221106,
      "loss": 0.2208,
      "step": 466
    },
    {
      "epoch": 4.170786516853933,
      "grad_norm": 0.8834488987922668,
      "learning_rate": 0.00010733668341708543,
      "loss": 0.2246,
      "step": 467
    },
    {
      "epoch": 4.179775280898877,
      "grad_norm": 0.821602463722229,
      "learning_rate": 0.00010713567839195982,
      "loss": 0.2519,
      "step": 468
    },
    {
      "epoch": 4.18876404494382,
      "grad_norm": 0.841163158416748,
      "learning_rate": 0.00010693467336683418,
      "loss": 0.2511,
      "step": 469
    },
    {
      "epoch": 4.197752808988764,
      "grad_norm": 1.2113871574401855,
      "learning_rate": 0.00010673366834170855,
      "loss": 0.2276,
      "step": 470
    },
    {
      "epoch": 4.206741573033708,
      "grad_norm": 1.1432843208312988,
      "learning_rate": 0.00010653266331658291,
      "loss": 0.2408,
      "step": 471
    },
    {
      "epoch": 4.215730337078652,
      "grad_norm": 0.8568527698516846,
      "learning_rate": 0.00010633165829145728,
      "loss": 0.2313,
      "step": 472
    },
    {
      "epoch": 4.224719101123595,
      "grad_norm": 0.7555668950080872,
      "learning_rate": 0.00010613065326633167,
      "loss": 0.1709,
      "step": 473
    },
    {
      "epoch": 4.233707865168539,
      "grad_norm": 0.8678665161132812,
      "learning_rate": 0.00010592964824120604,
      "loss": 0.2027,
      "step": 474
    },
    {
      "epoch": 4.242696629213484,
      "grad_norm": 0.8474178910255432,
      "learning_rate": 0.0001057286432160804,
      "loss": 0.2342,
      "step": 475
    },
    {
      "epoch": 4.251685393258427,
      "grad_norm": 0.8506501913070679,
      "learning_rate": 0.00010552763819095478,
      "loss": 0.1726,
      "step": 476
    },
    {
      "epoch": 4.260674157303371,
      "grad_norm": 0.7650928497314453,
      "learning_rate": 0.00010532663316582916,
      "loss": 0.1879,
      "step": 477
    },
    {
      "epoch": 4.269662921348314,
      "grad_norm": 0.7401047348976135,
      "learning_rate": 0.00010512562814070352,
      "loss": 0.1746,
      "step": 478
    },
    {
      "epoch": 4.278651685393259,
      "grad_norm": 0.7966935634613037,
      "learning_rate": 0.0001049246231155779,
      "loss": 0.1963,
      "step": 479
    },
    {
      "epoch": 4.287640449438202,
      "grad_norm": 0.8457091450691223,
      "learning_rate": 0.00010472361809045225,
      "loss": 0.2247,
      "step": 480
    },
    {
      "epoch": 4.296629213483146,
      "grad_norm": 0.8635743856430054,
      "learning_rate": 0.00010452261306532664,
      "loss": 0.1918,
      "step": 481
    },
    {
      "epoch": 4.30561797752809,
      "grad_norm": 1.0104352235794067,
      "learning_rate": 0.00010432160804020101,
      "loss": 0.2678,
      "step": 482
    },
    {
      "epoch": 4.314606741573034,
      "grad_norm": 0.8418344855308533,
      "learning_rate": 0.00010412060301507539,
      "loss": 0.2104,
      "step": 483
    },
    {
      "epoch": 4.323595505617978,
      "grad_norm": 0.8345755934715271,
      "learning_rate": 0.00010391959798994975,
      "loss": 0.2212,
      "step": 484
    },
    {
      "epoch": 4.332584269662921,
      "grad_norm": 0.8102269768714905,
      "learning_rate": 0.00010371859296482413,
      "loss": 0.2349,
      "step": 485
    },
    {
      "epoch": 4.341573033707865,
      "grad_norm": 0.8695855140686035,
      "learning_rate": 0.0001035175879396985,
      "loss": 0.1842,
      "step": 486
    },
    {
      "epoch": 4.350561797752809,
      "grad_norm": 0.8787556290626526,
      "learning_rate": 0.00010331658291457286,
      "loss": 0.1635,
      "step": 487
    },
    {
      "epoch": 4.359550561797753,
      "grad_norm": 0.9394383430480957,
      "learning_rate": 0.00010311557788944724,
      "loss": 0.2754,
      "step": 488
    },
    {
      "epoch": 4.368539325842697,
      "grad_norm": 0.832755982875824,
      "learning_rate": 0.00010291457286432162,
      "loss": 0.2544,
      "step": 489
    },
    {
      "epoch": 4.37752808988764,
      "grad_norm": 0.8823708891868591,
      "learning_rate": 0.00010271356783919598,
      "loss": 0.2526,
      "step": 490
    },
    {
      "epoch": 4.3865168539325845,
      "grad_norm": 0.8316506147384644,
      "learning_rate": 0.00010251256281407036,
      "loss": 0.2217,
      "step": 491
    },
    {
      "epoch": 4.395505617977528,
      "grad_norm": 0.925698459148407,
      "learning_rate": 0.00010231155778894473,
      "loss": 0.2725,
      "step": 492
    },
    {
      "epoch": 4.404494382022472,
      "grad_norm": 0.8309943675994873,
      "learning_rate": 0.00010211055276381909,
      "loss": 0.2343,
      "step": 493
    },
    {
      "epoch": 4.413483146067415,
      "grad_norm": 0.7822101712226868,
      "learning_rate": 0.00010190954773869348,
      "loss": 0.2078,
      "step": 494
    },
    {
      "epoch": 4.4224719101123595,
      "grad_norm": 0.8135600686073303,
      "learning_rate": 0.00010170854271356785,
      "loss": 0.2221,
      "step": 495
    },
    {
      "epoch": 4.431460674157304,
      "grad_norm": 1.037107229232788,
      "learning_rate": 0.00010150753768844221,
      "loss": 0.2225,
      "step": 496
    },
    {
      "epoch": 4.440449438202247,
      "grad_norm": 0.8496840000152588,
      "learning_rate": 0.00010130653266331658,
      "loss": 0.1529,
      "step": 497
    },
    {
      "epoch": 4.449438202247191,
      "grad_norm": 0.8700290322303772,
      "learning_rate": 0.00010110552763819097,
      "loss": 0.2347,
      "step": 498
    },
    {
      "epoch": 4.4584269662921345,
      "grad_norm": 0.7039309740066528,
      "learning_rate": 0.00010090452261306533,
      "loss": 0.1548,
      "step": 499
    },
    {
      "epoch": 4.467415730337079,
      "grad_norm": 0.7159139513969421,
      "learning_rate": 0.0001007035175879397,
      "loss": 0.1884,
      "step": 500
    }
  ],
  "logging_steps": 1,
  "max_steps": 1000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 9,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 4.225177192393248e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
