{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 8.934831460674157,
  "eval_steps": 500,
  "global_step": 1000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.008988764044943821,
      "grad_norm": 5.867749214172363,
      "learning_rate": 0.0,
      "loss": 2.9327,
      "step": 1
    },
    {
      "epoch": 0.017977528089887642,
      "grad_norm": 6.089925289154053,
      "learning_rate": 4e-05,
      "loss": 2.8975,
      "step": 2
    },
    {
      "epoch": 0.02696629213483146,
      "grad_norm": 5.607311725616455,
      "learning_rate": 8e-05,
      "loss": 2.8929,
      "step": 3
    },
    {
      "epoch": 0.035955056179775284,
      "grad_norm": 3.70684552192688,
      "learning_rate": 0.00012,
      "loss": 2.4366,
      "step": 4
    },
    {
      "epoch": 0.0449438202247191,
      "grad_norm": 2.045083522796631,
      "learning_rate": 0.00016,
      "loss": 2.0695,
      "step": 5
    },
    {
      "epoch": 0.05393258426966292,
      "grad_norm": 1.2802430391311646,
      "learning_rate": 0.0002,
      "loss": 1.891,
      "step": 6
    },
    {
      "epoch": 0.06292134831460675,
      "grad_norm": 0.898371160030365,
      "learning_rate": 0.00019979899497487438,
      "loss": 1.6193,
      "step": 7
    },
    {
      "epoch": 0.07191011235955057,
      "grad_norm": 0.9284430146217346,
      "learning_rate": 0.00019959798994974876,
      "loss": 1.6732,
      "step": 8
    },
    {
      "epoch": 0.08089887640449438,
      "grad_norm": 0.856439471244812,
      "learning_rate": 0.00019939698492462313,
      "loss": 1.541,
      "step": 9
    },
    {
      "epoch": 0.0898876404494382,
      "grad_norm": 0.7945054769515991,
      "learning_rate": 0.0001991959798994975,
      "loss": 1.4773,
      "step": 10
    },
    {
      "epoch": 0.09887640449438202,
      "grad_norm": 0.7183399200439453,
      "learning_rate": 0.00019899497487437187,
      "loss": 1.4706,
      "step": 11
    },
    {
      "epoch": 0.10786516853932585,
      "grad_norm": 0.6312628984451294,
      "learning_rate": 0.00019879396984924622,
      "loss": 1.2916,
      "step": 12
    },
    {
      "epoch": 0.11685393258426967,
      "grad_norm": 0.582293689250946,
      "learning_rate": 0.00019859296482412062,
      "loss": 1.2124,
      "step": 13
    },
    {
      "epoch": 0.1258426966292135,
      "grad_norm": 0.5891973972320557,
      "learning_rate": 0.000198391959798995,
      "loss": 1.2031,
      "step": 14
    },
    {
      "epoch": 0.1348314606741573,
      "grad_norm": 0.4892641603946686,
      "learning_rate": 0.00019819095477386937,
      "loss": 1.2475,
      "step": 15
    },
    {
      "epoch": 0.14382022471910114,
      "grad_norm": 0.4891263544559479,
      "learning_rate": 0.0001979899497487437,
      "loss": 1.1212,
      "step": 16
    },
    {
      "epoch": 0.15280898876404495,
      "grad_norm": 0.5373388528823853,
      "learning_rate": 0.0001977889447236181,
      "loss": 1.075,
      "step": 17
    },
    {
      "epoch": 0.16179775280898875,
      "grad_norm": 0.49342024326324463,
      "learning_rate": 0.00019758793969849249,
      "loss": 1.0955,
      "step": 18
    },
    {
      "epoch": 0.1707865168539326,
      "grad_norm": 0.5118899345397949,
      "learning_rate": 0.00019738693467336683,
      "loss": 1.094,
      "step": 19
    },
    {
      "epoch": 0.1797752808988764,
      "grad_norm": 0.5248528122901917,
      "learning_rate": 0.0001971859296482412,
      "loss": 1.0609,
      "step": 20
    },
    {
      "epoch": 0.18876404494382024,
      "grad_norm": 0.46604856848716736,
      "learning_rate": 0.0001969849246231156,
      "loss": 1.0628,
      "step": 21
    },
    {
      "epoch": 0.19775280898876405,
      "grad_norm": 0.4173665940761566,
      "learning_rate": 0.00019678391959798995,
      "loss": 0.9104,
      "step": 22
    },
    {
      "epoch": 0.20674157303370785,
      "grad_norm": 0.42213624715805054,
      "learning_rate": 0.00019658291457286432,
      "loss": 1.0498,
      "step": 23
    },
    {
      "epoch": 0.2157303370786517,
      "grad_norm": 0.4096555709838867,
      "learning_rate": 0.0001963819095477387,
      "loss": 0.9594,
      "step": 24
    },
    {
      "epoch": 0.2247191011235955,
      "grad_norm": 0.377143919467926,
      "learning_rate": 0.0001961809045226131,
      "loss": 0.9617,
      "step": 25
    },
    {
      "epoch": 0.23370786516853934,
      "grad_norm": 0.40291813015937805,
      "learning_rate": 0.00019597989949748744,
      "loss": 0.9425,
      "step": 26
    },
    {
      "epoch": 0.24269662921348314,
      "grad_norm": 0.4336307942867279,
      "learning_rate": 0.00019577889447236181,
      "loss": 1.0204,
      "step": 27
    },
    {
      "epoch": 0.251685393258427,
      "grad_norm": 0.402761846780777,
      "learning_rate": 0.0001955778894472362,
      "loss": 0.9487,
      "step": 28
    },
    {
      "epoch": 0.2606741573033708,
      "grad_norm": 0.4199514091014862,
      "learning_rate": 0.00019537688442211056,
      "loss": 1.0548,
      "step": 29
    },
    {
      "epoch": 0.2696629213483146,
      "grad_norm": 0.37908244132995605,
      "learning_rate": 0.00019517587939698493,
      "loss": 0.9881,
      "step": 30
    },
    {
      "epoch": 0.2786516853932584,
      "grad_norm": 0.4199800193309784,
      "learning_rate": 0.0001949748743718593,
      "loss": 0.9603,
      "step": 31
    },
    {
      "epoch": 0.2876404494382023,
      "grad_norm": 0.3951322138309479,
      "learning_rate": 0.00019477386934673368,
      "loss": 0.9788,
      "step": 32
    },
    {
      "epoch": 0.2966292134831461,
      "grad_norm": 0.41350582242012024,
      "learning_rate": 0.00019457286432160805,
      "loss": 0.982,
      "step": 33
    },
    {
      "epoch": 0.3056179775280899,
      "grad_norm": 0.39367079734802246,
      "learning_rate": 0.00019437185929648243,
      "loss": 0.9235,
      "step": 34
    },
    {
      "epoch": 0.3146067415730337,
      "grad_norm": 0.40450218319892883,
      "learning_rate": 0.0001941708542713568,
      "loss": 0.9477,
      "step": 35
    },
    {
      "epoch": 0.3235955056179775,
      "grad_norm": 0.3980608880519867,
      "learning_rate": 0.00019396984924623117,
      "loss": 0.9309,
      "step": 36
    },
    {
      "epoch": 0.3325842696629214,
      "grad_norm": 0.3938213288784027,
      "learning_rate": 0.00019376884422110552,
      "loss": 0.9232,
      "step": 37
    },
    {
      "epoch": 0.3415730337078652,
      "grad_norm": 0.3818555176258087,
      "learning_rate": 0.00019356783919597992,
      "loss": 0.9612,
      "step": 38
    },
    {
      "epoch": 0.350561797752809,
      "grad_norm": 0.39738729596138,
      "learning_rate": 0.0001933668341708543,
      "loss": 0.9214,
      "step": 39
    },
    {
      "epoch": 0.3595505617977528,
      "grad_norm": 0.4112373888492584,
      "learning_rate": 0.00019316582914572864,
      "loss": 0.9443,
      "step": 40
    },
    {
      "epoch": 0.3685393258426966,
      "grad_norm": 0.44761043787002563,
      "learning_rate": 0.000192964824120603,
      "loss": 0.9779,
      "step": 41
    },
    {
      "epoch": 0.3775280898876405,
      "grad_norm": 0.3748815953731537,
      "learning_rate": 0.0001927638190954774,
      "loss": 0.9403,
      "step": 42
    },
    {
      "epoch": 0.3865168539325843,
      "grad_norm": 0.38828393816947937,
      "learning_rate": 0.00019256281407035178,
      "loss": 0.8938,
      "step": 43
    },
    {
      "epoch": 0.3955056179775281,
      "grad_norm": 0.3828636407852173,
      "learning_rate": 0.00019236180904522613,
      "loss": 0.9022,
      "step": 44
    },
    {
      "epoch": 0.4044943820224719,
      "grad_norm": 0.3481641709804535,
      "learning_rate": 0.0001921608040201005,
      "loss": 0.8908,
      "step": 45
    },
    {
      "epoch": 0.4134831460674157,
      "grad_norm": 0.3703015148639679,
      "learning_rate": 0.0001919597989949749,
      "loss": 0.9164,
      "step": 46
    },
    {
      "epoch": 0.42247191011235957,
      "grad_norm": 0.3773420751094818,
      "learning_rate": 0.00019175879396984925,
      "loss": 0.9058,
      "step": 47
    },
    {
      "epoch": 0.4314606741573034,
      "grad_norm": 0.38058748841285706,
      "learning_rate": 0.00019155778894472362,
      "loss": 1.0132,
      "step": 48
    },
    {
      "epoch": 0.4404494382022472,
      "grad_norm": 0.3675095736980438,
      "learning_rate": 0.000191356783919598,
      "loss": 0.9217,
      "step": 49
    },
    {
      "epoch": 0.449438202247191,
      "grad_norm": 0.35707536339759827,
      "learning_rate": 0.0001911557788944724,
      "loss": 0.8823,
      "step": 50
    },
    {
      "epoch": 0.4584269662921348,
      "grad_norm": 0.35422247648239136,
      "learning_rate": 0.00019095477386934674,
      "loss": 0.8278,
      "step": 51
    },
    {
      "epoch": 0.46741573033707867,
      "grad_norm": 0.36722618341445923,
      "learning_rate": 0.0001907537688442211,
      "loss": 0.8718,
      "step": 52
    },
    {
      "epoch": 0.4764044943820225,
      "grad_norm": 0.38193291425704956,
      "learning_rate": 0.00019055276381909548,
      "loss": 0.9399,
      "step": 53
    },
    {
      "epoch": 0.4853932584269663,
      "grad_norm": 0.390215128660202,
      "learning_rate": 0.00019035175879396986,
      "loss": 0.921,
      "step": 54
    },
    {
      "epoch": 0.4943820224719101,
      "grad_norm": 0.3703664243221283,
      "learning_rate": 0.00019015075376884423,
      "loss": 0.9191,
      "step": 55
    },
    {
      "epoch": 0.503370786516854,
      "grad_norm": 0.431052029132843,
      "learning_rate": 0.0001899497487437186,
      "loss": 0.8556,
      "step": 56
    },
    {
      "epoch": 0.5123595505617977,
      "grad_norm": 0.37735891342163086,
      "learning_rate": 0.00018974874371859298,
      "loss": 0.9284,
      "step": 57
    },
    {
      "epoch": 0.5213483146067416,
      "grad_norm": 0.39499998092651367,
      "learning_rate": 0.00018954773869346732,
      "loss": 0.9808,
      "step": 58
    },
    {
      "epoch": 0.5303370786516854,
      "grad_norm": 0.41476577520370483,
      "learning_rate": 0.00018934673366834172,
      "loss": 0.9974,
      "step": 59
    },
    {
      "epoch": 0.5393258426966292,
      "grad_norm": 0.3912324905395508,
      "learning_rate": 0.0001891457286432161,
      "loss": 0.8562,
      "step": 60
    },
    {
      "epoch": 0.5483146067415731,
      "grad_norm": 0.3886553645133972,
      "learning_rate": 0.00018894472361809047,
      "loss": 0.9381,
      "step": 61
    },
    {
      "epoch": 0.5573033707865168,
      "grad_norm": 0.3735424876213074,
      "learning_rate": 0.00018874371859296481,
      "loss": 0.8479,
      "step": 62
    },
    {
      "epoch": 0.5662921348314607,
      "grad_norm": 0.3583531975746155,
      "learning_rate": 0.00018854271356783921,
      "loss": 0.9041,
      "step": 63
    },
    {
      "epoch": 0.5752808988764045,
      "grad_norm": 0.3895391523838043,
      "learning_rate": 0.0001883417085427136,
      "loss": 0.905,
      "step": 64
    },
    {
      "epoch": 0.5842696629213483,
      "grad_norm": 0.348271906375885,
      "learning_rate": 0.00018814070351758793,
      "loss": 0.8225,
      "step": 65
    },
    {
      "epoch": 0.5932584269662922,
      "grad_norm": 0.33687055110931396,
      "learning_rate": 0.0001879396984924623,
      "loss": 0.8673,
      "step": 66
    },
    {
      "epoch": 0.6022471910112359,
      "grad_norm": 0.34555262327194214,
      "learning_rate": 0.0001877386934673367,
      "loss": 0.7744,
      "step": 67
    },
    {
      "epoch": 0.6112359550561798,
      "grad_norm": 0.34469354152679443,
      "learning_rate": 0.00018753768844221108,
      "loss": 0.831,
      "step": 68
    },
    {
      "epoch": 0.6202247191011236,
      "grad_norm": 0.38777756690979004,
      "learning_rate": 0.00018733668341708543,
      "loss": 0.8816,
      "step": 69
    },
    {
      "epoch": 0.6292134831460674,
      "grad_norm": 0.39915797114372253,
      "learning_rate": 0.0001871356783919598,
      "loss": 0.903,
      "step": 70
    },
    {
      "epoch": 0.6382022471910113,
      "grad_norm": 0.415497362613678,
      "learning_rate": 0.0001869346733668342,
      "loss": 0.8148,
      "step": 71
    },
    {
      "epoch": 0.647191011235955,
      "grad_norm": 0.4267079532146454,
      "learning_rate": 0.00018673366834170854,
      "loss": 0.9171,
      "step": 72
    },
    {
      "epoch": 0.6561797752808989,
      "grad_norm": 0.38350701332092285,
      "learning_rate": 0.00018653266331658292,
      "loss": 0.9319,
      "step": 73
    },
    {
      "epoch": 0.6651685393258427,
      "grad_norm": 0.3482752740383148,
      "learning_rate": 0.0001863316582914573,
      "loss": 0.7812,
      "step": 74
    },
    {
      "epoch": 0.6741573033707865,
      "grad_norm": 0.37183454632759094,
      "learning_rate": 0.0001861306532663317,
      "loss": 0.8417,
      "step": 75
    },
    {
      "epoch": 0.6831460674157304,
      "grad_norm": 0.4105025827884674,
      "learning_rate": 0.00018592964824120604,
      "loss": 0.8403,
      "step": 76
    },
    {
      "epoch": 0.6921348314606741,
      "grad_norm": 0.3946227431297302,
      "learning_rate": 0.0001857286432160804,
      "loss": 0.8547,
      "step": 77
    },
    {
      "epoch": 0.701123595505618,
      "grad_norm": 0.4017118811607361,
      "learning_rate": 0.00018552763819095478,
      "loss": 0.8397,
      "step": 78
    },
    {
      "epoch": 0.7101123595505618,
      "grad_norm": 0.40974161028862,
      "learning_rate": 0.00018532663316582915,
      "loss": 0.8774,
      "step": 79
    },
    {
      "epoch": 0.7191011235955056,
      "grad_norm": 0.35914596915245056,
      "learning_rate": 0.00018512562814070353,
      "loss": 0.8316,
      "step": 80
    },
    {
      "epoch": 0.7280898876404495,
      "grad_norm": 0.3894263803958893,
      "learning_rate": 0.0001849246231155779,
      "loss": 0.8834,
      "step": 81
    },
    {
      "epoch": 0.7370786516853932,
      "grad_norm": 0.3998435139656067,
      "learning_rate": 0.00018472361809045227,
      "loss": 0.8497,
      "step": 82
    },
    {
      "epoch": 0.7460674157303371,
      "grad_norm": 0.3712238669395447,
      "learning_rate": 0.00018452261306532662,
      "loss": 0.8256,
      "step": 83
    },
    {
      "epoch": 0.755056179775281,
      "grad_norm": 0.37852635979652405,
      "learning_rate": 0.00018432160804020102,
      "loss": 0.8428,
      "step": 84
    },
    {
      "epoch": 0.7640449438202247,
      "grad_norm": 0.41039490699768066,
      "learning_rate": 0.0001841206030150754,
      "loss": 0.8384,
      "step": 85
    },
    {
      "epoch": 0.7730337078651686,
      "grad_norm": 0.36587080359458923,
      "learning_rate": 0.00018391959798994977,
      "loss": 0.805,
      "step": 86
    },
    {
      "epoch": 0.7820224719101123,
      "grad_norm": 0.3999593257904053,
      "learning_rate": 0.0001837185929648241,
      "loss": 0.8175,
      "step": 87
    },
    {
      "epoch": 0.7910112359550562,
      "grad_norm": 0.3999386429786682,
      "learning_rate": 0.0001835175879396985,
      "loss": 0.8624,
      "step": 88
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.3902212679386139,
      "learning_rate": 0.00018331658291457288,
      "loss": 0.7897,
      "step": 89
    },
    {
      "epoch": 0.8089887640449438,
      "grad_norm": 0.38353726267814636,
      "learning_rate": 0.00018311557788944723,
      "loss": 0.8559,
      "step": 90
    },
    {
      "epoch": 0.8179775280898877,
      "grad_norm": 0.42697617411613464,
      "learning_rate": 0.0001829145728643216,
      "loss": 0.8541,
      "step": 91
    },
    {
      "epoch": 0.8269662921348314,
      "grad_norm": 0.42537394165992737,
      "learning_rate": 0.000182713567839196,
      "loss": 0.8613,
      "step": 92
    },
    {
      "epoch": 0.8359550561797753,
      "grad_norm": 0.3900884687900543,
      "learning_rate": 0.00018251256281407038,
      "loss": 0.8603,
      "step": 93
    },
    {
      "epoch": 0.8449438202247191,
      "grad_norm": 0.40019869804382324,
      "learning_rate": 0.00018231155778894472,
      "loss": 0.8292,
      "step": 94
    },
    {
      "epoch": 0.8539325842696629,
      "grad_norm": 0.43005529046058655,
      "learning_rate": 0.0001821105527638191,
      "loss": 0.8261,
      "step": 95
    },
    {
      "epoch": 0.8629213483146068,
      "grad_norm": 0.3859888017177582,
      "learning_rate": 0.0001819095477386935,
      "loss": 0.7778,
      "step": 96
    },
    {
      "epoch": 0.8719101123595505,
      "grad_norm": 0.36568424105644226,
      "learning_rate": 0.00018170854271356784,
      "loss": 0.8256,
      "step": 97
    },
    {
      "epoch": 0.8808988764044944,
      "grad_norm": 0.39068710803985596,
      "learning_rate": 0.00018150753768844221,
      "loss": 0.8632,
      "step": 98
    },
    {
      "epoch": 0.8898876404494382,
      "grad_norm": 0.3732796609401703,
      "learning_rate": 0.0001813065326633166,
      "loss": 0.7997,
      "step": 99
    },
    {
      "epoch": 0.898876404494382,
      "grad_norm": 0.40858006477355957,
      "learning_rate": 0.00018110552763819096,
      "loss": 0.8876,
      "step": 100
    },
    {
      "epoch": 0.9078651685393259,
      "grad_norm": 0.3990350663661957,
      "learning_rate": 0.00018090452261306533,
      "loss": 0.8671,
      "step": 101
    },
    {
      "epoch": 0.9168539325842696,
      "grad_norm": 0.36895596981048584,
      "learning_rate": 0.0001807035175879397,
      "loss": 0.8447,
      "step": 102
    },
    {
      "epoch": 0.9258426966292135,
      "grad_norm": 0.39416226744651794,
      "learning_rate": 0.00018050251256281408,
      "loss": 0.8825,
      "step": 103
    },
    {
      "epoch": 0.9348314606741573,
      "grad_norm": 0.38766059279441833,
      "learning_rate": 0.00018030150753768845,
      "loss": 0.8204,
      "step": 104
    },
    {
      "epoch": 0.9438202247191011,
      "grad_norm": 0.42018699645996094,
      "learning_rate": 0.00018010050251256282,
      "loss": 0.8231,
      "step": 105
    },
    {
      "epoch": 0.952808988764045,
      "grad_norm": 0.39098498225212097,
      "learning_rate": 0.0001798994974874372,
      "loss": 0.8061,
      "step": 106
    },
    {
      "epoch": 0.9617977528089887,
      "grad_norm": 0.36769038438796997,
      "learning_rate": 0.00017969849246231157,
      "loss": 0.7754,
      "step": 107
    },
    {
      "epoch": 0.9707865168539326,
      "grad_norm": 0.4232609272003174,
      "learning_rate": 0.00017949748743718592,
      "loss": 0.8302,
      "step": 108
    },
    {
      "epoch": 0.9797752808988764,
      "grad_norm": 0.3972019851207733,
      "learning_rate": 0.00017929648241206032,
      "loss": 0.7942,
      "step": 109
    },
    {
      "epoch": 0.9887640449438202,
      "grad_norm": 0.39545804262161255,
      "learning_rate": 0.0001790954773869347,
      "loss": 0.8044,
      "step": 110
    },
    {
      "epoch": 0.9977528089887641,
      "grad_norm": 0.417758971452713,
      "learning_rate": 0.00017889447236180906,
      "loss": 0.8142,
      "step": 111
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.7638589143753052,
      "learning_rate": 0.0001786934673366834,
      "loss": 0.8826,
      "step": 112
    },
    {
      "epoch": 1.0089887640449438,
      "grad_norm": 0.3843201994895935,
      "learning_rate": 0.0001784924623115578,
      "loss": 0.7894,
      "step": 113
    },
    {
      "epoch": 1.0179775280898877,
      "grad_norm": 0.3837498724460602,
      "learning_rate": 0.00017829145728643218,
      "loss": 0.7617,
      "step": 114
    },
    {
      "epoch": 1.0269662921348315,
      "grad_norm": 0.3774866759777069,
      "learning_rate": 0.00017809045226130653,
      "loss": 0.7437,
      "step": 115
    },
    {
      "epoch": 1.0359550561797752,
      "grad_norm": 0.39342543482780457,
      "learning_rate": 0.0001778894472361809,
      "loss": 0.6878,
      "step": 116
    },
    {
      "epoch": 1.0449438202247192,
      "grad_norm": 0.40413787961006165,
      "learning_rate": 0.0001776884422110553,
      "loss": 0.714,
      "step": 117
    },
    {
      "epoch": 1.053932584269663,
      "grad_norm": 0.43508851528167725,
      "learning_rate": 0.00017748743718592967,
      "loss": 0.7655,
      "step": 118
    },
    {
      "epoch": 1.0629213483146067,
      "grad_norm": 0.42643389105796814,
      "learning_rate": 0.00017728643216080402,
      "loss": 0.7436,
      "step": 119
    },
    {
      "epoch": 1.0719101123595505,
      "grad_norm": 0.42035067081451416,
      "learning_rate": 0.0001770854271356784,
      "loss": 0.7224,
      "step": 120
    },
    {
      "epoch": 1.0808988764044944,
      "grad_norm": 0.4430274963378906,
      "learning_rate": 0.0001768844221105528,
      "loss": 0.7331,
      "step": 121
    },
    {
      "epoch": 1.0898876404494382,
      "grad_norm": 0.46041616797447205,
      "learning_rate": 0.00017668341708542714,
      "loss": 0.7634,
      "step": 122
    },
    {
      "epoch": 1.098876404494382,
      "grad_norm": 0.44022336602211,
      "learning_rate": 0.0001764824120603015,
      "loss": 0.7226,
      "step": 123
    },
    {
      "epoch": 1.107865168539326,
      "grad_norm": 0.4280224144458771,
      "learning_rate": 0.00017628140703517588,
      "loss": 0.6994,
      "step": 124
    },
    {
      "epoch": 1.1168539325842697,
      "grad_norm": 0.4560234546661377,
      "learning_rate": 0.00017608040201005026,
      "loss": 0.6701,
      "step": 125
    },
    {
      "epoch": 1.1258426966292134,
      "grad_norm": 0.43843650817871094,
      "learning_rate": 0.00017587939698492463,
      "loss": 0.7225,
      "step": 126
    },
    {
      "epoch": 1.1348314606741572,
      "grad_norm": 0.4603751003742218,
      "learning_rate": 0.000175678391959799,
      "loss": 0.6977,
      "step": 127
    },
    {
      "epoch": 1.1438202247191012,
      "grad_norm": 0.488694965839386,
      "learning_rate": 0.00017547738693467338,
      "loss": 0.7399,
      "step": 128
    },
    {
      "epoch": 1.152808988764045,
      "grad_norm": 0.5058482885360718,
      "learning_rate": 0.00017527638190954775,
      "loss": 0.822,
      "step": 129
    },
    {
      "epoch": 1.1617977528089887,
      "grad_norm": 0.42756450176239014,
      "learning_rate": 0.00017507537688442212,
      "loss": 0.6806,
      "step": 130
    },
    {
      "epoch": 1.1707865168539326,
      "grad_norm": 0.4557887315750122,
      "learning_rate": 0.0001748743718592965,
      "loss": 0.7495,
      "step": 131
    },
    {
      "epoch": 1.1797752808988764,
      "grad_norm": 0.4222213625907898,
      "learning_rate": 0.00017467336683417087,
      "loss": 0.7032,
      "step": 132
    },
    {
      "epoch": 1.1887640449438202,
      "grad_norm": 0.4623889923095703,
      "learning_rate": 0.00017447236180904521,
      "loss": 0.7604,
      "step": 133
    },
    {
      "epoch": 1.1977528089887641,
      "grad_norm": 0.46354904770851135,
      "learning_rate": 0.00017427135678391961,
      "loss": 0.7059,
      "step": 134
    },
    {
      "epoch": 1.2067415730337079,
      "grad_norm": 0.4317735731601715,
      "learning_rate": 0.000174070351758794,
      "loss": 0.684,
      "step": 135
    },
    {
      "epoch": 1.2157303370786516,
      "grad_norm": 0.45820531249046326,
      "learning_rate": 0.00017386934673366836,
      "loss": 0.751,
      "step": 136
    },
    {
      "epoch": 1.2247191011235956,
      "grad_norm": 0.44425931572914124,
      "learning_rate": 0.0001736683417085427,
      "loss": 0.6861,
      "step": 137
    },
    {
      "epoch": 1.2337078651685394,
      "grad_norm": 0.48116546869277954,
      "learning_rate": 0.0001734673366834171,
      "loss": 0.7022,
      "step": 138
    },
    {
      "epoch": 1.2426966292134831,
      "grad_norm": 0.4522624909877777,
      "learning_rate": 0.00017326633165829148,
      "loss": 0.7369,
      "step": 139
    },
    {
      "epoch": 1.2516853932584269,
      "grad_norm": 0.4743351638317108,
      "learning_rate": 0.00017306532663316582,
      "loss": 0.6894,
      "step": 140
    },
    {
      "epoch": 1.2606741573033708,
      "grad_norm": 0.47365182638168335,
      "learning_rate": 0.0001728643216080402,
      "loss": 0.6986,
      "step": 141
    },
    {
      "epoch": 1.2696629213483146,
      "grad_norm": 0.4866678714752197,
      "learning_rate": 0.0001726633165829146,
      "loss": 0.6603,
      "step": 142
    },
    {
      "epoch": 1.2786516853932584,
      "grad_norm": 0.4980037808418274,
      "learning_rate": 0.00017246231155778897,
      "loss": 0.6927,
      "step": 143
    },
    {
      "epoch": 1.2876404494382023,
      "grad_norm": 0.5299847722053528,
      "learning_rate": 0.00017226130653266332,
      "loss": 0.6806,
      "step": 144
    },
    {
      "epoch": 1.296629213483146,
      "grad_norm": 0.4967336058616638,
      "learning_rate": 0.0001720603015075377,
      "loss": 0.7083,
      "step": 145
    },
    {
      "epoch": 1.3056179775280898,
      "grad_norm": 0.4745408296585083,
      "learning_rate": 0.00017185929648241206,
      "loss": 0.6671,
      "step": 146
    },
    {
      "epoch": 1.3146067415730336,
      "grad_norm": 0.5049158334732056,
      "learning_rate": 0.00017165829145728644,
      "loss": 0.718,
      "step": 147
    },
    {
      "epoch": 1.3235955056179776,
      "grad_norm": 0.4823002517223358,
      "learning_rate": 0.0001714572864321608,
      "loss": 0.7348,
      "step": 148
    },
    {
      "epoch": 1.3325842696629213,
      "grad_norm": 0.48722559213638306,
      "learning_rate": 0.00017125628140703518,
      "loss": 0.7277,
      "step": 149
    },
    {
      "epoch": 1.3415730337078653,
      "grad_norm": 0.45071959495544434,
      "learning_rate": 0.00017105527638190955,
      "loss": 0.6373,
      "step": 150
    },
    {
      "epoch": 1.350561797752809,
      "grad_norm": 0.4802282452583313,
      "learning_rate": 0.00017085427135678393,
      "loss": 0.6944,
      "step": 151
    },
    {
      "epoch": 1.3595505617977528,
      "grad_norm": 0.47302353382110596,
      "learning_rate": 0.0001706532663316583,
      "loss": 0.6844,
      "step": 152
    },
    {
      "epoch": 1.3685393258426966,
      "grad_norm": 0.5128833651542664,
      "learning_rate": 0.00017045226130653267,
      "loss": 0.7695,
      "step": 153
    },
    {
      "epoch": 1.3775280898876405,
      "grad_norm": 0.4940306842327118,
      "learning_rate": 0.00017025125628140705,
      "loss": 0.6969,
      "step": 154
    },
    {
      "epoch": 1.3865168539325843,
      "grad_norm": 0.4941319227218628,
      "learning_rate": 0.00017005025125628142,
      "loss": 0.7141,
      "step": 155
    },
    {
      "epoch": 1.395505617977528,
      "grad_norm": 0.4479774236679077,
      "learning_rate": 0.0001698492462311558,
      "loss": 0.5948,
      "step": 156
    },
    {
      "epoch": 1.404494382022472,
      "grad_norm": 0.4846738576889038,
      "learning_rate": 0.00016964824120603016,
      "loss": 0.6525,
      "step": 157
    },
    {
      "epoch": 1.4134831460674158,
      "grad_norm": 0.5554487705230713,
      "learning_rate": 0.0001694472361809045,
      "loss": 0.7579,
      "step": 158
    },
    {
      "epoch": 1.4224719101123595,
      "grad_norm": 0.5085360407829285,
      "learning_rate": 0.0001692462311557789,
      "loss": 0.7527,
      "step": 159
    },
    {
      "epoch": 1.4314606741573033,
      "grad_norm": 0.5170609951019287,
      "learning_rate": 0.00016904522613065328,
      "loss": 0.7776,
      "step": 160
    },
    {
      "epoch": 1.4404494382022472,
      "grad_norm": 0.5048285722732544,
      "learning_rate": 0.00016884422110552766,
      "loss": 0.7281,
      "step": 161
    },
    {
      "epoch": 1.449438202247191,
      "grad_norm": 0.507969081401825,
      "learning_rate": 0.000168643216080402,
      "loss": 0.6509,
      "step": 162
    },
    {
      "epoch": 1.4584269662921348,
      "grad_norm": 0.5137774348258972,
      "learning_rate": 0.0001684422110552764,
      "loss": 0.766,
      "step": 163
    },
    {
      "epoch": 1.4674157303370787,
      "grad_norm": 0.5057291388511658,
      "learning_rate": 0.00016824120603015078,
      "loss": 0.6835,
      "step": 164
    },
    {
      "epoch": 1.4764044943820225,
      "grad_norm": 0.4965907037258148,
      "learning_rate": 0.00016804020100502512,
      "loss": 0.6177,
      "step": 165
    },
    {
      "epoch": 1.4853932584269662,
      "grad_norm": 0.5021895170211792,
      "learning_rate": 0.0001678391959798995,
      "loss": 0.6733,
      "step": 166
    },
    {
      "epoch": 1.49438202247191,
      "grad_norm": 0.5060408711433411,
      "learning_rate": 0.0001676381909547739,
      "loss": 0.5612,
      "step": 167
    },
    {
      "epoch": 1.503370786516854,
      "grad_norm": 0.5448092818260193,
      "learning_rate": 0.00016743718592964827,
      "loss": 0.6875,
      "step": 168
    },
    {
      "epoch": 1.5123595505617977,
      "grad_norm": 0.5072811245918274,
      "learning_rate": 0.0001672361809045226,
      "loss": 0.6463,
      "step": 169
    },
    {
      "epoch": 1.5213483146067417,
      "grad_norm": 0.518211305141449,
      "learning_rate": 0.00016703517587939699,
      "loss": 0.6449,
      "step": 170
    },
    {
      "epoch": 1.5303370786516854,
      "grad_norm": 0.4931667745113373,
      "learning_rate": 0.00016683417085427136,
      "loss": 0.6912,
      "step": 171
    },
    {
      "epoch": 1.5393258426966292,
      "grad_norm": 0.49399125576019287,
      "learning_rate": 0.00016663316582914573,
      "loss": 0.7422,
      "step": 172
    },
    {
      "epoch": 1.548314606741573,
      "grad_norm": 0.4899473488330841,
      "learning_rate": 0.0001664321608040201,
      "loss": 0.8077,
      "step": 173
    },
    {
      "epoch": 1.5573033707865167,
      "grad_norm": 0.5195055603981018,
      "learning_rate": 0.00016623115577889448,
      "loss": 0.6577,
      "step": 174
    },
    {
      "epoch": 1.5662921348314607,
      "grad_norm": 0.5048450231552124,
      "learning_rate": 0.00016603015075376885,
      "loss": 0.687,
      "step": 175
    },
    {
      "epoch": 1.5752808988764047,
      "grad_norm": 0.48674046993255615,
      "learning_rate": 0.00016582914572864322,
      "loss": 0.7065,
      "step": 176
    },
    {
      "epoch": 1.5842696629213484,
      "grad_norm": 0.5055518746376038,
      "learning_rate": 0.0001656281407035176,
      "loss": 0.7426,
      "step": 177
    },
    {
      "epoch": 1.5932584269662922,
      "grad_norm": 0.4986704885959625,
      "learning_rate": 0.00016542713567839197,
      "loss": 0.6988,
      "step": 178
    },
    {
      "epoch": 1.602247191011236,
      "grad_norm": 0.5376096963882446,
      "learning_rate": 0.00016522613065326634,
      "loss": 0.6533,
      "step": 179
    },
    {
      "epoch": 1.6112359550561797,
      "grad_norm": 0.48395565152168274,
      "learning_rate": 0.00016502512562814072,
      "loss": 0.6976,
      "step": 180
    },
    {
      "epoch": 1.6202247191011236,
      "grad_norm": 0.5291516780853271,
      "learning_rate": 0.0001648241206030151,
      "loss": 0.7121,
      "step": 181
    },
    {
      "epoch": 1.6292134831460674,
      "grad_norm": 0.4831722378730774,
      "learning_rate": 0.00016462311557788946,
      "loss": 0.7012,
      "step": 182
    },
    {
      "epoch": 1.6382022471910114,
      "grad_norm": 0.5175377130508423,
      "learning_rate": 0.0001644221105527638,
      "loss": 0.7017,
      "step": 183
    },
    {
      "epoch": 1.6471910112359551,
      "grad_norm": 0.5111058950424194,
      "learning_rate": 0.0001642211055276382,
      "loss": 0.704,
      "step": 184
    },
    {
      "epoch": 1.6561797752808989,
      "grad_norm": 0.4826604127883911,
      "learning_rate": 0.00016402010050251258,
      "loss": 0.6078,
      "step": 185
    },
    {
      "epoch": 1.6651685393258426,
      "grad_norm": 0.5259034633636475,
      "learning_rate": 0.00016381909547738695,
      "loss": 0.7103,
      "step": 186
    },
    {
      "epoch": 1.6741573033707864,
      "grad_norm": 0.5101408362388611,
      "learning_rate": 0.0001636180904522613,
      "loss": 0.7293,
      "step": 187
    },
    {
      "epoch": 1.6831460674157304,
      "grad_norm": 0.5450435280799866,
      "learning_rate": 0.0001634170854271357,
      "loss": 0.5243,
      "step": 188
    },
    {
      "epoch": 1.6921348314606741,
      "grad_norm": 0.5203365087509155,
      "learning_rate": 0.00016321608040201007,
      "loss": 0.6445,
      "step": 189
    },
    {
      "epoch": 1.701123595505618,
      "grad_norm": 0.527142345905304,
      "learning_rate": 0.00016301507537688442,
      "loss": 0.6936,
      "step": 190
    },
    {
      "epoch": 1.7101123595505618,
      "grad_norm": 0.49742498993873596,
      "learning_rate": 0.0001628140703517588,
      "loss": 0.6219,
      "step": 191
    },
    {
      "epoch": 1.7191011235955056,
      "grad_norm": 0.5218608379364014,
      "learning_rate": 0.00016261306532663316,
      "loss": 0.6898,
      "step": 192
    },
    {
      "epoch": 1.7280898876404494,
      "grad_norm": 0.5369536280632019,
      "learning_rate": 0.00016241206030150756,
      "loss": 0.6533,
      "step": 193
    },
    {
      "epoch": 1.737078651685393,
      "grad_norm": 0.5223860740661621,
      "learning_rate": 0.0001622110552763819,
      "loss": 0.6607,
      "step": 194
    },
    {
      "epoch": 1.746067415730337,
      "grad_norm": 0.5324874520301819,
      "learning_rate": 0.00016201005025125628,
      "loss": 0.7002,
      "step": 195
    },
    {
      "epoch": 1.755056179775281,
      "grad_norm": 0.5073167681694031,
      "learning_rate": 0.00016180904522613066,
      "loss": 0.7446,
      "step": 196
    },
    {
      "epoch": 1.7640449438202248,
      "grad_norm": 0.4847022294998169,
      "learning_rate": 0.00016160804020100503,
      "loss": 0.7033,
      "step": 197
    },
    {
      "epoch": 1.7730337078651686,
      "grad_norm": 0.4975110590457916,
      "learning_rate": 0.0001614070351758794,
      "loss": 0.705,
      "step": 198
    },
    {
      "epoch": 1.7820224719101123,
      "grad_norm": 0.49392595887184143,
      "learning_rate": 0.00016120603015075378,
      "loss": 0.7073,
      "step": 199
    },
    {
      "epoch": 1.791011235955056,
      "grad_norm": 0.4920112192630768,
      "learning_rate": 0.00016100502512562815,
      "loss": 0.644,
      "step": 200
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.5274104475975037,
      "learning_rate": 0.00016080402010050252,
      "loss": 0.6477,
      "step": 201
    },
    {
      "epoch": 1.8089887640449438,
      "grad_norm": 0.5122180581092834,
      "learning_rate": 0.0001606030150753769,
      "loss": 0.6666,
      "step": 202
    },
    {
      "epoch": 1.8179775280898878,
      "grad_norm": 0.44857776165008545,
      "learning_rate": 0.00016040201005025127,
      "loss": 0.5581,
      "step": 203
    },
    {
      "epoch": 1.8269662921348315,
      "grad_norm": 0.49341297149658203,
      "learning_rate": 0.00016020100502512564,
      "loss": 0.5232,
      "step": 204
    },
    {
      "epoch": 1.8359550561797753,
      "grad_norm": 0.5167354345321655,
      "learning_rate": 0.00016,
      "loss": 0.5907,
      "step": 205
    },
    {
      "epoch": 1.844943820224719,
      "grad_norm": 0.5707966685295105,
      "learning_rate": 0.00015979899497487439,
      "loss": 0.6816,
      "step": 206
    },
    {
      "epoch": 1.8539325842696628,
      "grad_norm": 0.5250276327133179,
      "learning_rate": 0.00015959798994974876,
      "loss": 0.6869,
      "step": 207
    },
    {
      "epoch": 1.8629213483146068,
      "grad_norm": 0.5712555050849915,
      "learning_rate": 0.0001593969849246231,
      "loss": 0.7121,
      "step": 208
    },
    {
      "epoch": 1.8719101123595505,
      "grad_norm": 0.550301730632782,
      "learning_rate": 0.0001591959798994975,
      "loss": 0.6475,
      "step": 209
    },
    {
      "epoch": 1.8808988764044945,
      "grad_norm": 0.4825449585914612,
      "learning_rate": 0.00015899497487437188,
      "loss": 0.6321,
      "step": 210
    },
    {
      "epoch": 1.8898876404494382,
      "grad_norm": 0.5767778158187866,
      "learning_rate": 0.00015879396984924625,
      "loss": 0.5749,
      "step": 211
    },
    {
      "epoch": 1.898876404494382,
      "grad_norm": 0.5126257538795471,
      "learning_rate": 0.0001585929648241206,
      "loss": 0.734,
      "step": 212
    },
    {
      "epoch": 1.9078651685393258,
      "grad_norm": 0.5008631944656372,
      "learning_rate": 0.000158391959798995,
      "loss": 0.6667,
      "step": 213
    },
    {
      "epoch": 1.9168539325842695,
      "grad_norm": 0.5043883323669434,
      "learning_rate": 0.00015819095477386937,
      "loss": 0.6787,
      "step": 214
    },
    {
      "epoch": 1.9258426966292135,
      "grad_norm": 0.4949117600917816,
      "learning_rate": 0.00015798994974874372,
      "loss": 0.6328,
      "step": 215
    },
    {
      "epoch": 1.9348314606741575,
      "grad_norm": 0.5707808136940002,
      "learning_rate": 0.0001577889447236181,
      "loss": 0.616,
      "step": 216
    },
    {
      "epoch": 1.9438202247191012,
      "grad_norm": 0.7143571972846985,
      "learning_rate": 0.00015758793969849246,
      "loss": 0.6714,
      "step": 217
    },
    {
      "epoch": 1.952808988764045,
      "grad_norm": 0.6252564191818237,
      "learning_rate": 0.00015738693467336686,
      "loss": 0.5917,
      "step": 218
    },
    {
      "epoch": 1.9617977528089887,
      "grad_norm": 0.5254539251327515,
      "learning_rate": 0.0001571859296482412,
      "loss": 0.6649,
      "step": 219
    },
    {
      "epoch": 1.9707865168539325,
      "grad_norm": 0.5377323627471924,
      "learning_rate": 0.00015698492462311558,
      "loss": 0.6644,
      "step": 220
    },
    {
      "epoch": 1.9797752808988764,
      "grad_norm": 0.5130908489227295,
      "learning_rate": 0.00015678391959798995,
      "loss": 0.5696,
      "step": 221
    },
    {
      "epoch": 1.9887640449438202,
      "grad_norm": 0.5565310716629028,
      "learning_rate": 0.00015658291457286433,
      "loss": 0.5736,
      "step": 222
    },
    {
      "epoch": 1.9977528089887642,
      "grad_norm": 0.4949910044670105,
      "learning_rate": 0.0001563819095477387,
      "loss": 0.6281,
      "step": 223
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.0195002555847168,
      "learning_rate": 0.00015618090452261307,
      "loss": 0.5102,
      "step": 224
    },
    {
      "epoch": 2.0089887640449438,
      "grad_norm": 0.4660240113735199,
      "learning_rate": 0.00015597989949748745,
      "loss": 0.5222,
      "step": 225
    },
    {
      "epoch": 2.0179775280898875,
      "grad_norm": 0.5553634762763977,
      "learning_rate": 0.00015577889447236182,
      "loss": 0.4074,
      "step": 226
    },
    {
      "epoch": 2.0269662921348313,
      "grad_norm": 0.5150858163833618,
      "learning_rate": 0.0001555778894472362,
      "loss": 0.5437,
      "step": 227
    },
    {
      "epoch": 2.0359550561797755,
      "grad_norm": 0.5600498914718628,
      "learning_rate": 0.00015537688442211056,
      "loss": 0.521,
      "step": 228
    },
    {
      "epoch": 2.044943820224719,
      "grad_norm": 0.5323230028152466,
      "learning_rate": 0.00015517587939698494,
      "loss": 0.5334,
      "step": 229
    },
    {
      "epoch": 2.053932584269663,
      "grad_norm": 0.5573216080665588,
      "learning_rate": 0.0001549748743718593,
      "loss": 0.5199,
      "step": 230
    },
    {
      "epoch": 2.0629213483146067,
      "grad_norm": 0.6677011847496033,
      "learning_rate": 0.00015477386934673368,
      "loss": 0.5506,
      "step": 231
    },
    {
      "epoch": 2.0719101123595505,
      "grad_norm": 0.6032688617706299,
      "learning_rate": 0.00015457286432160806,
      "loss": 0.6113,
      "step": 232
    },
    {
      "epoch": 2.0808988764044942,
      "grad_norm": 0.7408851981163025,
      "learning_rate": 0.0001543718592964824,
      "loss": 0.4487,
      "step": 233
    },
    {
      "epoch": 2.0898876404494384,
      "grad_norm": 0.8626886010169983,
      "learning_rate": 0.0001541708542713568,
      "loss": 0.6083,
      "step": 234
    },
    {
      "epoch": 2.098876404494382,
      "grad_norm": 0.6213219165802002,
      "learning_rate": 0.00015396984924623117,
      "loss": 0.5407,
      "step": 235
    },
    {
      "epoch": 2.107865168539326,
      "grad_norm": 0.6023510098457336,
      "learning_rate": 0.00015376884422110555,
      "loss": 0.5464,
      "step": 236
    },
    {
      "epoch": 2.1168539325842697,
      "grad_norm": 0.5395693778991699,
      "learning_rate": 0.0001535678391959799,
      "loss": 0.4975,
      "step": 237
    },
    {
      "epoch": 2.1258426966292134,
      "grad_norm": 0.5109840631484985,
      "learning_rate": 0.00015336683417085427,
      "loss": 0.4496,
      "step": 238
    },
    {
      "epoch": 2.134831460674157,
      "grad_norm": 0.5461961627006531,
      "learning_rate": 0.00015316582914572867,
      "loss": 0.4442,
      "step": 239
    },
    {
      "epoch": 2.143820224719101,
      "grad_norm": 0.5748690366744995,
      "learning_rate": 0.000152964824120603,
      "loss": 0.5014,
      "step": 240
    },
    {
      "epoch": 2.152808988764045,
      "grad_norm": 0.5407891273498535,
      "learning_rate": 0.00015276381909547739,
      "loss": 0.4825,
      "step": 241
    },
    {
      "epoch": 2.161797752808989,
      "grad_norm": 0.5855742692947388,
      "learning_rate": 0.00015256281407035176,
      "loss": 0.4442,
      "step": 242
    },
    {
      "epoch": 2.1707865168539326,
      "grad_norm": 0.5812053084373474,
      "learning_rate": 0.00015236180904522613,
      "loss": 0.5011,
      "step": 243
    },
    {
      "epoch": 2.1797752808988764,
      "grad_norm": 0.6219434142112732,
      "learning_rate": 0.0001521608040201005,
      "loss": 0.5654,
      "step": 244
    },
    {
      "epoch": 2.18876404494382,
      "grad_norm": 0.6690217852592468,
      "learning_rate": 0.00015195979899497488,
      "loss": 0.5959,
      "step": 245
    },
    {
      "epoch": 2.197752808988764,
      "grad_norm": 0.615625262260437,
      "learning_rate": 0.00015175879396984925,
      "loss": 0.5846,
      "step": 246
    },
    {
      "epoch": 2.2067415730337077,
      "grad_norm": 0.5975291728973389,
      "learning_rate": 0.00015155778894472362,
      "loss": 0.4663,
      "step": 247
    },
    {
      "epoch": 2.215730337078652,
      "grad_norm": 0.5664569139480591,
      "learning_rate": 0.000151356783919598,
      "loss": 0.4738,
      "step": 248
    },
    {
      "epoch": 2.2247191011235956,
      "grad_norm": 0.6067225933074951,
      "learning_rate": 0.00015115577889447237,
      "loss": 0.545,
      "step": 249
    },
    {
      "epoch": 2.2337078651685394,
      "grad_norm": 0.6892182230949402,
      "learning_rate": 0.00015095477386934674,
      "loss": 0.4804,
      "step": 250
    },
    {
      "epoch": 2.242696629213483,
      "grad_norm": 0.654926061630249,
      "learning_rate": 0.00015075376884422112,
      "loss": 0.4978,
      "step": 251
    },
    {
      "epoch": 2.251685393258427,
      "grad_norm": 0.5852486491203308,
      "learning_rate": 0.0001505527638190955,
      "loss": 0.5754,
      "step": 252
    },
    {
      "epoch": 2.2606741573033706,
      "grad_norm": 0.5744386315345764,
      "learning_rate": 0.00015035175879396986,
      "loss": 0.4953,
      "step": 253
    },
    {
      "epoch": 2.2696629213483144,
      "grad_norm": 0.5505176186561584,
      "learning_rate": 0.00015015075376884423,
      "loss": 0.4115,
      "step": 254
    },
    {
      "epoch": 2.2786516853932586,
      "grad_norm": 0.6617196202278137,
      "learning_rate": 0.0001499497487437186,
      "loss": 0.4522,
      "step": 255
    },
    {
      "epoch": 2.2876404494382023,
      "grad_norm": 0.5835797190666199,
      "learning_rate": 0.00014974874371859298,
      "loss": 0.5154,
      "step": 256
    },
    {
      "epoch": 2.296629213483146,
      "grad_norm": 0.629218578338623,
      "learning_rate": 0.00014954773869346735,
      "loss": 0.5049,
      "step": 257
    },
    {
      "epoch": 2.30561797752809,
      "grad_norm": 0.6005010008811951,
      "learning_rate": 0.0001493467336683417,
      "loss": 0.515,
      "step": 258
    },
    {
      "epoch": 2.3146067415730336,
      "grad_norm": 0.7216350436210632,
      "learning_rate": 0.0001491457286432161,
      "loss": 0.6126,
      "step": 259
    },
    {
      "epoch": 2.3235955056179773,
      "grad_norm": 0.6677060723304749,
      "learning_rate": 0.00014894472361809047,
      "loss": 0.5323,
      "step": 260
    },
    {
      "epoch": 2.3325842696629215,
      "grad_norm": 0.5940092206001282,
      "learning_rate": 0.00014874371859296482,
      "loss": 0.4089,
      "step": 261
    },
    {
      "epoch": 2.3415730337078653,
      "grad_norm": 0.599693238735199,
      "learning_rate": 0.0001485427135678392,
      "loss": 0.5056,
      "step": 262
    },
    {
      "epoch": 2.350561797752809,
      "grad_norm": 0.6471900343894958,
      "learning_rate": 0.00014834170854271356,
      "loss": 0.56,
      "step": 263
    },
    {
      "epoch": 2.359550561797753,
      "grad_norm": 0.6030563116073608,
      "learning_rate": 0.00014814070351758796,
      "loss": 0.5402,
      "step": 264
    },
    {
      "epoch": 2.3685393258426966,
      "grad_norm": 0.6223505735397339,
      "learning_rate": 0.0001479396984924623,
      "loss": 0.4698,
      "step": 265
    },
    {
      "epoch": 2.3775280898876403,
      "grad_norm": 0.6378539800643921,
      "learning_rate": 0.00014773869346733668,
      "loss": 0.619,
      "step": 266
    },
    {
      "epoch": 2.3865168539325845,
      "grad_norm": 0.6360874176025391,
      "learning_rate": 0.00014753768844221106,
      "loss": 0.478,
      "step": 267
    },
    {
      "epoch": 2.3955056179775283,
      "grad_norm": 0.6217352747917175,
      "learning_rate": 0.00014733668341708543,
      "loss": 0.541,
      "step": 268
    },
    {
      "epoch": 2.404494382022472,
      "grad_norm": 0.587964653968811,
      "learning_rate": 0.0001471356783919598,
      "loss": 0.5041,
      "step": 269
    },
    {
      "epoch": 2.4134831460674158,
      "grad_norm": 0.5437304377555847,
      "learning_rate": 0.00014693467336683417,
      "loss": 0.4398,
      "step": 270
    },
    {
      "epoch": 2.4224719101123595,
      "grad_norm": 0.6016120910644531,
      "learning_rate": 0.00014673366834170855,
      "loss": 0.4713,
      "step": 271
    },
    {
      "epoch": 2.4314606741573033,
      "grad_norm": 0.6375266909599304,
      "learning_rate": 0.00014653266331658292,
      "loss": 0.4592,
      "step": 272
    },
    {
      "epoch": 2.440449438202247,
      "grad_norm": 0.6722357273101807,
      "learning_rate": 0.0001463316582914573,
      "loss": 0.5549,
      "step": 273
    },
    {
      "epoch": 2.449438202247191,
      "grad_norm": 0.6131983399391174,
      "learning_rate": 0.00014613065326633167,
      "loss": 0.567,
      "step": 274
    },
    {
      "epoch": 2.458426966292135,
      "grad_norm": 0.6151455044746399,
      "learning_rate": 0.00014592964824120604,
      "loss": 0.5816,
      "step": 275
    },
    {
      "epoch": 2.4674157303370787,
      "grad_norm": 0.6787477135658264,
      "learning_rate": 0.0001457286432160804,
      "loss": 0.4874,
      "step": 276
    },
    {
      "epoch": 2.4764044943820225,
      "grad_norm": 0.5861737132072449,
      "learning_rate": 0.00014552763819095479,
      "loss": 0.515,
      "step": 277
    },
    {
      "epoch": 2.4853932584269662,
      "grad_norm": 0.6954892873764038,
      "learning_rate": 0.00014532663316582916,
      "loss": 0.4164,
      "step": 278
    },
    {
      "epoch": 2.49438202247191,
      "grad_norm": 0.5795653462409973,
      "learning_rate": 0.00014512562814070353,
      "loss": 0.3879,
      "step": 279
    },
    {
      "epoch": 2.5033707865168537,
      "grad_norm": 0.6603276133537292,
      "learning_rate": 0.0001449246231155779,
      "loss": 0.5487,
      "step": 280
    },
    {
      "epoch": 2.512359550561798,
      "grad_norm": 0.5875048637390137,
      "learning_rate": 0.00014472361809045228,
      "loss": 0.4917,
      "step": 281
    },
    {
      "epoch": 2.5213483146067417,
      "grad_norm": 0.6392641067504883,
      "learning_rate": 0.00014452261306532665,
      "loss": 0.568,
      "step": 282
    },
    {
      "epoch": 2.5303370786516854,
      "grad_norm": 0.6484583020210266,
      "learning_rate": 0.000144321608040201,
      "loss": 0.4412,
      "step": 283
    },
    {
      "epoch": 2.539325842696629,
      "grad_norm": 0.6449112892150879,
      "learning_rate": 0.00014412060301507537,
      "loss": 0.5526,
      "step": 284
    },
    {
      "epoch": 2.548314606741573,
      "grad_norm": 0.6698688864707947,
      "learning_rate": 0.00014391959798994977,
      "loss": 0.5832,
      "step": 285
    },
    {
      "epoch": 2.5573033707865167,
      "grad_norm": 0.6654890775680542,
      "learning_rate": 0.00014371859296482411,
      "loss": 0.4937,
      "step": 286
    },
    {
      "epoch": 2.5662921348314605,
      "grad_norm": 0.6368288397789001,
      "learning_rate": 0.0001435175879396985,
      "loss": 0.6144,
      "step": 287
    },
    {
      "epoch": 2.5752808988764047,
      "grad_norm": 0.5983182191848755,
      "learning_rate": 0.00014331658291457286,
      "loss": 0.5085,
      "step": 288
    },
    {
      "epoch": 2.5842696629213484,
      "grad_norm": 0.580386221408844,
      "learning_rate": 0.00014311557788944726,
      "loss": 0.4726,
      "step": 289
    },
    {
      "epoch": 2.593258426966292,
      "grad_norm": 0.7215433716773987,
      "learning_rate": 0.0001429145728643216,
      "loss": 0.4915,
      "step": 290
    },
    {
      "epoch": 2.602247191011236,
      "grad_norm": 0.5611022114753723,
      "learning_rate": 0.00014271356783919598,
      "loss": 0.4294,
      "step": 291
    },
    {
      "epoch": 2.6112359550561797,
      "grad_norm": 0.5867642760276794,
      "learning_rate": 0.00014251256281407035,
      "loss": 0.5665,
      "step": 292
    },
    {
      "epoch": 2.620224719101124,
      "grad_norm": 0.6344872713088989,
      "learning_rate": 0.00014231155778894473,
      "loss": 0.4829,
      "step": 293
    },
    {
      "epoch": 2.629213483146067,
      "grad_norm": 0.6000683903694153,
      "learning_rate": 0.0001421105527638191,
      "loss": 0.3992,
      "step": 294
    },
    {
      "epoch": 2.6382022471910114,
      "grad_norm": 0.6512176394462585,
      "learning_rate": 0.00014190954773869347,
      "loss": 0.5714,
      "step": 295
    },
    {
      "epoch": 2.647191011235955,
      "grad_norm": 0.7028009295463562,
      "learning_rate": 0.00014170854271356784,
      "loss": 0.5783,
      "step": 296
    },
    {
      "epoch": 2.656179775280899,
      "grad_norm": 0.6494923233985901,
      "learning_rate": 0.00014150753768844222,
      "loss": 0.5988,
      "step": 297
    },
    {
      "epoch": 2.6651685393258426,
      "grad_norm": 0.5978695750236511,
      "learning_rate": 0.0001413065326633166,
      "loss": 0.5985,
      "step": 298
    },
    {
      "epoch": 2.6741573033707864,
      "grad_norm": 0.5788876414299011,
      "learning_rate": 0.00014110552763819096,
      "loss": 0.4572,
      "step": 299
    },
    {
      "epoch": 2.6831460674157306,
      "grad_norm": 0.6566181778907776,
      "learning_rate": 0.00014090452261306534,
      "loss": 0.5156,
      "step": 300
    },
    {
      "epoch": 2.692134831460674,
      "grad_norm": 0.6133995056152344,
      "learning_rate": 0.0001407035175879397,
      "loss": 0.3775,
      "step": 301
    },
    {
      "epoch": 2.701123595505618,
      "grad_norm": 0.7124189734458923,
      "learning_rate": 0.00014050251256281408,
      "loss": 0.5253,
      "step": 302
    },
    {
      "epoch": 2.710112359550562,
      "grad_norm": 0.6259135603904724,
      "learning_rate": 0.00014030150753768846,
      "loss": 0.4731,
      "step": 303
    },
    {
      "epoch": 2.7191011235955056,
      "grad_norm": 0.623723030090332,
      "learning_rate": 0.0001401005025125628,
      "loss": 0.5035,
      "step": 304
    },
    {
      "epoch": 2.7280898876404494,
      "grad_norm": 0.6508894562721252,
      "learning_rate": 0.0001398994974874372,
      "loss": 0.4888,
      "step": 305
    },
    {
      "epoch": 2.737078651685393,
      "grad_norm": 0.8689988851547241,
      "learning_rate": 0.00013969849246231157,
      "loss": 0.5793,
      "step": 306
    },
    {
      "epoch": 2.7460674157303373,
      "grad_norm": 0.684623122215271,
      "learning_rate": 0.00013949748743718595,
      "loss": 0.4543,
      "step": 307
    },
    {
      "epoch": 2.755056179775281,
      "grad_norm": 0.6090950965881348,
      "learning_rate": 0.0001392964824120603,
      "loss": 0.5538,
      "step": 308
    },
    {
      "epoch": 2.764044943820225,
      "grad_norm": 0.5619356632232666,
      "learning_rate": 0.00013909547738693467,
      "loss": 0.4644,
      "step": 309
    },
    {
      "epoch": 2.7730337078651686,
      "grad_norm": 0.6681782603263855,
      "learning_rate": 0.00013889447236180907,
      "loss": 0.548,
      "step": 310
    },
    {
      "epoch": 2.7820224719101123,
      "grad_norm": 0.6274324059486389,
      "learning_rate": 0.0001386934673366834,
      "loss": 0.5981,
      "step": 311
    },
    {
      "epoch": 2.791011235955056,
      "grad_norm": 0.5922242403030396,
      "learning_rate": 0.00013849246231155778,
      "loss": 0.5413,
      "step": 312
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.6123243570327759,
      "learning_rate": 0.00013829145728643216,
      "loss": 0.5823,
      "step": 313
    },
    {
      "epoch": 2.808988764044944,
      "grad_norm": 0.5891364812850952,
      "learning_rate": 0.00013809045226130656,
      "loss": 0.5406,
      "step": 314
    },
    {
      "epoch": 2.8179775280898878,
      "grad_norm": 0.5824383497238159,
      "learning_rate": 0.0001378894472361809,
      "loss": 0.5467,
      "step": 315
    },
    {
      "epoch": 2.8269662921348315,
      "grad_norm": 0.6133068799972534,
      "learning_rate": 0.00013768844221105528,
      "loss": 0.4834,
      "step": 316
    },
    {
      "epoch": 2.8359550561797753,
      "grad_norm": 0.6003114581108093,
      "learning_rate": 0.00013748743718592965,
      "loss": 0.4825,
      "step": 317
    },
    {
      "epoch": 2.844943820224719,
      "grad_norm": 0.6441670656204224,
      "learning_rate": 0.00013728643216080402,
      "loss": 0.5443,
      "step": 318
    },
    {
      "epoch": 2.853932584269663,
      "grad_norm": 0.5923592448234558,
      "learning_rate": 0.0001370854271356784,
      "loss": 0.3885,
      "step": 319
    },
    {
      "epoch": 2.8629213483146065,
      "grad_norm": 0.6361895799636841,
      "learning_rate": 0.00013688442211055277,
      "loss": 0.5519,
      "step": 320
    },
    {
      "epoch": 2.8719101123595507,
      "grad_norm": 0.6282822489738464,
      "learning_rate": 0.00013668341708542714,
      "loss": 0.4749,
      "step": 321
    },
    {
      "epoch": 2.8808988764044945,
      "grad_norm": 0.6911002993583679,
      "learning_rate": 0.00013648241206030151,
      "loss": 0.5507,
      "step": 322
    },
    {
      "epoch": 2.8898876404494382,
      "grad_norm": 0.6519534587860107,
      "learning_rate": 0.0001362814070351759,
      "loss": 0.5251,
      "step": 323
    },
    {
      "epoch": 2.898876404494382,
      "grad_norm": 0.568443775177002,
      "learning_rate": 0.00013608040201005026,
      "loss": 0.4177,
      "step": 324
    },
    {
      "epoch": 2.9078651685393258,
      "grad_norm": 0.6429846882820129,
      "learning_rate": 0.00013587939698492463,
      "loss": 0.578,
      "step": 325
    },
    {
      "epoch": 2.9168539325842695,
      "grad_norm": 0.6556768417358398,
      "learning_rate": 0.000135678391959799,
      "loss": 0.5041,
      "step": 326
    },
    {
      "epoch": 2.9258426966292133,
      "grad_norm": 0.6875482797622681,
      "learning_rate": 0.00013547738693467338,
      "loss": 0.5296,
      "step": 327
    },
    {
      "epoch": 2.9348314606741575,
      "grad_norm": 0.6077708601951599,
      "learning_rate": 0.00013527638190954775,
      "loss": 0.4493,
      "step": 328
    },
    {
      "epoch": 2.943820224719101,
      "grad_norm": 0.6266283392906189,
      "learning_rate": 0.0001350753768844221,
      "loss": 0.5076,
      "step": 329
    },
    {
      "epoch": 2.952808988764045,
      "grad_norm": 0.5939067602157593,
      "learning_rate": 0.00013487437185929647,
      "loss": 0.4287,
      "step": 330
    },
    {
      "epoch": 2.9617977528089887,
      "grad_norm": 0.6024183630943298,
      "learning_rate": 0.00013467336683417087,
      "loss": 0.5725,
      "step": 331
    },
    {
      "epoch": 2.9707865168539325,
      "grad_norm": 0.6141772866249084,
      "learning_rate": 0.00013447236180904524,
      "loss": 0.6012,
      "step": 332
    },
    {
      "epoch": 2.9797752808988767,
      "grad_norm": 0.6132792830467224,
      "learning_rate": 0.0001342713567839196,
      "loss": 0.5303,
      "step": 333
    },
    {
      "epoch": 2.98876404494382,
      "grad_norm": 0.6210426092147827,
      "learning_rate": 0.00013407035175879396,
      "loss": 0.5591,
      "step": 334
    },
    {
      "epoch": 2.997752808988764,
      "grad_norm": 0.604884147644043,
      "learning_rate": 0.00013386934673366836,
      "loss": 0.4974,
      "step": 335
    },
    {
      "epoch": 3.0,
      "grad_norm": 1.146360993385315,
      "learning_rate": 0.0001336683417085427,
      "loss": 0.4953,
      "step": 336
    },
    {
      "epoch": 3.0089887640449438,
      "grad_norm": 0.5554021000862122,
      "learning_rate": 0.00013346733668341708,
      "loss": 0.3507,
      "step": 337
    },
    {
      "epoch": 3.0179775280898875,
      "grad_norm": 0.5963609218597412,
      "learning_rate": 0.00013326633165829146,
      "loss": 0.3894,
      "step": 338
    },
    {
      "epoch": 3.0269662921348313,
      "grad_norm": 0.6020979285240173,
      "learning_rate": 0.00013306532663316586,
      "loss": 0.3324,
      "step": 339
    },
    {
      "epoch": 3.0359550561797755,
      "grad_norm": 0.6343191862106323,
      "learning_rate": 0.0001328643216080402,
      "loss": 0.3224,
      "step": 340
    },
    {
      "epoch": 3.044943820224719,
      "grad_norm": 0.6305267214775085,
      "learning_rate": 0.00013266331658291457,
      "loss": 0.3868,
      "step": 341
    },
    {
      "epoch": 3.053932584269663,
      "grad_norm": 0.7155238389968872,
      "learning_rate": 0.00013246231155778895,
      "loss": 0.3228,
      "step": 342
    },
    {
      "epoch": 3.0629213483146067,
      "grad_norm": 0.7738904356956482,
      "learning_rate": 0.00013226130653266332,
      "loss": 0.3558,
      "step": 343
    },
    {
      "epoch": 3.0719101123595505,
      "grad_norm": 0.7337636351585388,
      "learning_rate": 0.0001320603015075377,
      "loss": 0.4103,
      "step": 344
    },
    {
      "epoch": 3.0808988764044942,
      "grad_norm": 0.638300359249115,
      "learning_rate": 0.00013185929648241207,
      "loss": 0.2855,
      "step": 345
    },
    {
      "epoch": 3.0898876404494384,
      "grad_norm": 0.7031756639480591,
      "learning_rate": 0.00013165829145728644,
      "loss": 0.2787,
      "step": 346
    },
    {
      "epoch": 3.098876404494382,
      "grad_norm": 0.6939449310302734,
      "learning_rate": 0.0001314572864321608,
      "loss": 0.3155,
      "step": 347
    },
    {
      "epoch": 3.107865168539326,
      "grad_norm": 0.7217143177986145,
      "learning_rate": 0.00013125628140703518,
      "loss": 0.3563,
      "step": 348
    },
    {
      "epoch": 3.1168539325842697,
      "grad_norm": 0.7956618666648865,
      "learning_rate": 0.00013105527638190956,
      "loss": 0.3679,
      "step": 349
    },
    {
      "epoch": 3.1258426966292134,
      "grad_norm": 0.7286489605903625,
      "learning_rate": 0.00013085427135678393,
      "loss": 0.3831,
      "step": 350
    },
    {
      "epoch": 3.134831460674157,
      "grad_norm": 0.8053686022758484,
      "learning_rate": 0.0001306532663316583,
      "loss": 0.3743,
      "step": 351
    },
    {
      "epoch": 3.143820224719101,
      "grad_norm": 0.7931649684906006,
      "learning_rate": 0.00013045226130653268,
      "loss": 0.4057,
      "step": 352
    },
    {
      "epoch": 3.152808988764045,
      "grad_norm": 0.6974821090698242,
      "learning_rate": 0.00013025125628140705,
      "loss": 0.3598,
      "step": 353
    },
    {
      "epoch": 3.161797752808989,
      "grad_norm": 0.7493677139282227,
      "learning_rate": 0.0001300502512562814,
      "loss": 0.3893,
      "step": 354
    },
    {
      "epoch": 3.1707865168539326,
      "grad_norm": 0.7561050653457642,
      "learning_rate": 0.00012984924623115577,
      "loss": 0.3687,
      "step": 355
    },
    {
      "epoch": 3.1797752808988764,
      "grad_norm": 0.7245007157325745,
      "learning_rate": 0.00012964824120603017,
      "loss": 0.3343,
      "step": 356
    },
    {
      "epoch": 3.18876404494382,
      "grad_norm": 0.6741995811462402,
      "learning_rate": 0.00012944723618090454,
      "loss": 0.3753,
      "step": 357
    },
    {
      "epoch": 3.197752808988764,
      "grad_norm": 0.7612851858139038,
      "learning_rate": 0.0001292462311557789,
      "loss": 0.3518,
      "step": 358
    },
    {
      "epoch": 3.2067415730337077,
      "grad_norm": 0.7604615688323975,
      "learning_rate": 0.00012904522613065326,
      "loss": 0.3425,
      "step": 359
    },
    {
      "epoch": 3.215730337078652,
      "grad_norm": 0.7310434579849243,
      "learning_rate": 0.00012884422110552766,
      "loss": 0.3441,
      "step": 360
    },
    {
      "epoch": 3.2247191011235956,
      "grad_norm": 0.7486586570739746,
      "learning_rate": 0.000128643216080402,
      "loss": 0.3041,
      "step": 361
    },
    {
      "epoch": 3.2337078651685394,
      "grad_norm": 0.7074959874153137,
      "learning_rate": 0.00012844221105527638,
      "loss": 0.3567,
      "step": 362
    },
    {
      "epoch": 3.242696629213483,
      "grad_norm": 0.7554347515106201,
      "learning_rate": 0.00012824120603015075,
      "loss": 0.2938,
      "step": 363
    },
    {
      "epoch": 3.251685393258427,
      "grad_norm": 0.8878810405731201,
      "learning_rate": 0.00012804020100502515,
      "loss": 0.4509,
      "step": 364
    },
    {
      "epoch": 3.2606741573033706,
      "grad_norm": 0.849636435508728,
      "learning_rate": 0.0001278391959798995,
      "loss": 0.4112,
      "step": 365
    },
    {
      "epoch": 3.2696629213483144,
      "grad_norm": 0.7759038209915161,
      "learning_rate": 0.00012763819095477387,
      "loss": 0.3778,
      "step": 366
    },
    {
      "epoch": 3.2786516853932586,
      "grad_norm": 0.7177848815917969,
      "learning_rate": 0.00012743718592964824,
      "loss": 0.3334,
      "step": 367
    },
    {
      "epoch": 3.2876404494382023,
      "grad_norm": 0.7122929096221924,
      "learning_rate": 0.00012723618090452262,
      "loss": 0.3364,
      "step": 368
    },
    {
      "epoch": 3.296629213483146,
      "grad_norm": 0.8124998807907104,
      "learning_rate": 0.000127035175879397,
      "loss": 0.3188,
      "step": 369
    },
    {
      "epoch": 3.30561797752809,
      "grad_norm": 0.7098675966262817,
      "learning_rate": 0.00012683417085427136,
      "loss": 0.369,
      "step": 370
    },
    {
      "epoch": 3.3146067415730336,
      "grad_norm": 0.74213707447052,
      "learning_rate": 0.00012663316582914574,
      "loss": 0.4122,
      "step": 371
    },
    {
      "epoch": 3.3235955056179773,
      "grad_norm": 0.6893321871757507,
      "learning_rate": 0.0001264321608040201,
      "loss": 0.2779,
      "step": 372
    },
    {
      "epoch": 3.3325842696629215,
      "grad_norm": 0.719463050365448,
      "learning_rate": 0.00012623115577889448,
      "loss": 0.3726,
      "step": 373
    },
    {
      "epoch": 3.3415730337078653,
      "grad_norm": 0.7136383652687073,
      "learning_rate": 0.00012603015075376885,
      "loss": 0.3683,
      "step": 374
    },
    {
      "epoch": 3.350561797752809,
      "grad_norm": 0.743434727191925,
      "learning_rate": 0.00012582914572864323,
      "loss": 0.3665,
      "step": 375
    },
    {
      "epoch": 3.359550561797753,
      "grad_norm": 0.7832537293434143,
      "learning_rate": 0.0001256281407035176,
      "loss": 0.3686,
      "step": 376
    },
    {
      "epoch": 3.3685393258426966,
      "grad_norm": 0.7466504573822021,
      "learning_rate": 0.00012542713567839197,
      "loss": 0.3677,
      "step": 377
    },
    {
      "epoch": 3.3775280898876403,
      "grad_norm": 0.6690831184387207,
      "learning_rate": 0.00012522613065326635,
      "loss": 0.3013,
      "step": 378
    },
    {
      "epoch": 3.3865168539325845,
      "grad_norm": 0.756691575050354,
      "learning_rate": 0.0001250251256281407,
      "loss": 0.385,
      "step": 379
    },
    {
      "epoch": 3.3955056179775283,
      "grad_norm": 0.6948524117469788,
      "learning_rate": 0.00012482412060301507,
      "loss": 0.3517,
      "step": 380
    },
    {
      "epoch": 3.404494382022472,
      "grad_norm": 0.7330809235572815,
      "learning_rate": 0.00012462311557788947,
      "loss": 0.3494,
      "step": 381
    },
    {
      "epoch": 3.4134831460674158,
      "grad_norm": 0.7869780659675598,
      "learning_rate": 0.00012442211055276384,
      "loss": 0.4464,
      "step": 382
    },
    {
      "epoch": 3.4224719101123595,
      "grad_norm": 0.8178461194038391,
      "learning_rate": 0.00012422110552763818,
      "loss": 0.3412,
      "step": 383
    },
    {
      "epoch": 3.4314606741573033,
      "grad_norm": 0.7086721658706665,
      "learning_rate": 0.00012402010050251256,
      "loss": 0.3528,
      "step": 384
    },
    {
      "epoch": 3.440449438202247,
      "grad_norm": 0.7312350273132324,
      "learning_rate": 0.00012381909547738696,
      "loss": 0.3328,
      "step": 385
    },
    {
      "epoch": 3.449438202247191,
      "grad_norm": 0.7593942880630493,
      "learning_rate": 0.0001236180904522613,
      "loss": 0.3281,
      "step": 386
    },
    {
      "epoch": 3.458426966292135,
      "grad_norm": 0.7736111879348755,
      "learning_rate": 0.00012341708542713568,
      "loss": 0.4143,
      "step": 387
    },
    {
      "epoch": 3.4674157303370787,
      "grad_norm": 0.7553831934928894,
      "learning_rate": 0.00012321608040201005,
      "loss": 0.3764,
      "step": 388
    },
    {
      "epoch": 3.4764044943820225,
      "grad_norm": 0.6346025466918945,
      "learning_rate": 0.00012301507537688445,
      "loss": 0.3063,
      "step": 389
    },
    {
      "epoch": 3.4853932584269662,
      "grad_norm": 0.7386281490325928,
      "learning_rate": 0.0001228140703517588,
      "loss": 0.3583,
      "step": 390
    },
    {
      "epoch": 3.49438202247191,
      "grad_norm": 0.8871465921401978,
      "learning_rate": 0.00012261306532663317,
      "loss": 0.3872,
      "step": 391
    },
    {
      "epoch": 3.5033707865168537,
      "grad_norm": 0.7815625071525574,
      "learning_rate": 0.00012241206030150754,
      "loss": 0.4208,
      "step": 392
    },
    {
      "epoch": 3.512359550561798,
      "grad_norm": 0.6490662097930908,
      "learning_rate": 0.00012221105527638191,
      "loss": 0.3336,
      "step": 393
    },
    {
      "epoch": 3.5213483146067417,
      "grad_norm": 0.8376544117927551,
      "learning_rate": 0.00012201005025125629,
      "loss": 0.406,
      "step": 394
    },
    {
      "epoch": 3.5303370786516854,
      "grad_norm": 0.6983129978179932,
      "learning_rate": 0.00012180904522613066,
      "loss": 0.2934,
      "step": 395
    },
    {
      "epoch": 3.539325842696629,
      "grad_norm": 0.7467079758644104,
      "learning_rate": 0.00012160804020100502,
      "loss": 0.4012,
      "step": 396
    },
    {
      "epoch": 3.548314606741573,
      "grad_norm": 0.7544340491294861,
      "learning_rate": 0.00012140703517587942,
      "loss": 0.4022,
      "step": 397
    },
    {
      "epoch": 3.5573033707865167,
      "grad_norm": 0.6978884339332581,
      "learning_rate": 0.00012120603015075378,
      "loss": 0.3595,
      "step": 398
    },
    {
      "epoch": 3.5662921348314605,
      "grad_norm": 0.7388366460800171,
      "learning_rate": 0.00012100502512562815,
      "loss": 0.3647,
      "step": 399
    },
    {
      "epoch": 3.5752808988764047,
      "grad_norm": 0.8385876417160034,
      "learning_rate": 0.00012080402010050251,
      "loss": 0.3812,
      "step": 400
    },
    {
      "epoch": 3.5842696629213484,
      "grad_norm": 0.792842447757721,
      "learning_rate": 0.00012060301507537688,
      "loss": 0.4102,
      "step": 401
    },
    {
      "epoch": 3.593258426966292,
      "grad_norm": 0.6842297911643982,
      "learning_rate": 0.00012040201005025127,
      "loss": 0.351,
      "step": 402
    },
    {
      "epoch": 3.602247191011236,
      "grad_norm": 0.6840287446975708,
      "learning_rate": 0.00012020100502512563,
      "loss": 0.2561,
      "step": 403
    },
    {
      "epoch": 3.6112359550561797,
      "grad_norm": 0.749516487121582,
      "learning_rate": 0.00012,
      "loss": 0.3617,
      "step": 404
    },
    {
      "epoch": 3.620224719101124,
      "grad_norm": 0.7854650616645813,
      "learning_rate": 0.00011979899497487436,
      "loss": 0.4217,
      "step": 405
    },
    {
      "epoch": 3.629213483146067,
      "grad_norm": 0.7130045890808105,
      "learning_rate": 0.00011959798994974876,
      "loss": 0.3679,
      "step": 406
    },
    {
      "epoch": 3.6382022471910114,
      "grad_norm": 0.6821588277816772,
      "learning_rate": 0.00011939698492462312,
      "loss": 0.3038,
      "step": 407
    },
    {
      "epoch": 3.647191011235955,
      "grad_norm": 0.6782889366149902,
      "learning_rate": 0.0001191959798994975,
      "loss": 0.335,
      "step": 408
    },
    {
      "epoch": 3.656179775280899,
      "grad_norm": 0.842370331287384,
      "learning_rate": 0.00011899497487437185,
      "loss": 0.4232,
      "step": 409
    },
    {
      "epoch": 3.6651685393258426,
      "grad_norm": 0.8541638851165771,
      "learning_rate": 0.00011879396984924624,
      "loss": 0.4059,
      "step": 410
    },
    {
      "epoch": 3.6741573033707864,
      "grad_norm": 0.7281871438026428,
      "learning_rate": 0.00011859296482412061,
      "loss": 0.3201,
      "step": 411
    },
    {
      "epoch": 3.6831460674157306,
      "grad_norm": 0.8146765232086182,
      "learning_rate": 0.00011839195979899497,
      "loss": 0.3779,
      "step": 412
    },
    {
      "epoch": 3.692134831460674,
      "grad_norm": 0.8430759310722351,
      "learning_rate": 0.00011819095477386935,
      "loss": 0.4202,
      "step": 413
    },
    {
      "epoch": 3.701123595505618,
      "grad_norm": 0.787123441696167,
      "learning_rate": 0.00011798994974874373,
      "loss": 0.3185,
      "step": 414
    },
    {
      "epoch": 3.710112359550562,
      "grad_norm": 0.6547075510025024,
      "learning_rate": 0.0001177889447236181,
      "loss": 0.2879,
      "step": 415
    },
    {
      "epoch": 3.7191011235955056,
      "grad_norm": 0.6983742117881775,
      "learning_rate": 0.00011758793969849247,
      "loss": 0.3542,
      "step": 416
    },
    {
      "epoch": 3.7280898876404494,
      "grad_norm": 0.7807478904724121,
      "learning_rate": 0.00011738693467336684,
      "loss": 0.3881,
      "step": 417
    },
    {
      "epoch": 3.737078651685393,
      "grad_norm": 0.7677443027496338,
      "learning_rate": 0.00011718592964824122,
      "loss": 0.3875,
      "step": 418
    },
    {
      "epoch": 3.7460674157303373,
      "grad_norm": 0.794285237789154,
      "learning_rate": 0.00011698492462311558,
      "loss": 0.3679,
      "step": 419
    },
    {
      "epoch": 3.755056179775281,
      "grad_norm": 0.7937585115432739,
      "learning_rate": 0.00011678391959798996,
      "loss": 0.4337,
      "step": 420
    },
    {
      "epoch": 3.764044943820225,
      "grad_norm": 0.6814838647842407,
      "learning_rate": 0.00011658291457286432,
      "loss": 0.3486,
      "step": 421
    },
    {
      "epoch": 3.7730337078651686,
      "grad_norm": 0.6709129810333252,
      "learning_rate": 0.00011638190954773872,
      "loss": 0.2449,
      "step": 422
    },
    {
      "epoch": 3.7820224719101123,
      "grad_norm": 0.6068003177642822,
      "learning_rate": 0.00011618090452261308,
      "loss": 0.2801,
      "step": 423
    },
    {
      "epoch": 3.791011235955056,
      "grad_norm": 0.771164059638977,
      "learning_rate": 0.00011597989949748745,
      "loss": 0.344,
      "step": 424
    },
    {
      "epoch": 3.8,
      "grad_norm": 0.7781122922897339,
      "learning_rate": 0.00011577889447236181,
      "loss": 0.4293,
      "step": 425
    },
    {
      "epoch": 3.808988764044944,
      "grad_norm": 0.760399580001831,
      "learning_rate": 0.00011557788944723618,
      "loss": 0.3243,
      "step": 426
    },
    {
      "epoch": 3.8179775280898878,
      "grad_norm": 0.6893835663795471,
      "learning_rate": 0.00011537688442211057,
      "loss": 0.3106,
      "step": 427
    },
    {
      "epoch": 3.8269662921348315,
      "grad_norm": 0.7480776906013489,
      "learning_rate": 0.00011517587939698493,
      "loss": 0.3949,
      "step": 428
    },
    {
      "epoch": 3.8359550561797753,
      "grad_norm": 0.8022905588150024,
      "learning_rate": 0.0001149748743718593,
      "loss": 0.4187,
      "step": 429
    },
    {
      "epoch": 3.844943820224719,
      "grad_norm": 0.7737365365028381,
      "learning_rate": 0.00011477386934673366,
      "loss": 0.3538,
      "step": 430
    },
    {
      "epoch": 3.853932584269663,
      "grad_norm": 0.7838185429573059,
      "learning_rate": 0.00011457286432160806,
      "loss": 0.4341,
      "step": 431
    },
    {
      "epoch": 3.8629213483146065,
      "grad_norm": 0.7684136033058167,
      "learning_rate": 0.00011437185929648242,
      "loss": 0.3644,
      "step": 432
    },
    {
      "epoch": 3.8719101123595507,
      "grad_norm": 0.7471869587898254,
      "learning_rate": 0.00011417085427135679,
      "loss": 0.3982,
      "step": 433
    },
    {
      "epoch": 3.8808988764044945,
      "grad_norm": 0.6247967481613159,
      "learning_rate": 0.00011396984924623115,
      "loss": 0.2802,
      "step": 434
    },
    {
      "epoch": 3.8898876404494382,
      "grad_norm": 0.7592265009880066,
      "learning_rate": 0.00011376884422110554,
      "loss": 0.4051,
      "step": 435
    },
    {
      "epoch": 3.898876404494382,
      "grad_norm": 0.8130335807800293,
      "learning_rate": 0.00011356783919597991,
      "loss": 0.4261,
      "step": 436
    },
    {
      "epoch": 3.9078651685393258,
      "grad_norm": 0.8373491764068604,
      "learning_rate": 0.00011336683417085427,
      "loss": 0.3824,
      "step": 437
    },
    {
      "epoch": 3.9168539325842695,
      "grad_norm": 0.6816343069076538,
      "learning_rate": 0.00011316582914572864,
      "loss": 0.3138,
      "step": 438
    },
    {
      "epoch": 3.9258426966292133,
      "grad_norm": 0.7390284538269043,
      "learning_rate": 0.00011296482412060303,
      "loss": 0.3722,
      "step": 439
    },
    {
      "epoch": 3.9348314606741575,
      "grad_norm": 0.6764691472053528,
      "learning_rate": 0.0001127638190954774,
      "loss": 0.3041,
      "step": 440
    },
    {
      "epoch": 3.943820224719101,
      "grad_norm": 0.7113064527511597,
      "learning_rate": 0.00011256281407035176,
      "loss": 0.2926,
      "step": 441
    },
    {
      "epoch": 3.952808988764045,
      "grad_norm": 0.6337546706199646,
      "learning_rate": 0.00011236180904522614,
      "loss": 0.3036,
      "step": 442
    },
    {
      "epoch": 3.9617977528089887,
      "grad_norm": 0.7553158402442932,
      "learning_rate": 0.00011216080402010052,
      "loss": 0.3973,
      "step": 443
    },
    {
      "epoch": 3.9707865168539325,
      "grad_norm": 0.83677077293396,
      "learning_rate": 0.00011195979899497488,
      "loss": 0.4605,
      "step": 444
    },
    {
      "epoch": 3.9797752808988767,
      "grad_norm": 0.7215544581413269,
      "learning_rate": 0.00011175879396984925,
      "loss": 0.3487,
      "step": 445
    },
    {
      "epoch": 3.98876404494382,
      "grad_norm": 0.6975643038749695,
      "learning_rate": 0.00011155778894472361,
      "loss": 0.3027,
      "step": 446
    },
    {
      "epoch": 3.997752808988764,
      "grad_norm": 0.5573864579200745,
      "learning_rate": 0.00011135678391959799,
      "loss": 0.282,
      "step": 447
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.9003021717071533,
      "learning_rate": 0.00011115577889447237,
      "loss": 0.2126,
      "step": 448
    },
    {
      "epoch": 4.008988764044944,
      "grad_norm": 0.6495447158813477,
      "learning_rate": 0.00011095477386934675,
      "loss": 0.2508,
      "step": 449
    },
    {
      "epoch": 4.0179775280898875,
      "grad_norm": 0.6412861943244934,
      "learning_rate": 0.0001107537688442211,
      "loss": 0.2376,
      "step": 450
    },
    {
      "epoch": 4.026966292134832,
      "grad_norm": 0.4923248291015625,
      "learning_rate": 0.00011055276381909548,
      "loss": 0.1681,
      "step": 451
    },
    {
      "epoch": 4.035955056179775,
      "grad_norm": 0.6236695051193237,
      "learning_rate": 0.00011035175879396986,
      "loss": 0.1787,
      "step": 452
    },
    {
      "epoch": 4.044943820224719,
      "grad_norm": 0.7589457631111145,
      "learning_rate": 0.00011015075376884422,
      "loss": 0.2579,
      "step": 453
    },
    {
      "epoch": 4.0539325842696625,
      "grad_norm": 0.8254818916320801,
      "learning_rate": 0.0001099497487437186,
      "loss": 0.2637,
      "step": 454
    },
    {
      "epoch": 4.062921348314607,
      "grad_norm": 0.8028696775436401,
      "learning_rate": 0.00010974874371859296,
      "loss": 0.2462,
      "step": 455
    },
    {
      "epoch": 4.071910112359551,
      "grad_norm": 0.8442559242248535,
      "learning_rate": 0.00010954773869346736,
      "loss": 0.2409,
      "step": 456
    },
    {
      "epoch": 4.080898876404494,
      "grad_norm": 0.817123293876648,
      "learning_rate": 0.00010934673366834172,
      "loss": 0.1833,
      "step": 457
    },
    {
      "epoch": 4.089887640449438,
      "grad_norm": 1.053604245185852,
      "learning_rate": 0.00010914572864321609,
      "loss": 0.2499,
      "step": 458
    },
    {
      "epoch": 4.098876404494382,
      "grad_norm": 0.721295952796936,
      "learning_rate": 0.00010894472361809045,
      "loss": 0.1359,
      "step": 459
    },
    {
      "epoch": 4.107865168539326,
      "grad_norm": 1.2230534553527832,
      "learning_rate": 0.00010874371859296483,
      "loss": 0.2709,
      "step": 460
    },
    {
      "epoch": 4.116853932584269,
      "grad_norm": 0.9722910523414612,
      "learning_rate": 0.00010854271356783921,
      "loss": 0.2126,
      "step": 461
    },
    {
      "epoch": 4.125842696629213,
      "grad_norm": 0.8737917542457581,
      "learning_rate": 0.00010834170854271357,
      "loss": 0.1852,
      "step": 462
    },
    {
      "epoch": 4.134831460674158,
      "grad_norm": 0.9618750214576721,
      "learning_rate": 0.00010814070351758794,
      "loss": 0.2582,
      "step": 463
    },
    {
      "epoch": 4.143820224719101,
      "grad_norm": 0.8098375201225281,
      "learning_rate": 0.00010793969849246233,
      "loss": 0.2365,
      "step": 464
    },
    {
      "epoch": 4.152808988764045,
      "grad_norm": 0.8762398362159729,
      "learning_rate": 0.0001077386934673367,
      "loss": 0.2572,
      "step": 465
    },
    {
      "epoch": 4.1617977528089884,
      "grad_norm": 0.7826884984970093,
      "learning_rate": 0.00010753768844221106,
      "loss": 0.2208,
      "step": 466
    },
    {
      "epoch": 4.170786516853933,
      "grad_norm": 0.8834488987922668,
      "learning_rate": 0.00010733668341708543,
      "loss": 0.2246,
      "step": 467
    },
    {
      "epoch": 4.179775280898877,
      "grad_norm": 0.821602463722229,
      "learning_rate": 0.00010713567839195982,
      "loss": 0.2519,
      "step": 468
    },
    {
      "epoch": 4.18876404494382,
      "grad_norm": 0.841163158416748,
      "learning_rate": 0.00010693467336683418,
      "loss": 0.2511,
      "step": 469
    },
    {
      "epoch": 4.197752808988764,
      "grad_norm": 1.2113871574401855,
      "learning_rate": 0.00010673366834170855,
      "loss": 0.2276,
      "step": 470
    },
    {
      "epoch": 4.206741573033708,
      "grad_norm": 1.1432843208312988,
      "learning_rate": 0.00010653266331658291,
      "loss": 0.2408,
      "step": 471
    },
    {
      "epoch": 4.215730337078652,
      "grad_norm": 0.8568527698516846,
      "learning_rate": 0.00010633165829145728,
      "loss": 0.2313,
      "step": 472
    },
    {
      "epoch": 4.224719101123595,
      "grad_norm": 0.7555668950080872,
      "learning_rate": 0.00010613065326633167,
      "loss": 0.1709,
      "step": 473
    },
    {
      "epoch": 4.233707865168539,
      "grad_norm": 0.8678665161132812,
      "learning_rate": 0.00010592964824120604,
      "loss": 0.2027,
      "step": 474
    },
    {
      "epoch": 4.242696629213484,
      "grad_norm": 0.8474178910255432,
      "learning_rate": 0.0001057286432160804,
      "loss": 0.2342,
      "step": 475
    },
    {
      "epoch": 4.251685393258427,
      "grad_norm": 0.8506501913070679,
      "learning_rate": 0.00010552763819095478,
      "loss": 0.1726,
      "step": 476
    },
    {
      "epoch": 4.260674157303371,
      "grad_norm": 0.7650928497314453,
      "learning_rate": 0.00010532663316582916,
      "loss": 0.1879,
      "step": 477
    },
    {
      "epoch": 4.269662921348314,
      "grad_norm": 0.7401047348976135,
      "learning_rate": 0.00010512562814070352,
      "loss": 0.1746,
      "step": 478
    },
    {
      "epoch": 4.278651685393259,
      "grad_norm": 0.7966935634613037,
      "learning_rate": 0.0001049246231155779,
      "loss": 0.1963,
      "step": 479
    },
    {
      "epoch": 4.287640449438202,
      "grad_norm": 0.8457091450691223,
      "learning_rate": 0.00010472361809045225,
      "loss": 0.2247,
      "step": 480
    },
    {
      "epoch": 4.296629213483146,
      "grad_norm": 0.8635743856430054,
      "learning_rate": 0.00010452261306532664,
      "loss": 0.1918,
      "step": 481
    },
    {
      "epoch": 4.30561797752809,
      "grad_norm": 1.0104352235794067,
      "learning_rate": 0.00010432160804020101,
      "loss": 0.2678,
      "step": 482
    },
    {
      "epoch": 4.314606741573034,
      "grad_norm": 0.8418344855308533,
      "learning_rate": 0.00010412060301507539,
      "loss": 0.2104,
      "step": 483
    },
    {
      "epoch": 4.323595505617978,
      "grad_norm": 0.8345755934715271,
      "learning_rate": 0.00010391959798994975,
      "loss": 0.2212,
      "step": 484
    },
    {
      "epoch": 4.332584269662921,
      "grad_norm": 0.8102269768714905,
      "learning_rate": 0.00010371859296482413,
      "loss": 0.2349,
      "step": 485
    },
    {
      "epoch": 4.341573033707865,
      "grad_norm": 0.8695855140686035,
      "learning_rate": 0.0001035175879396985,
      "loss": 0.1842,
      "step": 486
    },
    {
      "epoch": 4.350561797752809,
      "grad_norm": 0.8787556290626526,
      "learning_rate": 0.00010331658291457286,
      "loss": 0.1635,
      "step": 487
    },
    {
      "epoch": 4.359550561797753,
      "grad_norm": 0.9394383430480957,
      "learning_rate": 0.00010311557788944724,
      "loss": 0.2754,
      "step": 488
    },
    {
      "epoch": 4.368539325842697,
      "grad_norm": 0.832755982875824,
      "learning_rate": 0.00010291457286432162,
      "loss": 0.2544,
      "step": 489
    },
    {
      "epoch": 4.37752808988764,
      "grad_norm": 0.8823708891868591,
      "learning_rate": 0.00010271356783919598,
      "loss": 0.2526,
      "step": 490
    },
    {
      "epoch": 4.3865168539325845,
      "grad_norm": 0.8316506147384644,
      "learning_rate": 0.00010251256281407036,
      "loss": 0.2217,
      "step": 491
    },
    {
      "epoch": 4.395505617977528,
      "grad_norm": 0.925698459148407,
      "learning_rate": 0.00010231155778894473,
      "loss": 0.2725,
      "step": 492
    },
    {
      "epoch": 4.404494382022472,
      "grad_norm": 0.8309943675994873,
      "learning_rate": 0.00010211055276381909,
      "loss": 0.2343,
      "step": 493
    },
    {
      "epoch": 4.413483146067415,
      "grad_norm": 0.7822101712226868,
      "learning_rate": 0.00010190954773869348,
      "loss": 0.2078,
      "step": 494
    },
    {
      "epoch": 4.4224719101123595,
      "grad_norm": 0.8135600686073303,
      "learning_rate": 0.00010170854271356785,
      "loss": 0.2221,
      "step": 495
    },
    {
      "epoch": 4.431460674157304,
      "grad_norm": 1.037107229232788,
      "learning_rate": 0.00010150753768844221,
      "loss": 0.2225,
      "step": 496
    },
    {
      "epoch": 4.440449438202247,
      "grad_norm": 0.8496840000152588,
      "learning_rate": 0.00010130653266331658,
      "loss": 0.1529,
      "step": 497
    },
    {
      "epoch": 4.449438202247191,
      "grad_norm": 0.8700290322303772,
      "learning_rate": 0.00010110552763819097,
      "loss": 0.2347,
      "step": 498
    },
    {
      "epoch": 4.4584269662921345,
      "grad_norm": 0.7039309740066528,
      "learning_rate": 0.00010090452261306533,
      "loss": 0.1548,
      "step": 499
    },
    {
      "epoch": 4.467415730337079,
      "grad_norm": 0.7159139513969421,
      "learning_rate": 0.0001007035175879397,
      "loss": 0.1884,
      "step": 500
    },
    {
      "epoch": 4.476404494382022,
      "grad_norm": 0.7827560901641846,
      "learning_rate": 0.00010050251256281407,
      "loss": 0.1932,
      "step": 501
    },
    {
      "epoch": 4.485393258426966,
      "grad_norm": 0.8728410005569458,
      "learning_rate": 0.00010030150753768846,
      "loss": 0.2423,
      "step": 502
    },
    {
      "epoch": 4.49438202247191,
      "grad_norm": 0.7617059350013733,
      "learning_rate": 0.00010010050251256282,
      "loss": 0.2013,
      "step": 503
    },
    {
      "epoch": 4.503370786516854,
      "grad_norm": 0.8571162223815918,
      "learning_rate": 9.989949748743719e-05,
      "loss": 0.2532,
      "step": 504
    },
    {
      "epoch": 4.512359550561798,
      "grad_norm": 0.9142491817474365,
      "learning_rate": 9.969849246231156e-05,
      "loss": 0.2572,
      "step": 505
    },
    {
      "epoch": 4.521348314606741,
      "grad_norm": 0.8514188528060913,
      "learning_rate": 9.949748743718594e-05,
      "loss": 0.218,
      "step": 506
    },
    {
      "epoch": 4.5303370786516854,
      "grad_norm": 1.3144577741622925,
      "learning_rate": 9.929648241206031e-05,
      "loss": 0.2391,
      "step": 507
    },
    {
      "epoch": 4.539325842696629,
      "grad_norm": 0.8897493481636047,
      "learning_rate": 9.909547738693468e-05,
      "loss": 0.2563,
      "step": 508
    },
    {
      "epoch": 4.548314606741573,
      "grad_norm": 0.8062596321105957,
      "learning_rate": 9.889447236180906e-05,
      "loss": 0.1948,
      "step": 509
    },
    {
      "epoch": 4.557303370786517,
      "grad_norm": 0.7618328928947449,
      "learning_rate": 9.869346733668342e-05,
      "loss": 0.209,
      "step": 510
    },
    {
      "epoch": 4.5662921348314605,
      "grad_norm": 0.7925639748573303,
      "learning_rate": 9.84924623115578e-05,
      "loss": 0.2195,
      "step": 511
    },
    {
      "epoch": 4.575280898876405,
      "grad_norm": 1.0301642417907715,
      "learning_rate": 9.829145728643216e-05,
      "loss": 0.2732,
      "step": 512
    },
    {
      "epoch": 4.584269662921348,
      "grad_norm": 0.9164537787437439,
      "learning_rate": 9.809045226130655e-05,
      "loss": 0.2849,
      "step": 513
    },
    {
      "epoch": 4.593258426966292,
      "grad_norm": 1.008405327796936,
      "learning_rate": 9.788944723618091e-05,
      "loss": 0.2496,
      "step": 514
    },
    {
      "epoch": 4.6022471910112355,
      "grad_norm": 0.8623641133308411,
      "learning_rate": 9.768844221105528e-05,
      "loss": 0.2287,
      "step": 515
    },
    {
      "epoch": 4.61123595505618,
      "grad_norm": 0.7490643858909607,
      "learning_rate": 9.748743718592965e-05,
      "loss": 0.1922,
      "step": 516
    },
    {
      "epoch": 4.620224719101124,
      "grad_norm": 0.8775993585586548,
      "learning_rate": 9.728643216080403e-05,
      "loss": 0.2805,
      "step": 517
    },
    {
      "epoch": 4.629213483146067,
      "grad_norm": 0.8953645825386047,
      "learning_rate": 9.70854271356784e-05,
      "loss": 0.2705,
      "step": 518
    },
    {
      "epoch": 4.638202247191011,
      "grad_norm": 1.2113584280014038,
      "learning_rate": 9.688442211055276e-05,
      "loss": 0.2369,
      "step": 519
    },
    {
      "epoch": 4.647191011235955,
      "grad_norm": 0.9550536870956421,
      "learning_rate": 9.668341708542715e-05,
      "loss": 0.2644,
      "step": 520
    },
    {
      "epoch": 4.656179775280899,
      "grad_norm": 0.8497539758682251,
      "learning_rate": 9.64824120603015e-05,
      "loss": 0.2056,
      "step": 521
    },
    {
      "epoch": 4.665168539325843,
      "grad_norm": 0.7544856071472168,
      "learning_rate": 9.628140703517589e-05,
      "loss": 0.2187,
      "step": 522
    },
    {
      "epoch": 4.674157303370786,
      "grad_norm": 0.8686268329620361,
      "learning_rate": 9.608040201005025e-05,
      "loss": 0.219,
      "step": 523
    },
    {
      "epoch": 4.683146067415731,
      "grad_norm": 0.8762931227684021,
      "learning_rate": 9.587939698492462e-05,
      "loss": 0.2108,
      "step": 524
    },
    {
      "epoch": 4.692134831460674,
      "grad_norm": 0.817994236946106,
      "learning_rate": 9.5678391959799e-05,
      "loss": 0.2401,
      "step": 525
    },
    {
      "epoch": 4.701123595505618,
      "grad_norm": 0.8209389448165894,
      "learning_rate": 9.547738693467337e-05,
      "loss": 0.2077,
      "step": 526
    },
    {
      "epoch": 4.710112359550562,
      "grad_norm": 0.8976430892944336,
      "learning_rate": 9.527638190954774e-05,
      "loss": 0.2403,
      "step": 527
    },
    {
      "epoch": 4.719101123595506,
      "grad_norm": 0.7954127788543701,
      "learning_rate": 9.507537688442212e-05,
      "loss": 0.1809,
      "step": 528
    },
    {
      "epoch": 4.72808988764045,
      "grad_norm": 0.8460317254066467,
      "learning_rate": 9.487437185929649e-05,
      "loss": 0.2295,
      "step": 529
    },
    {
      "epoch": 4.737078651685393,
      "grad_norm": 0.773540198802948,
      "learning_rate": 9.467336683417086e-05,
      "loss": 0.2223,
      "step": 530
    },
    {
      "epoch": 4.746067415730337,
      "grad_norm": 0.8182321786880493,
      "learning_rate": 9.447236180904523e-05,
      "loss": 0.2244,
      "step": 531
    },
    {
      "epoch": 4.755056179775281,
      "grad_norm": 0.8828237652778625,
      "learning_rate": 9.427135678391961e-05,
      "loss": 0.2321,
      "step": 532
    },
    {
      "epoch": 4.764044943820225,
      "grad_norm": 0.9630143046379089,
      "learning_rate": 9.407035175879397e-05,
      "loss": 0.2831,
      "step": 533
    },
    {
      "epoch": 4.773033707865169,
      "grad_norm": 0.7828625440597534,
      "learning_rate": 9.386934673366835e-05,
      "loss": 0.2275,
      "step": 534
    },
    {
      "epoch": 4.782022471910112,
      "grad_norm": 0.8256980776786804,
      "learning_rate": 9.366834170854271e-05,
      "loss": 0.2526,
      "step": 535
    },
    {
      "epoch": 4.7910112359550565,
      "grad_norm": 0.8015206456184387,
      "learning_rate": 9.34673366834171e-05,
      "loss": 0.2417,
      "step": 536
    },
    {
      "epoch": 4.8,
      "grad_norm": 0.6070647835731506,
      "learning_rate": 9.326633165829146e-05,
      "loss": 0.152,
      "step": 537
    },
    {
      "epoch": 4.808988764044944,
      "grad_norm": 1.1248042583465576,
      "learning_rate": 9.306532663316585e-05,
      "loss": 0.2254,
      "step": 538
    },
    {
      "epoch": 4.817977528089887,
      "grad_norm": 0.8076364398002625,
      "learning_rate": 9.28643216080402e-05,
      "loss": 0.231,
      "step": 539
    },
    {
      "epoch": 4.8269662921348315,
      "grad_norm": 0.7604075074195862,
      "learning_rate": 9.266331658291458e-05,
      "loss": 0.2409,
      "step": 540
    },
    {
      "epoch": 4.835955056179776,
      "grad_norm": 0.7149854898452759,
      "learning_rate": 9.246231155778895e-05,
      "loss": 0.1947,
      "step": 541
    },
    {
      "epoch": 4.844943820224719,
      "grad_norm": 0.9404659271240234,
      "learning_rate": 9.226130653266331e-05,
      "loss": 0.2826,
      "step": 542
    },
    {
      "epoch": 4.853932584269663,
      "grad_norm": 0.6889961957931519,
      "learning_rate": 9.20603015075377e-05,
      "loss": 0.1683,
      "step": 543
    },
    {
      "epoch": 4.8629213483146065,
      "grad_norm": 0.8409780859947205,
      "learning_rate": 9.185929648241206e-05,
      "loss": 0.238,
      "step": 544
    },
    {
      "epoch": 4.871910112359551,
      "grad_norm": 0.8457804322242737,
      "learning_rate": 9.165829145728644e-05,
      "loss": 0.2068,
      "step": 545
    },
    {
      "epoch": 4.880898876404494,
      "grad_norm": 0.8882004618644714,
      "learning_rate": 9.14572864321608e-05,
      "loss": 0.2259,
      "step": 546
    },
    {
      "epoch": 4.889887640449438,
      "grad_norm": 0.9201176166534424,
      "learning_rate": 9.125628140703519e-05,
      "loss": 0.2641,
      "step": 547
    },
    {
      "epoch": 4.898876404494382,
      "grad_norm": 0.814575731754303,
      "learning_rate": 9.105527638190955e-05,
      "loss": 0.2138,
      "step": 548
    },
    {
      "epoch": 4.907865168539326,
      "grad_norm": 0.8942732810974121,
      "learning_rate": 9.085427135678392e-05,
      "loss": 0.2483,
      "step": 549
    },
    {
      "epoch": 4.91685393258427,
      "grad_norm": 0.7981822490692139,
      "learning_rate": 9.06532663316583e-05,
      "loss": 0.2226,
      "step": 550
    },
    {
      "epoch": 4.925842696629213,
      "grad_norm": 0.9360243678092957,
      "learning_rate": 9.045226130653267e-05,
      "loss": 0.2839,
      "step": 551
    },
    {
      "epoch": 4.9348314606741575,
      "grad_norm": 0.8720349073410034,
      "learning_rate": 9.025125628140704e-05,
      "loss": 0.2351,
      "step": 552
    },
    {
      "epoch": 4.943820224719101,
      "grad_norm": 0.78605055809021,
      "learning_rate": 9.005025125628141e-05,
      "loss": 0.2035,
      "step": 553
    },
    {
      "epoch": 4.952808988764045,
      "grad_norm": 0.921025276184082,
      "learning_rate": 8.984924623115579e-05,
      "loss": 0.2595,
      "step": 554
    },
    {
      "epoch": 4.961797752808989,
      "grad_norm": 0.7992529273033142,
      "learning_rate": 8.964824120603016e-05,
      "loss": 0.2559,
      "step": 555
    },
    {
      "epoch": 4.9707865168539325,
      "grad_norm": 0.8522776961326599,
      "learning_rate": 8.944723618090453e-05,
      "loss": 0.2323,
      "step": 556
    },
    {
      "epoch": 4.979775280898877,
      "grad_norm": 0.7810289263725281,
      "learning_rate": 8.92462311557789e-05,
      "loss": 0.2485,
      "step": 557
    },
    {
      "epoch": 4.98876404494382,
      "grad_norm": 0.8643763661384583,
      "learning_rate": 8.904522613065326e-05,
      "loss": 0.2374,
      "step": 558
    },
    {
      "epoch": 4.997752808988764,
      "grad_norm": 0.7328014969825745,
      "learning_rate": 8.884422110552765e-05,
      "loss": 0.1402,
      "step": 559
    },
    {
      "epoch": 5.0,
      "grad_norm": 1.6732932329177856,
      "learning_rate": 8.864321608040201e-05,
      "loss": 0.2804,
      "step": 560
    },
    {
      "epoch": 5.008988764044944,
      "grad_norm": 0.6328826546669006,
      "learning_rate": 8.84422110552764e-05,
      "loss": 0.1291,
      "step": 561
    },
    {
      "epoch": 5.0179775280898875,
      "grad_norm": 0.7595165967941284,
      "learning_rate": 8.824120603015076e-05,
      "loss": 0.1434,
      "step": 562
    },
    {
      "epoch": 5.026966292134832,
      "grad_norm": 0.5240498781204224,
      "learning_rate": 8.804020100502513e-05,
      "loss": 0.1036,
      "step": 563
    },
    {
      "epoch": 5.035955056179775,
      "grad_norm": 0.6403650045394897,
      "learning_rate": 8.78391959798995e-05,
      "loss": 0.1208,
      "step": 564
    },
    {
      "epoch": 5.044943820224719,
      "grad_norm": 0.5547477006912231,
      "learning_rate": 8.763819095477387e-05,
      "loss": 0.0849,
      "step": 565
    },
    {
      "epoch": 5.0539325842696625,
      "grad_norm": 0.7071437835693359,
      "learning_rate": 8.743718592964825e-05,
      "loss": 0.1057,
      "step": 566
    },
    {
      "epoch": 5.062921348314607,
      "grad_norm": 0.6914066076278687,
      "learning_rate": 8.723618090452261e-05,
      "loss": 0.0984,
      "step": 567
    },
    {
      "epoch": 5.071910112359551,
      "grad_norm": 0.8321857452392578,
      "learning_rate": 8.7035175879397e-05,
      "loss": 0.0941,
      "step": 568
    },
    {
      "epoch": 5.080898876404494,
      "grad_norm": 0.9950762391090393,
      "learning_rate": 8.683417085427135e-05,
      "loss": 0.1347,
      "step": 569
    },
    {
      "epoch": 5.089887640449438,
      "grad_norm": 0.8879238367080688,
      "learning_rate": 8.663316582914574e-05,
      "loss": 0.1146,
      "step": 570
    },
    {
      "epoch": 5.098876404494382,
      "grad_norm": 1.0718597173690796,
      "learning_rate": 8.64321608040201e-05,
      "loss": 0.1347,
      "step": 571
    },
    {
      "epoch": 5.107865168539326,
      "grad_norm": 1.0072975158691406,
      "learning_rate": 8.623115577889449e-05,
      "loss": 0.1236,
      "step": 572
    },
    {
      "epoch": 5.116853932584269,
      "grad_norm": 0.9800323843955994,
      "learning_rate": 8.603015075376884e-05,
      "loss": 0.1208,
      "step": 573
    },
    {
      "epoch": 5.125842696629213,
      "grad_norm": 0.9401484727859497,
      "learning_rate": 8.582914572864322e-05,
      "loss": 0.1186,
      "step": 574
    },
    {
      "epoch": 5.134831460674158,
      "grad_norm": 0.8877111077308655,
      "learning_rate": 8.562814070351759e-05,
      "loss": 0.1233,
      "step": 575
    },
    {
      "epoch": 5.143820224719101,
      "grad_norm": 0.9317702651023865,
      "learning_rate": 8.542713567839196e-05,
      "loss": 0.1122,
      "step": 576
    },
    {
      "epoch": 5.152808988764045,
      "grad_norm": 0.9840295314788818,
      "learning_rate": 8.522613065326634e-05,
      "loss": 0.1225,
      "step": 577
    },
    {
      "epoch": 5.1617977528089884,
      "grad_norm": 1.058971881866455,
      "learning_rate": 8.502512562814071e-05,
      "loss": 0.1593,
      "step": 578
    },
    {
      "epoch": 5.170786516853933,
      "grad_norm": 0.8582284450531006,
      "learning_rate": 8.482412060301508e-05,
      "loss": 0.1209,
      "step": 579
    },
    {
      "epoch": 5.179775280898877,
      "grad_norm": 0.7770978212356567,
      "learning_rate": 8.462311557788946e-05,
      "loss": 0.1013,
      "step": 580
    },
    {
      "epoch": 5.18876404494382,
      "grad_norm": 0.9582062363624573,
      "learning_rate": 8.442211055276383e-05,
      "loss": 0.1227,
      "step": 581
    },
    {
      "epoch": 5.197752808988764,
      "grad_norm": 0.8662171363830566,
      "learning_rate": 8.42211055276382e-05,
      "loss": 0.1223,
      "step": 582
    },
    {
      "epoch": 5.206741573033708,
      "grad_norm": 0.7964221835136414,
      "learning_rate": 8.402010050251256e-05,
      "loss": 0.1098,
      "step": 583
    },
    {
      "epoch": 5.215730337078652,
      "grad_norm": 0.7506003379821777,
      "learning_rate": 8.381909547738695e-05,
      "loss": 0.1128,
      "step": 584
    },
    {
      "epoch": 5.224719101123595,
      "grad_norm": 0.8886776566505432,
      "learning_rate": 8.36180904522613e-05,
      "loss": 0.1487,
      "step": 585
    },
    {
      "epoch": 5.233707865168539,
      "grad_norm": 0.792411744594574,
      "learning_rate": 8.341708542713568e-05,
      "loss": 0.1015,
      "step": 586
    },
    {
      "epoch": 5.242696629213484,
      "grad_norm": 0.7079986333847046,
      "learning_rate": 8.321608040201005e-05,
      "loss": 0.0933,
      "step": 587
    },
    {
      "epoch": 5.251685393258427,
      "grad_norm": 0.9579490423202515,
      "learning_rate": 8.301507537688443e-05,
      "loss": 0.1206,
      "step": 588
    },
    {
      "epoch": 5.260674157303371,
      "grad_norm": 0.6636231541633606,
      "learning_rate": 8.28140703517588e-05,
      "loss": 0.0961,
      "step": 589
    },
    {
      "epoch": 5.269662921348314,
      "grad_norm": 0.8478922843933105,
      "learning_rate": 8.261306532663317e-05,
      "loss": 0.1218,
      "step": 590
    },
    {
      "epoch": 5.278651685393259,
      "grad_norm": 0.9396899342536926,
      "learning_rate": 8.241206030150754e-05,
      "loss": 0.1178,
      "step": 591
    },
    {
      "epoch": 5.287640449438202,
      "grad_norm": 0.9946132898330688,
      "learning_rate": 8.22110552763819e-05,
      "loss": 0.1379,
      "step": 592
    },
    {
      "epoch": 5.296629213483146,
      "grad_norm": 0.9403896331787109,
      "learning_rate": 8.201005025125629e-05,
      "loss": 0.1433,
      "step": 593
    },
    {
      "epoch": 5.30561797752809,
      "grad_norm": 0.9887720942497253,
      "learning_rate": 8.180904522613065e-05,
      "loss": 0.1493,
      "step": 594
    },
    {
      "epoch": 5.314606741573034,
      "grad_norm": 0.8239089846611023,
      "learning_rate": 8.160804020100504e-05,
      "loss": 0.1134,
      "step": 595
    },
    {
      "epoch": 5.323595505617978,
      "grad_norm": 0.933784008026123,
      "learning_rate": 8.14070351758794e-05,
      "loss": 0.1512,
      "step": 596
    },
    {
      "epoch": 5.332584269662921,
      "grad_norm": 0.9797033071517944,
      "learning_rate": 8.120603015075378e-05,
      "loss": 0.1422,
      "step": 597
    },
    {
      "epoch": 5.341573033707865,
      "grad_norm": 0.8115176558494568,
      "learning_rate": 8.100502512562814e-05,
      "loss": 0.1202,
      "step": 598
    },
    {
      "epoch": 5.350561797752809,
      "grad_norm": 1.1294258832931519,
      "learning_rate": 8.080402010050251e-05,
      "loss": 0.1494,
      "step": 599
    },
    {
      "epoch": 5.359550561797753,
      "grad_norm": 0.7876671552658081,
      "learning_rate": 8.060301507537689e-05,
      "loss": 0.1066,
      "step": 600
    },
    {
      "epoch": 5.368539325842697,
      "grad_norm": 0.7373098731040955,
      "learning_rate": 8.040201005025126e-05,
      "loss": 0.116,
      "step": 601
    },
    {
      "epoch": 5.37752808988764,
      "grad_norm": 0.9216926097869873,
      "learning_rate": 8.020100502512563e-05,
      "loss": 0.157,
      "step": 602
    },
    {
      "epoch": 5.3865168539325845,
      "grad_norm": 1.0342787504196167,
      "learning_rate": 8e-05,
      "loss": 0.1702,
      "step": 603
    },
    {
      "epoch": 5.395505617977528,
      "grad_norm": 0.8380178213119507,
      "learning_rate": 7.979899497487438e-05,
      "loss": 0.1301,
      "step": 604
    },
    {
      "epoch": 5.404494382022472,
      "grad_norm": 0.7098482847213745,
      "learning_rate": 7.959798994974875e-05,
      "loss": 0.0968,
      "step": 605
    },
    {
      "epoch": 5.413483146067415,
      "grad_norm": 0.8558669686317444,
      "learning_rate": 7.939698492462313e-05,
      "loss": 0.1168,
      "step": 606
    },
    {
      "epoch": 5.4224719101123595,
      "grad_norm": 0.8892849087715149,
      "learning_rate": 7.91959798994975e-05,
      "loss": 0.1474,
      "step": 607
    },
    {
      "epoch": 5.431460674157304,
      "grad_norm": 0.6875675916671753,
      "learning_rate": 7.899497487437186e-05,
      "loss": 0.1035,
      "step": 608
    },
    {
      "epoch": 5.440449438202247,
      "grad_norm": 0.8377317190170288,
      "learning_rate": 7.879396984924623e-05,
      "loss": 0.1352,
      "step": 609
    },
    {
      "epoch": 5.449438202247191,
      "grad_norm": 0.9261096715927124,
      "learning_rate": 7.85929648241206e-05,
      "loss": 0.1322,
      "step": 610
    },
    {
      "epoch": 5.4584269662921345,
      "grad_norm": 0.8260776400566101,
      "learning_rate": 7.839195979899498e-05,
      "loss": 0.1231,
      "step": 611
    },
    {
      "epoch": 5.467415730337079,
      "grad_norm": 1.0081748962402344,
      "learning_rate": 7.819095477386935e-05,
      "loss": 0.1573,
      "step": 612
    },
    {
      "epoch": 5.476404494382022,
      "grad_norm": 0.9516775012016296,
      "learning_rate": 7.798994974874372e-05,
      "loss": 0.1458,
      "step": 613
    },
    {
      "epoch": 5.485393258426966,
      "grad_norm": 0.8600365519523621,
      "learning_rate": 7.77889447236181e-05,
      "loss": 0.1283,
      "step": 614
    },
    {
      "epoch": 5.49438202247191,
      "grad_norm": 0.8189635276794434,
      "learning_rate": 7.758793969849247e-05,
      "loss": 0.1089,
      "step": 615
    },
    {
      "epoch": 5.503370786516854,
      "grad_norm": 1.0389952659606934,
      "learning_rate": 7.738693467336684e-05,
      "loss": 0.1686,
      "step": 616
    },
    {
      "epoch": 5.512359550561798,
      "grad_norm": 0.717279314994812,
      "learning_rate": 7.71859296482412e-05,
      "loss": 0.104,
      "step": 617
    },
    {
      "epoch": 5.521348314606741,
      "grad_norm": 0.9578083157539368,
      "learning_rate": 7.698492462311559e-05,
      "loss": 0.1358,
      "step": 618
    },
    {
      "epoch": 5.5303370786516854,
      "grad_norm": 0.9937049150466919,
      "learning_rate": 7.678391959798995e-05,
      "loss": 0.1583,
      "step": 619
    },
    {
      "epoch": 5.539325842696629,
      "grad_norm": 0.7870664596557617,
      "learning_rate": 7.658291457286433e-05,
      "loss": 0.1189,
      "step": 620
    },
    {
      "epoch": 5.548314606741573,
      "grad_norm": 0.778238832950592,
      "learning_rate": 7.638190954773869e-05,
      "loss": 0.1214,
      "step": 621
    },
    {
      "epoch": 5.557303370786517,
      "grad_norm": 0.8247886300086975,
      "learning_rate": 7.618090452261307e-05,
      "loss": 0.1216,
      "step": 622
    },
    {
      "epoch": 5.5662921348314605,
      "grad_norm": 1.04477059841156,
      "learning_rate": 7.597989949748744e-05,
      "loss": 0.1549,
      "step": 623
    },
    {
      "epoch": 5.575280898876405,
      "grad_norm": 0.8048946857452393,
      "learning_rate": 7.577889447236181e-05,
      "loss": 0.1267,
      "step": 624
    },
    {
      "epoch": 5.584269662921348,
      "grad_norm": 0.917270302772522,
      "learning_rate": 7.557788944723618e-05,
      "loss": 0.1365,
      "step": 625
    },
    {
      "epoch": 5.593258426966292,
      "grad_norm": 0.8172959089279175,
      "learning_rate": 7.537688442211056e-05,
      "loss": 0.1406,
      "step": 626
    },
    {
      "epoch": 5.6022471910112355,
      "grad_norm": 0.8267268538475037,
      "learning_rate": 7.517587939698493e-05,
      "loss": 0.1336,
      "step": 627
    },
    {
      "epoch": 5.61123595505618,
      "grad_norm": 0.9417851567268372,
      "learning_rate": 7.49748743718593e-05,
      "loss": 0.1557,
      "step": 628
    },
    {
      "epoch": 5.620224719101124,
      "grad_norm": 0.9431745409965515,
      "learning_rate": 7.477386934673368e-05,
      "loss": 0.1494,
      "step": 629
    },
    {
      "epoch": 5.629213483146067,
      "grad_norm": 0.8202044367790222,
      "learning_rate": 7.457286432160805e-05,
      "loss": 0.1284,
      "step": 630
    },
    {
      "epoch": 5.638202247191011,
      "grad_norm": 0.7698252201080322,
      "learning_rate": 7.437185929648241e-05,
      "loss": 0.1189,
      "step": 631
    },
    {
      "epoch": 5.647191011235955,
      "grad_norm": 0.934152364730835,
      "learning_rate": 7.417085427135678e-05,
      "loss": 0.1621,
      "step": 632
    },
    {
      "epoch": 5.656179775280899,
      "grad_norm": 0.7938246130943298,
      "learning_rate": 7.396984924623115e-05,
      "loss": 0.112,
      "step": 633
    },
    {
      "epoch": 5.665168539325843,
      "grad_norm": 0.6606358289718628,
      "learning_rate": 7.376884422110553e-05,
      "loss": 0.1021,
      "step": 634
    },
    {
      "epoch": 5.674157303370786,
      "grad_norm": 0.88918536901474,
      "learning_rate": 7.35678391959799e-05,
      "loss": 0.1263,
      "step": 635
    },
    {
      "epoch": 5.683146067415731,
      "grad_norm": 0.9257373213768005,
      "learning_rate": 7.336683417085427e-05,
      "loss": 0.1347,
      "step": 636
    },
    {
      "epoch": 5.692134831460674,
      "grad_norm": 0.9082857966423035,
      "learning_rate": 7.316582914572865e-05,
      "loss": 0.1359,
      "step": 637
    },
    {
      "epoch": 5.701123595505618,
      "grad_norm": 0.9879689812660217,
      "learning_rate": 7.296482412060302e-05,
      "loss": 0.1494,
      "step": 638
    },
    {
      "epoch": 5.710112359550562,
      "grad_norm": 0.9178323149681091,
      "learning_rate": 7.276381909547739e-05,
      "loss": 0.1514,
      "step": 639
    },
    {
      "epoch": 5.719101123595506,
      "grad_norm": 0.7933792471885681,
      "learning_rate": 7.256281407035177e-05,
      "loss": 0.1071,
      "step": 640
    },
    {
      "epoch": 5.72808988764045,
      "grad_norm": 1.1058943271636963,
      "learning_rate": 7.236180904522614e-05,
      "loss": 0.1699,
      "step": 641
    },
    {
      "epoch": 5.737078651685393,
      "grad_norm": 0.7837674617767334,
      "learning_rate": 7.21608040201005e-05,
      "loss": 0.1083,
      "step": 642
    },
    {
      "epoch": 5.746067415730337,
      "grad_norm": 0.961969256401062,
      "learning_rate": 7.195979899497488e-05,
      "loss": 0.1514,
      "step": 643
    },
    {
      "epoch": 5.755056179775281,
      "grad_norm": 0.8537554740905762,
      "learning_rate": 7.175879396984924e-05,
      "loss": 0.1469,
      "step": 644
    },
    {
      "epoch": 5.764044943820225,
      "grad_norm": 1.0469610691070557,
      "learning_rate": 7.155778894472363e-05,
      "loss": 0.1624,
      "step": 645
    },
    {
      "epoch": 5.773033707865169,
      "grad_norm": 0.8618713021278381,
      "learning_rate": 7.135678391959799e-05,
      "loss": 0.1259,
      "step": 646
    },
    {
      "epoch": 5.782022471910112,
      "grad_norm": 1.0067728757858276,
      "learning_rate": 7.115577889447236e-05,
      "loss": 0.1592,
      "step": 647
    },
    {
      "epoch": 5.7910112359550565,
      "grad_norm": 0.9565613865852356,
      "learning_rate": 7.095477386934674e-05,
      "loss": 0.131,
      "step": 648
    },
    {
      "epoch": 5.8,
      "grad_norm": 0.8598449230194092,
      "learning_rate": 7.075376884422111e-05,
      "loss": 0.1456,
      "step": 649
    },
    {
      "epoch": 5.808988764044944,
      "grad_norm": 0.8594934344291687,
      "learning_rate": 7.055276381909548e-05,
      "loss": 0.1309,
      "step": 650
    },
    {
      "epoch": 5.817977528089887,
      "grad_norm": 0.7968019247055054,
      "learning_rate": 7.035175879396985e-05,
      "loss": 0.13,
      "step": 651
    },
    {
      "epoch": 5.8269662921348315,
      "grad_norm": 0.7706556916236877,
      "learning_rate": 7.015075376884423e-05,
      "loss": 0.1057,
      "step": 652
    },
    {
      "epoch": 5.835955056179776,
      "grad_norm": 0.8830094337463379,
      "learning_rate": 6.99497487437186e-05,
      "loss": 0.1415,
      "step": 653
    },
    {
      "epoch": 5.844943820224719,
      "grad_norm": 0.8978375196456909,
      "learning_rate": 6.974874371859297e-05,
      "loss": 0.155,
      "step": 654
    },
    {
      "epoch": 5.853932584269663,
      "grad_norm": 0.8696443438529968,
      "learning_rate": 6.954773869346733e-05,
      "loss": 0.1384,
      "step": 655
    },
    {
      "epoch": 5.8629213483146065,
      "grad_norm": 0.7788429856300354,
      "learning_rate": 6.93467336683417e-05,
      "loss": 0.1031,
      "step": 656
    },
    {
      "epoch": 5.871910112359551,
      "grad_norm": 0.878122866153717,
      "learning_rate": 6.914572864321608e-05,
      "loss": 0.1322,
      "step": 657
    },
    {
      "epoch": 5.880898876404494,
      "grad_norm": 0.7750670313835144,
      "learning_rate": 6.894472361809045e-05,
      "loss": 0.1209,
      "step": 658
    },
    {
      "epoch": 5.889887640449438,
      "grad_norm": 0.8444107174873352,
      "learning_rate": 6.874371859296482e-05,
      "loss": 0.13,
      "step": 659
    },
    {
      "epoch": 5.898876404494382,
      "grad_norm": 0.757583737373352,
      "learning_rate": 6.85427135678392e-05,
      "loss": 0.0991,
      "step": 660
    },
    {
      "epoch": 5.907865168539326,
      "grad_norm": 0.8722019791603088,
      "learning_rate": 6.834170854271357e-05,
      "loss": 0.1158,
      "step": 661
    },
    {
      "epoch": 5.91685393258427,
      "grad_norm": 0.6448444724082947,
      "learning_rate": 6.814070351758794e-05,
      "loss": 0.0922,
      "step": 662
    },
    {
      "epoch": 5.925842696629213,
      "grad_norm": 0.9685204029083252,
      "learning_rate": 6.793969849246232e-05,
      "loss": 0.1187,
      "step": 663
    },
    {
      "epoch": 5.9348314606741575,
      "grad_norm": 0.6289836168289185,
      "learning_rate": 6.773869346733669e-05,
      "loss": 0.0719,
      "step": 664
    },
    {
      "epoch": 5.943820224719101,
      "grad_norm": 0.8666939735412598,
      "learning_rate": 6.753768844221105e-05,
      "loss": 0.1333,
      "step": 665
    },
    {
      "epoch": 5.952808988764045,
      "grad_norm": 0.8353011608123779,
      "learning_rate": 6.733668341708544e-05,
      "loss": 0.1215,
      "step": 666
    },
    {
      "epoch": 5.961797752808989,
      "grad_norm": 1.0434074401855469,
      "learning_rate": 6.71356783919598e-05,
      "loss": 0.1536,
      "step": 667
    },
    {
      "epoch": 5.9707865168539325,
      "grad_norm": 0.9540296196937561,
      "learning_rate": 6.693467336683418e-05,
      "loss": 0.1706,
      "step": 668
    },
    {
      "epoch": 5.979775280898877,
      "grad_norm": 0.8829556107521057,
      "learning_rate": 6.673366834170854e-05,
      "loss": 0.1322,
      "step": 669
    },
    {
      "epoch": 5.98876404494382,
      "grad_norm": 0.800635039806366,
      "learning_rate": 6.653266331658293e-05,
      "loss": 0.1151,
      "step": 670
    },
    {
      "epoch": 5.997752808988764,
      "grad_norm": 0.8594426512718201,
      "learning_rate": 6.633165829145729e-05,
      "loss": 0.1346,
      "step": 671
    },
    {
      "epoch": 6.0,
      "grad_norm": 1.7473315000534058,
      "learning_rate": 6.613065326633166e-05,
      "loss": 0.1576,
      "step": 672
    },
    {
      "epoch": 6.008988764044944,
      "grad_norm": 0.47342148423194885,
      "learning_rate": 6.592964824120603e-05,
      "loss": 0.0637,
      "step": 673
    },
    {
      "epoch": 6.0179775280898875,
      "grad_norm": 0.48591601848602295,
      "learning_rate": 6.57286432160804e-05,
      "loss": 0.0586,
      "step": 674
    },
    {
      "epoch": 6.026966292134832,
      "grad_norm": 0.4300891160964966,
      "learning_rate": 6.552763819095478e-05,
      "loss": 0.0545,
      "step": 675
    },
    {
      "epoch": 6.035955056179775,
      "grad_norm": 0.5361961722373962,
      "learning_rate": 6.532663316582915e-05,
      "loss": 0.0732,
      "step": 676
    },
    {
      "epoch": 6.044943820224719,
      "grad_norm": 0.5610045790672302,
      "learning_rate": 6.512562814070352e-05,
      "loss": 0.0554,
      "step": 677
    },
    {
      "epoch": 6.0539325842696625,
      "grad_norm": 0.7032301425933838,
      "learning_rate": 6.492462311557788e-05,
      "loss": 0.0828,
      "step": 678
    },
    {
      "epoch": 6.062921348314607,
      "grad_norm": 0.8052923083305359,
      "learning_rate": 6.472361809045227e-05,
      "loss": 0.0867,
      "step": 679
    },
    {
      "epoch": 6.071910112359551,
      "grad_norm": 0.5502392053604126,
      "learning_rate": 6.452261306532663e-05,
      "loss": 0.0515,
      "step": 680
    },
    {
      "epoch": 6.080898876404494,
      "grad_norm": 0.555253267288208,
      "learning_rate": 6.4321608040201e-05,
      "loss": 0.0573,
      "step": 681
    },
    {
      "epoch": 6.089887640449438,
      "grad_norm": 1.016706109046936,
      "learning_rate": 6.412060301507538e-05,
      "loss": 0.0656,
      "step": 682
    },
    {
      "epoch": 6.098876404494382,
      "grad_norm": 0.6757444739341736,
      "learning_rate": 6.391959798994975e-05,
      "loss": 0.0604,
      "step": 683
    },
    {
      "epoch": 6.107865168539326,
      "grad_norm": 0.8290342688560486,
      "learning_rate": 6.371859296482412e-05,
      "loss": 0.0758,
      "step": 684
    },
    {
      "epoch": 6.116853932584269,
      "grad_norm": 0.8580583333969116,
      "learning_rate": 6.35175879396985e-05,
      "loss": 0.0864,
      "step": 685
    },
    {
      "epoch": 6.125842696629213,
      "grad_norm": 0.898958683013916,
      "learning_rate": 6.331658291457287e-05,
      "loss": 0.0717,
      "step": 686
    },
    {
      "epoch": 6.134831460674158,
      "grad_norm": 0.6996578574180603,
      "learning_rate": 6.311557788944724e-05,
      "loss": 0.0585,
      "step": 687
    },
    {
      "epoch": 6.143820224719101,
      "grad_norm": 0.7565838694572449,
      "learning_rate": 6.291457286432161e-05,
      "loss": 0.0709,
      "step": 688
    },
    {
      "epoch": 6.152808988764045,
      "grad_norm": 0.670695960521698,
      "learning_rate": 6.271356783919599e-05,
      "loss": 0.0631,
      "step": 689
    },
    {
      "epoch": 6.1617977528089884,
      "grad_norm": 0.7585590481758118,
      "learning_rate": 6.251256281407035e-05,
      "loss": 0.0673,
      "step": 690
    },
    {
      "epoch": 6.170786516853933,
      "grad_norm": 0.6762809157371521,
      "learning_rate": 6.231155778894473e-05,
      "loss": 0.0636,
      "step": 691
    },
    {
      "epoch": 6.179775280898877,
      "grad_norm": 0.7354475855827332,
      "learning_rate": 6.211055276381909e-05,
      "loss": 0.0682,
      "step": 692
    },
    {
      "epoch": 6.18876404494382,
      "grad_norm": 0.7206984162330627,
      "learning_rate": 6.190954773869348e-05,
      "loss": 0.0673,
      "step": 693
    },
    {
      "epoch": 6.197752808988764,
      "grad_norm": 0.704343855381012,
      "learning_rate": 6.170854271356784e-05,
      "loss": 0.0698,
      "step": 694
    },
    {
      "epoch": 6.206741573033708,
      "grad_norm": 0.7133187651634216,
      "learning_rate": 6.150753768844222e-05,
      "loss": 0.063,
      "step": 695
    },
    {
      "epoch": 6.215730337078652,
      "grad_norm": 0.5864713191986084,
      "learning_rate": 6.130653266331658e-05,
      "loss": 0.0583,
      "step": 696
    },
    {
      "epoch": 6.224719101123595,
      "grad_norm": 0.7760382890701294,
      "learning_rate": 6.110552763819096e-05,
      "loss": 0.0734,
      "step": 697
    },
    {
      "epoch": 6.233707865168539,
      "grad_norm": 0.6779539585113525,
      "learning_rate": 6.090452261306533e-05,
      "loss": 0.0668,
      "step": 698
    },
    {
      "epoch": 6.242696629213484,
      "grad_norm": 0.7222036123275757,
      "learning_rate": 6.070351758793971e-05,
      "loss": 0.0714,
      "step": 699
    },
    {
      "epoch": 6.251685393258427,
      "grad_norm": 0.7591395378112793,
      "learning_rate": 6.0502512562814076e-05,
      "loss": 0.0796,
      "step": 700
    },
    {
      "epoch": 6.260674157303371,
      "grad_norm": 0.8065852522850037,
      "learning_rate": 6.030150753768844e-05,
      "loss": 0.0869,
      "step": 701
    },
    {
      "epoch": 6.269662921348314,
      "grad_norm": 0.6894078254699707,
      "learning_rate": 6.0100502512562815e-05,
      "loss": 0.0661,
      "step": 702
    },
    {
      "epoch": 6.278651685393259,
      "grad_norm": 0.6169759035110474,
      "learning_rate": 5.989949748743718e-05,
      "loss": 0.062,
      "step": 703
    },
    {
      "epoch": 6.287640449438202,
      "grad_norm": 0.6180002093315125,
      "learning_rate": 5.969849246231156e-05,
      "loss": 0.0583,
      "step": 704
    },
    {
      "epoch": 6.296629213483146,
      "grad_norm": 0.5800358057022095,
      "learning_rate": 5.949748743718593e-05,
      "loss": 0.0566,
      "step": 705
    },
    {
      "epoch": 6.30561797752809,
      "grad_norm": 0.6424681544303894,
      "learning_rate": 5.929648241206031e-05,
      "loss": 0.0613,
      "step": 706
    },
    {
      "epoch": 6.314606741573034,
      "grad_norm": 0.6681240797042847,
      "learning_rate": 5.909547738693467e-05,
      "loss": 0.0552,
      "step": 707
    },
    {
      "epoch": 6.323595505617978,
      "grad_norm": 0.8820522427558899,
      "learning_rate": 5.889447236180905e-05,
      "loss": 0.0806,
      "step": 708
    },
    {
      "epoch": 6.332584269662921,
      "grad_norm": 0.697541356086731,
      "learning_rate": 5.869346733668342e-05,
      "loss": 0.0681,
      "step": 709
    },
    {
      "epoch": 6.341573033707865,
      "grad_norm": 0.9000743627548218,
      "learning_rate": 5.849246231155779e-05,
      "loss": 0.08,
      "step": 710
    },
    {
      "epoch": 6.350561797752809,
      "grad_norm": 0.7504714131355286,
      "learning_rate": 5.829145728643216e-05,
      "loss": 0.0717,
      "step": 711
    },
    {
      "epoch": 6.359550561797753,
      "grad_norm": 0.6185814142227173,
      "learning_rate": 5.809045226130654e-05,
      "loss": 0.0674,
      "step": 712
    },
    {
      "epoch": 6.368539325842697,
      "grad_norm": 0.7378799915313721,
      "learning_rate": 5.7889447236180904e-05,
      "loss": 0.0805,
      "step": 713
    },
    {
      "epoch": 6.37752808988764,
      "grad_norm": 0.8155654072761536,
      "learning_rate": 5.7688442211055284e-05,
      "loss": 0.074,
      "step": 714
    },
    {
      "epoch": 6.3865168539325845,
      "grad_norm": 0.5926880240440369,
      "learning_rate": 5.748743718592965e-05,
      "loss": 0.0606,
      "step": 715
    },
    {
      "epoch": 6.395505617977528,
      "grad_norm": 0.6187089681625366,
      "learning_rate": 5.728643216080403e-05,
      "loss": 0.0567,
      "step": 716
    },
    {
      "epoch": 6.404494382022472,
      "grad_norm": 0.6285867691040039,
      "learning_rate": 5.7085427135678396e-05,
      "loss": 0.0678,
      "step": 717
    },
    {
      "epoch": 6.413483146067415,
      "grad_norm": 0.5826138854026794,
      "learning_rate": 5.688442211055277e-05,
      "loss": 0.0555,
      "step": 718
    },
    {
      "epoch": 6.4224719101123595,
      "grad_norm": 0.7886212468147278,
      "learning_rate": 5.6683417085427135e-05,
      "loss": 0.0757,
      "step": 719
    },
    {
      "epoch": 6.431460674157304,
      "grad_norm": 1.1480172872543335,
      "learning_rate": 5.6482412060301515e-05,
      "loss": 0.074,
      "step": 720
    },
    {
      "epoch": 6.440449438202247,
      "grad_norm": 0.7989625930786133,
      "learning_rate": 5.628140703517588e-05,
      "loss": 0.0732,
      "step": 721
    },
    {
      "epoch": 6.449438202247191,
      "grad_norm": 0.7428969144821167,
      "learning_rate": 5.608040201005026e-05,
      "loss": 0.0774,
      "step": 722
    },
    {
      "epoch": 6.4584269662921345,
      "grad_norm": 0.6258492469787598,
      "learning_rate": 5.587939698492463e-05,
      "loss": 0.0589,
      "step": 723
    },
    {
      "epoch": 6.467415730337079,
      "grad_norm": 0.8702659606933594,
      "learning_rate": 5.567839195979899e-05,
      "loss": 0.0845,
      "step": 724
    },
    {
      "epoch": 6.476404494382022,
      "grad_norm": 0.5378155708312988,
      "learning_rate": 5.547738693467337e-05,
      "loss": 0.0601,
      "step": 725
    },
    {
      "epoch": 6.485393258426966,
      "grad_norm": 0.7958455681800842,
      "learning_rate": 5.527638190954774e-05,
      "loss": 0.0751,
      "step": 726
    },
    {
      "epoch": 6.49438202247191,
      "grad_norm": 0.49194905161857605,
      "learning_rate": 5.507537688442211e-05,
      "loss": 0.0502,
      "step": 727
    },
    {
      "epoch": 6.503370786516854,
      "grad_norm": 0.8069722056388855,
      "learning_rate": 5.487437185929648e-05,
      "loss": 0.0757,
      "step": 728
    },
    {
      "epoch": 6.512359550561798,
      "grad_norm": 0.7816961407661438,
      "learning_rate": 5.467336683417086e-05,
      "loss": 0.0811,
      "step": 729
    },
    {
      "epoch": 6.521348314606741,
      "grad_norm": 0.5912923812866211,
      "learning_rate": 5.4472361809045224e-05,
      "loss": 0.0566,
      "step": 730
    },
    {
      "epoch": 6.5303370786516854,
      "grad_norm": 0.8037512898445129,
      "learning_rate": 5.4271356783919604e-05,
      "loss": 0.0827,
      "step": 731
    },
    {
      "epoch": 6.539325842696629,
      "grad_norm": 0.5931857228279114,
      "learning_rate": 5.407035175879397e-05,
      "loss": 0.0509,
      "step": 732
    },
    {
      "epoch": 6.548314606741573,
      "grad_norm": 0.7428365349769592,
      "learning_rate": 5.386934673366835e-05,
      "loss": 0.073,
      "step": 733
    },
    {
      "epoch": 6.557303370786517,
      "grad_norm": 0.8081555366516113,
      "learning_rate": 5.3668341708542716e-05,
      "loss": 0.077,
      "step": 734
    },
    {
      "epoch": 6.5662921348314605,
      "grad_norm": 0.6868345141410828,
      "learning_rate": 5.346733668341709e-05,
      "loss": 0.0657,
      "step": 735
    },
    {
      "epoch": 6.575280898876405,
      "grad_norm": 0.670045018196106,
      "learning_rate": 5.3266331658291455e-05,
      "loss": 0.0668,
      "step": 736
    },
    {
      "epoch": 6.584269662921348,
      "grad_norm": 0.5548645257949829,
      "learning_rate": 5.3065326633165835e-05,
      "loss": 0.05,
      "step": 737
    },
    {
      "epoch": 6.593258426966292,
      "grad_norm": 0.719646692276001,
      "learning_rate": 5.28643216080402e-05,
      "loss": 0.0561,
      "step": 738
    },
    {
      "epoch": 6.6022471910112355,
      "grad_norm": 0.7115384340286255,
      "learning_rate": 5.266331658291458e-05,
      "loss": 0.0714,
      "step": 739
    },
    {
      "epoch": 6.61123595505618,
      "grad_norm": 0.8859148025512695,
      "learning_rate": 5.246231155778895e-05,
      "loss": 0.0736,
      "step": 740
    },
    {
      "epoch": 6.620224719101124,
      "grad_norm": 0.7739696502685547,
      "learning_rate": 5.226130653266332e-05,
      "loss": 0.0811,
      "step": 741
    },
    {
      "epoch": 6.629213483146067,
      "grad_norm": 0.7735329270362854,
      "learning_rate": 5.206030150753769e-05,
      "loss": 0.0757,
      "step": 742
    },
    {
      "epoch": 6.638202247191011,
      "grad_norm": 0.5577191710472107,
      "learning_rate": 5.1859296482412066e-05,
      "loss": 0.0534,
      "step": 743
    },
    {
      "epoch": 6.647191011235955,
      "grad_norm": 0.6232277750968933,
      "learning_rate": 5.165829145728643e-05,
      "loss": 0.0699,
      "step": 744
    },
    {
      "epoch": 6.656179775280899,
      "grad_norm": 0.5497653484344482,
      "learning_rate": 5.145728643216081e-05,
      "loss": 0.0523,
      "step": 745
    },
    {
      "epoch": 6.665168539325843,
      "grad_norm": 0.8625667691230774,
      "learning_rate": 5.125628140703518e-05,
      "loss": 0.0824,
      "step": 746
    },
    {
      "epoch": 6.674157303370786,
      "grad_norm": 0.6303309798240662,
      "learning_rate": 5.1055276381909544e-05,
      "loss": 0.0632,
      "step": 747
    },
    {
      "epoch": 6.683146067415731,
      "grad_norm": 0.5429088473320007,
      "learning_rate": 5.0854271356783924e-05,
      "loss": 0.0581,
      "step": 748
    },
    {
      "epoch": 6.692134831460674,
      "grad_norm": 0.7641626000404358,
      "learning_rate": 5.065326633165829e-05,
      "loss": 0.0752,
      "step": 749
    },
    {
      "epoch": 6.701123595505618,
      "grad_norm": 0.7470322251319885,
      "learning_rate": 5.045226130653266e-05,
      "loss": 0.077,
      "step": 750
    },
    {
      "epoch": 6.710112359550562,
      "grad_norm": 0.6611917614936829,
      "learning_rate": 5.0251256281407036e-05,
      "loss": 0.058,
      "step": 751
    },
    {
      "epoch": 6.719101123595506,
      "grad_norm": 0.7596578001976013,
      "learning_rate": 5.005025125628141e-05,
      "loss": 0.0738,
      "step": 752
    },
    {
      "epoch": 6.72808988764045,
      "grad_norm": 0.7737931609153748,
      "learning_rate": 4.984924623115578e-05,
      "loss": 0.0765,
      "step": 753
    },
    {
      "epoch": 6.737078651685393,
      "grad_norm": 0.7285981774330139,
      "learning_rate": 4.9648241206030155e-05,
      "loss": 0.0689,
      "step": 754
    },
    {
      "epoch": 6.746067415730337,
      "grad_norm": 0.8735149502754211,
      "learning_rate": 4.944723618090453e-05,
      "loss": 0.0741,
      "step": 755
    },
    {
      "epoch": 6.755056179775281,
      "grad_norm": 0.8341994285583496,
      "learning_rate": 4.92462311557789e-05,
      "loss": 0.0652,
      "step": 756
    },
    {
      "epoch": 6.764044943820225,
      "grad_norm": 0.7208125591278076,
      "learning_rate": 4.9045226130653274e-05,
      "loss": 0.061,
      "step": 757
    },
    {
      "epoch": 6.773033707865169,
      "grad_norm": 0.755291759967804,
      "learning_rate": 4.884422110552764e-05,
      "loss": 0.0638,
      "step": 758
    },
    {
      "epoch": 6.782022471910112,
      "grad_norm": 0.700304388999939,
      "learning_rate": 4.864321608040201e-05,
      "loss": 0.0636,
      "step": 759
    },
    {
      "epoch": 6.7910112359550565,
      "grad_norm": 0.6279141306877136,
      "learning_rate": 4.844221105527638e-05,
      "loss": 0.0603,
      "step": 760
    },
    {
      "epoch": 6.8,
      "grad_norm": 0.7656819224357605,
      "learning_rate": 4.824120603015075e-05,
      "loss": 0.0678,
      "step": 761
    },
    {
      "epoch": 6.808988764044944,
      "grad_norm": 0.7003180384635925,
      "learning_rate": 4.8040201005025125e-05,
      "loss": 0.0721,
      "step": 762
    },
    {
      "epoch": 6.817977528089887,
      "grad_norm": 0.68515545129776,
      "learning_rate": 4.78391959798995e-05,
      "loss": 0.0679,
      "step": 763
    },
    {
      "epoch": 6.8269662921348315,
      "grad_norm": 0.6718376874923706,
      "learning_rate": 4.763819095477387e-05,
      "loss": 0.0638,
      "step": 764
    },
    {
      "epoch": 6.835955056179776,
      "grad_norm": 0.6175905466079712,
      "learning_rate": 4.7437185929648244e-05,
      "loss": 0.0568,
      "step": 765
    },
    {
      "epoch": 6.844943820224719,
      "grad_norm": 0.8275371789932251,
      "learning_rate": 4.723618090452262e-05,
      "loss": 0.0863,
      "step": 766
    },
    {
      "epoch": 6.853932584269663,
      "grad_norm": 0.638264536857605,
      "learning_rate": 4.703517587939698e-05,
      "loss": 0.0617,
      "step": 767
    },
    {
      "epoch": 6.8629213483146065,
      "grad_norm": 0.7203929424285889,
      "learning_rate": 4.6834170854271356e-05,
      "loss": 0.0608,
      "step": 768
    },
    {
      "epoch": 6.871910112359551,
      "grad_norm": 0.8957692980766296,
      "learning_rate": 4.663316582914573e-05,
      "loss": 0.0877,
      "step": 769
    },
    {
      "epoch": 6.880898876404494,
      "grad_norm": 0.8246579170227051,
      "learning_rate": 4.64321608040201e-05,
      "loss": 0.0751,
      "step": 770
    },
    {
      "epoch": 6.889887640449438,
      "grad_norm": 0.7794975638389587,
      "learning_rate": 4.6231155778894475e-05,
      "loss": 0.0658,
      "step": 771
    },
    {
      "epoch": 6.898876404494382,
      "grad_norm": 0.603617250919342,
      "learning_rate": 4.603015075376885e-05,
      "loss": 0.0462,
      "step": 772
    },
    {
      "epoch": 6.907865168539326,
      "grad_norm": 0.7251864075660706,
      "learning_rate": 4.582914572864322e-05,
      "loss": 0.074,
      "step": 773
    },
    {
      "epoch": 6.91685393258427,
      "grad_norm": 0.9155154228210449,
      "learning_rate": 4.5628140703517594e-05,
      "loss": 0.0954,
      "step": 774
    },
    {
      "epoch": 6.925842696629213,
      "grad_norm": 0.7886826992034912,
      "learning_rate": 4.542713567839196e-05,
      "loss": 0.0676,
      "step": 775
    },
    {
      "epoch": 6.9348314606741575,
      "grad_norm": 0.6646925210952759,
      "learning_rate": 4.522613065326633e-05,
      "loss": 0.0674,
      "step": 776
    },
    {
      "epoch": 6.943820224719101,
      "grad_norm": 0.6266483068466187,
      "learning_rate": 4.5025125628140706e-05,
      "loss": 0.0519,
      "step": 777
    },
    {
      "epoch": 6.952808988764045,
      "grad_norm": 0.6545954346656799,
      "learning_rate": 4.482412060301508e-05,
      "loss": 0.0626,
      "step": 778
    },
    {
      "epoch": 6.961797752808989,
      "grad_norm": 0.8488231897354126,
      "learning_rate": 4.462311557788945e-05,
      "loss": 0.0854,
      "step": 779
    },
    {
      "epoch": 6.9707865168539325,
      "grad_norm": 0.7219066023826599,
      "learning_rate": 4.4422110552763825e-05,
      "loss": 0.0617,
      "step": 780
    },
    {
      "epoch": 6.979775280898877,
      "grad_norm": 0.7892714142799377,
      "learning_rate": 4.42211055276382e-05,
      "loss": 0.0852,
      "step": 781
    },
    {
      "epoch": 6.98876404494382,
      "grad_norm": 0.7093201279640198,
      "learning_rate": 4.4020100502512564e-05,
      "loss": 0.0651,
      "step": 782
    },
    {
      "epoch": 6.997752808988764,
      "grad_norm": 0.4873143136501312,
      "learning_rate": 4.381909547738694e-05,
      "loss": 0.0462,
      "step": 783
    },
    {
      "epoch": 7.0,
      "grad_norm": 1.3385456800460815,
      "learning_rate": 4.3618090452261303e-05,
      "loss": 0.0676,
      "step": 784
    },
    {
      "epoch": 7.008988764044944,
      "grad_norm": 0.4527382254600525,
      "learning_rate": 4.3417085427135676e-05,
      "loss": 0.0394,
      "step": 785
    },
    {
      "epoch": 7.0179775280898875,
      "grad_norm": 0.3718474209308624,
      "learning_rate": 4.321608040201005e-05,
      "loss": 0.0383,
      "step": 786
    },
    {
      "epoch": 7.026966292134832,
      "grad_norm": 0.3365932106971741,
      "learning_rate": 4.301507537688442e-05,
      "loss": 0.0317,
      "step": 787
    },
    {
      "epoch": 7.035955056179775,
      "grad_norm": 0.32694289088249207,
      "learning_rate": 4.2814070351758795e-05,
      "loss": 0.0368,
      "step": 788
    },
    {
      "epoch": 7.044943820224719,
      "grad_norm": 0.41391873359680176,
      "learning_rate": 4.261306532663317e-05,
      "loss": 0.0344,
      "step": 789
    },
    {
      "epoch": 7.0539325842696625,
      "grad_norm": 0.451825350522995,
      "learning_rate": 4.241206030150754e-05,
      "loss": 0.0419,
      "step": 790
    },
    {
      "epoch": 7.062921348314607,
      "grad_norm": 0.36821404099464417,
      "learning_rate": 4.2211055276381914e-05,
      "loss": 0.0345,
      "step": 791
    },
    {
      "epoch": 7.071910112359551,
      "grad_norm": 0.45206981897354126,
      "learning_rate": 4.201005025125628e-05,
      "loss": 0.0383,
      "step": 792
    },
    {
      "epoch": 7.080898876404494,
      "grad_norm": 0.3668767511844635,
      "learning_rate": 4.180904522613065e-05,
      "loss": 0.031,
      "step": 793
    },
    {
      "epoch": 7.089887640449438,
      "grad_norm": 0.4501636326313019,
      "learning_rate": 4.1608040201005026e-05,
      "loss": 0.0417,
      "step": 794
    },
    {
      "epoch": 7.098876404494382,
      "grad_norm": 0.5076785087585449,
      "learning_rate": 4.14070351758794e-05,
      "loss": 0.0464,
      "step": 795
    },
    {
      "epoch": 7.107865168539326,
      "grad_norm": 0.5310789942741394,
      "learning_rate": 4.120603015075377e-05,
      "loss": 0.0439,
      "step": 796
    },
    {
      "epoch": 7.116853932584269,
      "grad_norm": 0.4547147750854492,
      "learning_rate": 4.1005025125628145e-05,
      "loss": 0.0379,
      "step": 797
    },
    {
      "epoch": 7.125842696629213,
      "grad_norm": 0.44506409764289856,
      "learning_rate": 4.080402010050252e-05,
      "loss": 0.0375,
      "step": 798
    },
    {
      "epoch": 7.134831460674158,
      "grad_norm": 0.36078372597694397,
      "learning_rate": 4.060301507537689e-05,
      "loss": 0.0332,
      "step": 799
    },
    {
      "epoch": 7.143820224719101,
      "grad_norm": 0.4225924611091614,
      "learning_rate": 4.040201005025126e-05,
      "loss": 0.0368,
      "step": 800
    },
    {
      "epoch": 7.152808988764045,
      "grad_norm": 0.3723277449607849,
      "learning_rate": 4.020100502512563e-05,
      "loss": 0.0313,
      "step": 801
    },
    {
      "epoch": 7.1617977528089884,
      "grad_norm": 0.48562607169151306,
      "learning_rate": 4e-05,
      "loss": 0.0406,
      "step": 802
    },
    {
      "epoch": 7.170786516853933,
      "grad_norm": 0.43685677647590637,
      "learning_rate": 3.9798994974874376e-05,
      "loss": 0.0355,
      "step": 803
    },
    {
      "epoch": 7.179775280898877,
      "grad_norm": 0.36712646484375,
      "learning_rate": 3.959798994974875e-05,
      "loss": 0.0302,
      "step": 804
    },
    {
      "epoch": 7.18876404494382,
      "grad_norm": 0.6077165603637695,
      "learning_rate": 3.9396984924623115e-05,
      "loss": 0.0421,
      "step": 805
    },
    {
      "epoch": 7.197752808988764,
      "grad_norm": 0.4424956440925598,
      "learning_rate": 3.919597989949749e-05,
      "loss": 0.0411,
      "step": 806
    },
    {
      "epoch": 7.206741573033708,
      "grad_norm": 0.44272226095199585,
      "learning_rate": 3.899497487437186e-05,
      "loss": 0.0372,
      "step": 807
    },
    {
      "epoch": 7.215730337078652,
      "grad_norm": 0.5057337880134583,
      "learning_rate": 3.8793969849246234e-05,
      "loss": 0.0389,
      "step": 808
    },
    {
      "epoch": 7.224719101123595,
      "grad_norm": 0.5174487829208374,
      "learning_rate": 3.85929648241206e-05,
      "loss": 0.0453,
      "step": 809
    },
    {
      "epoch": 7.233707865168539,
      "grad_norm": 0.4579974114894867,
      "learning_rate": 3.8391959798994973e-05,
      "loss": 0.039,
      "step": 810
    },
    {
      "epoch": 7.242696629213484,
      "grad_norm": 0.4539264440536499,
      "learning_rate": 3.8190954773869346e-05,
      "loss": 0.0327,
      "step": 811
    },
    {
      "epoch": 7.251685393258427,
      "grad_norm": 0.5071107745170593,
      "learning_rate": 3.798994974874372e-05,
      "loss": 0.0401,
      "step": 812
    },
    {
      "epoch": 7.260674157303371,
      "grad_norm": 0.6033917665481567,
      "learning_rate": 3.778894472361809e-05,
      "loss": 0.0375,
      "step": 813
    },
    {
      "epoch": 7.269662921348314,
      "grad_norm": 0.4325549900531769,
      "learning_rate": 3.7587939698492465e-05,
      "loss": 0.0378,
      "step": 814
    },
    {
      "epoch": 7.278651685393259,
      "grad_norm": 0.3446965217590332,
      "learning_rate": 3.738693467336684e-05,
      "loss": 0.0294,
      "step": 815
    },
    {
      "epoch": 7.287640449438202,
      "grad_norm": 0.46213191747665405,
      "learning_rate": 3.7185929648241204e-05,
      "loss": 0.0385,
      "step": 816
    },
    {
      "epoch": 7.296629213483146,
      "grad_norm": 0.4626966118812561,
      "learning_rate": 3.698492462311558e-05,
      "loss": 0.0395,
      "step": 817
    },
    {
      "epoch": 7.30561797752809,
      "grad_norm": 0.5682381391525269,
      "learning_rate": 3.678391959798995e-05,
      "loss": 0.0484,
      "step": 818
    },
    {
      "epoch": 7.314606741573034,
      "grad_norm": 0.5304281711578369,
      "learning_rate": 3.658291457286432e-05,
      "loss": 0.0389,
      "step": 819
    },
    {
      "epoch": 7.323595505617978,
      "grad_norm": 0.4500056803226471,
      "learning_rate": 3.6381909547738696e-05,
      "loss": 0.031,
      "step": 820
    },
    {
      "epoch": 7.332584269662921,
      "grad_norm": 0.44231685996055603,
      "learning_rate": 3.618090452261307e-05,
      "loss": 0.0404,
      "step": 821
    },
    {
      "epoch": 7.341573033707865,
      "grad_norm": 0.4873095452785492,
      "learning_rate": 3.597989949748744e-05,
      "loss": 0.0411,
      "step": 822
    },
    {
      "epoch": 7.350561797752809,
      "grad_norm": 0.5049818754196167,
      "learning_rate": 3.5778894472361815e-05,
      "loss": 0.0413,
      "step": 823
    },
    {
      "epoch": 7.359550561797753,
      "grad_norm": 0.36374884843826294,
      "learning_rate": 3.557788944723618e-05,
      "loss": 0.0294,
      "step": 824
    },
    {
      "epoch": 7.368539325842697,
      "grad_norm": 0.4328358471393585,
      "learning_rate": 3.5376884422110554e-05,
      "loss": 0.0353,
      "step": 825
    },
    {
      "epoch": 7.37752808988764,
      "grad_norm": 0.42645445466041565,
      "learning_rate": 3.517587939698493e-05,
      "loss": 0.0362,
      "step": 826
    },
    {
      "epoch": 7.3865168539325845,
      "grad_norm": 0.4536456763744354,
      "learning_rate": 3.49748743718593e-05,
      "loss": 0.0383,
      "step": 827
    },
    {
      "epoch": 7.395505617977528,
      "grad_norm": 0.3997233211994171,
      "learning_rate": 3.4773869346733667e-05,
      "loss": 0.0368,
      "step": 828
    },
    {
      "epoch": 7.404494382022472,
      "grad_norm": 0.6861959099769592,
      "learning_rate": 3.457286432160804e-05,
      "loss": 0.0507,
      "step": 829
    },
    {
      "epoch": 7.413483146067415,
      "grad_norm": 0.48148271441459656,
      "learning_rate": 3.437185929648241e-05,
      "loss": 0.0326,
      "step": 830
    },
    {
      "epoch": 7.4224719101123595,
      "grad_norm": 0.3764684796333313,
      "learning_rate": 3.4170854271356785e-05,
      "loss": 0.0337,
      "step": 831
    },
    {
      "epoch": 7.431460674157304,
      "grad_norm": 0.38522040843963623,
      "learning_rate": 3.396984924623116e-05,
      "loss": 0.0332,
      "step": 832
    },
    {
      "epoch": 7.440449438202247,
      "grad_norm": 0.33813750743865967,
      "learning_rate": 3.3768844221105525e-05,
      "loss": 0.0288,
      "step": 833
    },
    {
      "epoch": 7.449438202247191,
      "grad_norm": 0.387182354927063,
      "learning_rate": 3.35678391959799e-05,
      "loss": 0.0341,
      "step": 834
    },
    {
      "epoch": 7.4584269662921345,
      "grad_norm": 0.3717330992221832,
      "learning_rate": 3.336683417085427e-05,
      "loss": 0.0318,
      "step": 835
    },
    {
      "epoch": 7.467415730337079,
      "grad_norm": 0.48821160197257996,
      "learning_rate": 3.3165829145728643e-05,
      "loss": 0.0398,
      "step": 836
    },
    {
      "epoch": 7.476404494382022,
      "grad_norm": 0.46169987320899963,
      "learning_rate": 3.2964824120603016e-05,
      "loss": 0.0359,
      "step": 837
    },
    {
      "epoch": 7.485393258426966,
      "grad_norm": 0.39277175068855286,
      "learning_rate": 3.276381909547739e-05,
      "loss": 0.0336,
      "step": 838
    },
    {
      "epoch": 7.49438202247191,
      "grad_norm": 0.44021153450012207,
      "learning_rate": 3.256281407035176e-05,
      "loss": 0.0386,
      "step": 839
    },
    {
      "epoch": 7.503370786516854,
      "grad_norm": 0.4229154586791992,
      "learning_rate": 3.2361809045226135e-05,
      "loss": 0.0366,
      "step": 840
    },
    {
      "epoch": 7.512359550561798,
      "grad_norm": 0.44314783811569214,
      "learning_rate": 3.21608040201005e-05,
      "loss": 0.0396,
      "step": 841
    },
    {
      "epoch": 7.521348314606741,
      "grad_norm": 0.5530552864074707,
      "learning_rate": 3.1959798994974875e-05,
      "loss": 0.0433,
      "step": 842
    },
    {
      "epoch": 7.5303370786516854,
      "grad_norm": 0.41987141966819763,
      "learning_rate": 3.175879396984925e-05,
      "loss": 0.0386,
      "step": 843
    },
    {
      "epoch": 7.539325842696629,
      "grad_norm": 0.4640938639640808,
      "learning_rate": 3.155778894472362e-05,
      "loss": 0.0395,
      "step": 844
    },
    {
      "epoch": 7.548314606741573,
      "grad_norm": 0.38798192143440247,
      "learning_rate": 3.1356783919597993e-05,
      "loss": 0.0361,
      "step": 845
    },
    {
      "epoch": 7.557303370786517,
      "grad_norm": 0.4681292474269867,
      "learning_rate": 3.1155778894472366e-05,
      "loss": 0.0358,
      "step": 846
    },
    {
      "epoch": 7.5662921348314605,
      "grad_norm": 0.43131551146507263,
      "learning_rate": 3.095477386934674e-05,
      "loss": 0.0341,
      "step": 847
    },
    {
      "epoch": 7.575280898876405,
      "grad_norm": 0.373380571603775,
      "learning_rate": 3.075376884422111e-05,
      "loss": 0.0339,
      "step": 848
    },
    {
      "epoch": 7.584269662921348,
      "grad_norm": 0.45985284447669983,
      "learning_rate": 3.055276381909548e-05,
      "loss": 0.0387,
      "step": 849
    },
    {
      "epoch": 7.593258426966292,
      "grad_norm": 0.4335872530937195,
      "learning_rate": 3.0351758793969855e-05,
      "loss": 0.0367,
      "step": 850
    },
    {
      "epoch": 7.6022471910112355,
      "grad_norm": 0.5015818476676941,
      "learning_rate": 3.015075376884422e-05,
      "loss": 0.0424,
      "step": 851
    },
    {
      "epoch": 7.61123595505618,
      "grad_norm": 0.42663389444351196,
      "learning_rate": 2.994974874371859e-05,
      "loss": 0.0385,
      "step": 852
    },
    {
      "epoch": 7.620224719101124,
      "grad_norm": 0.47200289368629456,
      "learning_rate": 2.9748743718592964e-05,
      "loss": 0.0444,
      "step": 853
    },
    {
      "epoch": 7.629213483146067,
      "grad_norm": 0.5575270652770996,
      "learning_rate": 2.9547738693467337e-05,
      "loss": 0.0387,
      "step": 854
    },
    {
      "epoch": 7.638202247191011,
      "grad_norm": 0.4276168644428253,
      "learning_rate": 2.934673366834171e-05,
      "loss": 0.0338,
      "step": 855
    },
    {
      "epoch": 7.647191011235955,
      "grad_norm": 0.4376341700553894,
      "learning_rate": 2.914572864321608e-05,
      "loss": 0.0376,
      "step": 856
    },
    {
      "epoch": 7.656179775280899,
      "grad_norm": 0.43714743852615356,
      "learning_rate": 2.8944723618090452e-05,
      "loss": 0.0364,
      "step": 857
    },
    {
      "epoch": 7.665168539325843,
      "grad_norm": 0.41881683468818665,
      "learning_rate": 2.8743718592964825e-05,
      "loss": 0.0354,
      "step": 858
    },
    {
      "epoch": 7.674157303370786,
      "grad_norm": 0.45058584213256836,
      "learning_rate": 2.8542713567839198e-05,
      "loss": 0.0375,
      "step": 859
    },
    {
      "epoch": 7.683146067415731,
      "grad_norm": 0.47309252619743347,
      "learning_rate": 2.8341708542713568e-05,
      "loss": 0.0374,
      "step": 860
    },
    {
      "epoch": 7.692134831460674,
      "grad_norm": 0.47067949175834656,
      "learning_rate": 2.814070351758794e-05,
      "loss": 0.0423,
      "step": 861
    },
    {
      "epoch": 7.701123595505618,
      "grad_norm": 0.47309669852256775,
      "learning_rate": 2.7939698492462314e-05,
      "loss": 0.039,
      "step": 862
    },
    {
      "epoch": 7.710112359550562,
      "grad_norm": 0.39653855562210083,
      "learning_rate": 2.7738693467336686e-05,
      "loss": 0.0305,
      "step": 863
    },
    {
      "epoch": 7.719101123595506,
      "grad_norm": 0.44341570138931274,
      "learning_rate": 2.7537688442211056e-05,
      "loss": 0.0374,
      "step": 864
    },
    {
      "epoch": 7.72808988764045,
      "grad_norm": 0.5241318345069885,
      "learning_rate": 2.733668341708543e-05,
      "loss": 0.0416,
      "step": 865
    },
    {
      "epoch": 7.737078651685393,
      "grad_norm": 0.4053889513015747,
      "learning_rate": 2.7135678391959802e-05,
      "loss": 0.0365,
      "step": 866
    },
    {
      "epoch": 7.746067415730337,
      "grad_norm": 0.5533449649810791,
      "learning_rate": 2.6934673366834175e-05,
      "loss": 0.0405,
      "step": 867
    },
    {
      "epoch": 7.755056179775281,
      "grad_norm": 0.3946499824523926,
      "learning_rate": 2.6733668341708545e-05,
      "loss": 0.0336,
      "step": 868
    },
    {
      "epoch": 7.764044943820225,
      "grad_norm": 0.47879308462142944,
      "learning_rate": 2.6532663316582917e-05,
      "loss": 0.0355,
      "step": 869
    },
    {
      "epoch": 7.773033707865169,
      "grad_norm": 0.5289047956466675,
      "learning_rate": 2.633165829145729e-05,
      "loss": 0.0404,
      "step": 870
    },
    {
      "epoch": 7.782022471910112,
      "grad_norm": 0.4350457787513733,
      "learning_rate": 2.613065326633166e-05,
      "loss": 0.0355,
      "step": 871
    },
    {
      "epoch": 7.7910112359550565,
      "grad_norm": 0.4283429980278015,
      "learning_rate": 2.5929648241206033e-05,
      "loss": 0.0372,
      "step": 872
    },
    {
      "epoch": 7.8,
      "grad_norm": 0.4839974343776703,
      "learning_rate": 2.5728643216080406e-05,
      "loss": 0.0372,
      "step": 873
    },
    {
      "epoch": 7.808988764044944,
      "grad_norm": 0.39700639247894287,
      "learning_rate": 2.5527638190954772e-05,
      "loss": 0.0337,
      "step": 874
    },
    {
      "epoch": 7.817977528089887,
      "grad_norm": 0.38879045844078064,
      "learning_rate": 2.5326633165829145e-05,
      "loss": 0.0302,
      "step": 875
    },
    {
      "epoch": 7.8269662921348315,
      "grad_norm": 0.3235592544078827,
      "learning_rate": 2.5125628140703518e-05,
      "loss": 0.0282,
      "step": 876
    },
    {
      "epoch": 7.835955056179776,
      "grad_norm": 0.44465523958206177,
      "learning_rate": 2.492462311557789e-05,
      "loss": 0.0341,
      "step": 877
    },
    {
      "epoch": 7.844943820224719,
      "grad_norm": 0.3846980631351471,
      "learning_rate": 2.4723618090452264e-05,
      "loss": 0.0347,
      "step": 878
    },
    {
      "epoch": 7.853932584269663,
      "grad_norm": 0.3724551796913147,
      "learning_rate": 2.4522613065326637e-05,
      "loss": 0.033,
      "step": 879
    },
    {
      "epoch": 7.8629213483146065,
      "grad_norm": 0.38928407430648804,
      "learning_rate": 2.4321608040201007e-05,
      "loss": 0.0355,
      "step": 880
    },
    {
      "epoch": 7.871910112359551,
      "grad_norm": 0.563740074634552,
      "learning_rate": 2.4120603015075376e-05,
      "loss": 0.0413,
      "step": 881
    },
    {
      "epoch": 7.880898876404494,
      "grad_norm": 0.5308793187141418,
      "learning_rate": 2.391959798994975e-05,
      "loss": 0.0426,
      "step": 882
    },
    {
      "epoch": 7.889887640449438,
      "grad_norm": 0.48214098811149597,
      "learning_rate": 2.3718592964824122e-05,
      "loss": 0.0396,
      "step": 883
    },
    {
      "epoch": 7.898876404494382,
      "grad_norm": 0.39925137162208557,
      "learning_rate": 2.351758793969849e-05,
      "loss": 0.032,
      "step": 884
    },
    {
      "epoch": 7.907865168539326,
      "grad_norm": 0.5603592991828918,
      "learning_rate": 2.3316582914572865e-05,
      "loss": 0.0464,
      "step": 885
    },
    {
      "epoch": 7.91685393258427,
      "grad_norm": 0.6382415294647217,
      "learning_rate": 2.3115577889447238e-05,
      "loss": 0.0461,
      "step": 886
    },
    {
      "epoch": 7.925842696629213,
      "grad_norm": 0.3054088056087494,
      "learning_rate": 2.291457286432161e-05,
      "loss": 0.0251,
      "step": 887
    },
    {
      "epoch": 7.9348314606741575,
      "grad_norm": 0.4780307412147522,
      "learning_rate": 2.271356783919598e-05,
      "loss": 0.0406,
      "step": 888
    },
    {
      "epoch": 7.943820224719101,
      "grad_norm": 0.48859450221061707,
      "learning_rate": 2.2512562814070353e-05,
      "loss": 0.0341,
      "step": 889
    },
    {
      "epoch": 7.952808988764045,
      "grad_norm": 0.5126618146896362,
      "learning_rate": 2.2311557788944726e-05,
      "loss": 0.0374,
      "step": 890
    },
    {
      "epoch": 7.961797752808989,
      "grad_norm": 0.34399527311325073,
      "learning_rate": 2.21105527638191e-05,
      "loss": 0.0265,
      "step": 891
    },
    {
      "epoch": 7.9707865168539325,
      "grad_norm": 0.4902538061141968,
      "learning_rate": 2.190954773869347e-05,
      "loss": 0.0392,
      "step": 892
    },
    {
      "epoch": 7.979775280898877,
      "grad_norm": 0.44922730326652527,
      "learning_rate": 2.1708542713567838e-05,
      "loss": 0.0338,
      "step": 893
    },
    {
      "epoch": 7.98876404494382,
      "grad_norm": 0.5790209174156189,
      "learning_rate": 2.150753768844221e-05,
      "loss": 0.0353,
      "step": 894
    },
    {
      "epoch": 7.997752808988764,
      "grad_norm": 0.48645228147506714,
      "learning_rate": 2.1306532663316584e-05,
      "loss": 0.0346,
      "step": 895
    },
    {
      "epoch": 8.0,
      "grad_norm": 0.5648351907730103,
      "learning_rate": 2.1105527638190957e-05,
      "loss": 0.0242,
      "step": 896
    },
    {
      "epoch": 8.008988764044943,
      "grad_norm": 0.2569018006324768,
      "learning_rate": 2.0904522613065327e-05,
      "loss": 0.0254,
      "step": 897
    },
    {
      "epoch": 8.017977528089888,
      "grad_norm": 0.27859511971473694,
      "learning_rate": 2.07035175879397e-05,
      "loss": 0.0288,
      "step": 898
    },
    {
      "epoch": 8.026966292134832,
      "grad_norm": 0.28097033500671387,
      "learning_rate": 2.0502512562814073e-05,
      "loss": 0.0252,
      "step": 899
    },
    {
      "epoch": 8.035955056179775,
      "grad_norm": 0.26388221979141235,
      "learning_rate": 2.0301507537688446e-05,
      "loss": 0.0289,
      "step": 900
    },
    {
      "epoch": 8.044943820224718,
      "grad_norm": 0.25674495100975037,
      "learning_rate": 2.0100502512562815e-05,
      "loss": 0.0253,
      "step": 901
    },
    {
      "epoch": 8.053932584269663,
      "grad_norm": 0.26730048656463623,
      "learning_rate": 1.9899497487437188e-05,
      "loss": 0.0317,
      "step": 902
    },
    {
      "epoch": 8.062921348314607,
      "grad_norm": 0.24476681649684906,
      "learning_rate": 1.9698492462311558e-05,
      "loss": 0.026,
      "step": 903
    },
    {
      "epoch": 8.07191011235955,
      "grad_norm": 0.20951403677463531,
      "learning_rate": 1.949748743718593e-05,
      "loss": 0.0217,
      "step": 904
    },
    {
      "epoch": 8.080898876404495,
      "grad_norm": 0.3002471625804901,
      "learning_rate": 1.92964824120603e-05,
      "loss": 0.0297,
      "step": 905
    },
    {
      "epoch": 8.089887640449438,
      "grad_norm": 0.23913995921611786,
      "learning_rate": 1.9095477386934673e-05,
      "loss": 0.0242,
      "step": 906
    },
    {
      "epoch": 8.098876404494382,
      "grad_norm": 0.2553001642227173,
      "learning_rate": 1.8894472361809046e-05,
      "loss": 0.0261,
      "step": 907
    },
    {
      "epoch": 8.107865168539325,
      "grad_norm": 0.25240248441696167,
      "learning_rate": 1.869346733668342e-05,
      "loss": 0.0242,
      "step": 908
    },
    {
      "epoch": 8.11685393258427,
      "grad_norm": 0.2572178840637207,
      "learning_rate": 1.849246231155779e-05,
      "loss": 0.0243,
      "step": 909
    },
    {
      "epoch": 8.125842696629213,
      "grad_norm": 0.28954601287841797,
      "learning_rate": 1.829145728643216e-05,
      "loss": 0.0293,
      "step": 910
    },
    {
      "epoch": 8.134831460674157,
      "grad_norm": 0.29863840341567993,
      "learning_rate": 1.8090452261306535e-05,
      "loss": 0.0245,
      "step": 911
    },
    {
      "epoch": 8.143820224719102,
      "grad_norm": 0.20750682055950165,
      "learning_rate": 1.7889447236180908e-05,
      "loss": 0.025,
      "step": 912
    },
    {
      "epoch": 8.152808988764045,
      "grad_norm": 0.2757623493671417,
      "learning_rate": 1.7688442211055277e-05,
      "loss": 0.0253,
      "step": 913
    },
    {
      "epoch": 8.161797752808988,
      "grad_norm": 0.22390983998775482,
      "learning_rate": 1.748743718592965e-05,
      "loss": 0.0234,
      "step": 914
    },
    {
      "epoch": 8.170786516853932,
      "grad_norm": 0.2778894305229187,
      "learning_rate": 1.728643216080402e-05,
      "loss": 0.0257,
      "step": 915
    },
    {
      "epoch": 8.179775280898877,
      "grad_norm": 0.2487226277589798,
      "learning_rate": 1.7085427135678393e-05,
      "loss": 0.0236,
      "step": 916
    },
    {
      "epoch": 8.18876404494382,
      "grad_norm": 0.23553849756717682,
      "learning_rate": 1.6884422110552762e-05,
      "loss": 0.0222,
      "step": 917
    },
    {
      "epoch": 8.197752808988763,
      "grad_norm": 0.2802077531814575,
      "learning_rate": 1.6683417085427135e-05,
      "loss": 0.0255,
      "step": 918
    },
    {
      "epoch": 8.206741573033709,
      "grad_norm": 0.2677505314350128,
      "learning_rate": 1.6482412060301508e-05,
      "loss": 0.0254,
      "step": 919
    },
    {
      "epoch": 8.215730337078652,
      "grad_norm": 0.36222150921821594,
      "learning_rate": 1.628140703517588e-05,
      "loss": 0.0353,
      "step": 920
    },
    {
      "epoch": 8.224719101123595,
      "grad_norm": 0.33424293994903564,
      "learning_rate": 1.608040201005025e-05,
      "loss": 0.0255,
      "step": 921
    },
    {
      "epoch": 8.233707865168538,
      "grad_norm": 0.29658955335617065,
      "learning_rate": 1.5879396984924624e-05,
      "loss": 0.0232,
      "step": 922
    },
    {
      "epoch": 8.242696629213484,
      "grad_norm": 0.29968559741973877,
      "learning_rate": 1.5678391959798997e-05,
      "loss": 0.0263,
      "step": 923
    },
    {
      "epoch": 8.251685393258427,
      "grad_norm": 0.2441900372505188,
      "learning_rate": 1.547738693467337e-05,
      "loss": 0.0231,
      "step": 924
    },
    {
      "epoch": 8.26067415730337,
      "grad_norm": 0.30824777483940125,
      "learning_rate": 1.527638190954774e-05,
      "loss": 0.0308,
      "step": 925
    },
    {
      "epoch": 8.269662921348315,
      "grad_norm": 0.3216802179813385,
      "learning_rate": 1.507537688442211e-05,
      "loss": 0.0267,
      "step": 926
    },
    {
      "epoch": 8.278651685393259,
      "grad_norm": 0.3137473464012146,
      "learning_rate": 1.4874371859296482e-05,
      "loss": 0.0292,
      "step": 927
    },
    {
      "epoch": 8.287640449438202,
      "grad_norm": 0.2934947609901428,
      "learning_rate": 1.4673366834170855e-05,
      "loss": 0.028,
      "step": 928
    },
    {
      "epoch": 8.296629213483147,
      "grad_norm": 0.30527976155281067,
      "learning_rate": 1.4472361809045226e-05,
      "loss": 0.0264,
      "step": 929
    },
    {
      "epoch": 8.30561797752809,
      "grad_norm": 0.2557978928089142,
      "learning_rate": 1.4271356783919599e-05,
      "loss": 0.0268,
      "step": 930
    },
    {
      "epoch": 8.314606741573034,
      "grad_norm": 0.2618633210659027,
      "learning_rate": 1.407035175879397e-05,
      "loss": 0.0242,
      "step": 931
    },
    {
      "epoch": 8.323595505617977,
      "grad_norm": 0.4078367054462433,
      "learning_rate": 1.3869346733668343e-05,
      "loss": 0.0315,
      "step": 932
    },
    {
      "epoch": 8.332584269662922,
      "grad_norm": 0.3025127649307251,
      "learning_rate": 1.3668341708542715e-05,
      "loss": 0.0242,
      "step": 933
    },
    {
      "epoch": 8.341573033707865,
      "grad_norm": 0.2373831421136856,
      "learning_rate": 1.3467336683417087e-05,
      "loss": 0.0233,
      "step": 934
    },
    {
      "epoch": 8.350561797752809,
      "grad_norm": 0.27437591552734375,
      "learning_rate": 1.3266331658291459e-05,
      "loss": 0.0246,
      "step": 935
    },
    {
      "epoch": 8.359550561797754,
      "grad_norm": 0.2875991463661194,
      "learning_rate": 1.306532663316583e-05,
      "loss": 0.0233,
      "step": 936
    },
    {
      "epoch": 8.368539325842697,
      "grad_norm": 0.29281115531921387,
      "learning_rate": 1.2864321608040203e-05,
      "loss": 0.023,
      "step": 937
    },
    {
      "epoch": 8.37752808988764,
      "grad_norm": 0.27323636412620544,
      "learning_rate": 1.2663316582914573e-05,
      "loss": 0.0254,
      "step": 938
    },
    {
      "epoch": 8.386516853932584,
      "grad_norm": 0.3130357265472412,
      "learning_rate": 1.2462311557788946e-05,
      "loss": 0.0264,
      "step": 939
    },
    {
      "epoch": 8.395505617977529,
      "grad_norm": 0.21589432656764984,
      "learning_rate": 1.2261306532663318e-05,
      "loss": 0.0223,
      "step": 940
    },
    {
      "epoch": 8.404494382022472,
      "grad_norm": 0.2719046175479889,
      "learning_rate": 1.2060301507537688e-05,
      "loss": 0.0243,
      "step": 941
    },
    {
      "epoch": 8.413483146067415,
      "grad_norm": 0.2773081958293915,
      "learning_rate": 1.1859296482412061e-05,
      "loss": 0.0253,
      "step": 942
    },
    {
      "epoch": 8.42247191011236,
      "grad_norm": 0.2797165513038635,
      "learning_rate": 1.1658291457286432e-05,
      "loss": 0.0256,
      "step": 943
    },
    {
      "epoch": 8.431460674157304,
      "grad_norm": 0.2581692636013031,
      "learning_rate": 1.1457286432160805e-05,
      "loss": 0.0259,
      "step": 944
    },
    {
      "epoch": 8.440449438202247,
      "grad_norm": 0.4019531011581421,
      "learning_rate": 1.1256281407035177e-05,
      "loss": 0.03,
      "step": 945
    },
    {
      "epoch": 8.44943820224719,
      "grad_norm": 0.25484925508499146,
      "learning_rate": 1.105527638190955e-05,
      "loss": 0.0225,
      "step": 946
    },
    {
      "epoch": 8.458426966292135,
      "grad_norm": 0.357642263174057,
      "learning_rate": 1.0854271356783919e-05,
      "loss": 0.0255,
      "step": 947
    },
    {
      "epoch": 8.467415730337079,
      "grad_norm": 0.2824607193470001,
      "learning_rate": 1.0653266331658292e-05,
      "loss": 0.0256,
      "step": 948
    },
    {
      "epoch": 8.476404494382022,
      "grad_norm": 0.2888379991054535,
      "learning_rate": 1.0452261306532663e-05,
      "loss": 0.0248,
      "step": 949
    },
    {
      "epoch": 8.485393258426967,
      "grad_norm": 0.2985410988330841,
      "learning_rate": 1.0251256281407036e-05,
      "loss": 0.027,
      "step": 950
    },
    {
      "epoch": 8.49438202247191,
      "grad_norm": 0.35076114535331726,
      "learning_rate": 1.0050251256281408e-05,
      "loss": 0.027,
      "step": 951
    },
    {
      "epoch": 8.503370786516854,
      "grad_norm": 0.18446283042430878,
      "learning_rate": 9.849246231155779e-06,
      "loss": 0.0199,
      "step": 952
    },
    {
      "epoch": 8.512359550561797,
      "grad_norm": 0.47584813833236694,
      "learning_rate": 9.64824120603015e-06,
      "loss": 0.0263,
      "step": 953
    },
    {
      "epoch": 8.521348314606742,
      "grad_norm": 0.28875237703323364,
      "learning_rate": 9.447236180904523e-06,
      "loss": 0.0251,
      "step": 954
    },
    {
      "epoch": 8.530337078651685,
      "grad_norm": 0.2864939272403717,
      "learning_rate": 9.246231155778894e-06,
      "loss": 0.0249,
      "step": 955
    },
    {
      "epoch": 8.539325842696629,
      "grad_norm": 0.30155110359191895,
      "learning_rate": 9.045226130653267e-06,
      "loss": 0.0263,
      "step": 956
    },
    {
      "epoch": 8.548314606741574,
      "grad_norm": 0.2627485692501068,
      "learning_rate": 8.844221105527639e-06,
      "loss": 0.0267,
      "step": 957
    },
    {
      "epoch": 8.557303370786517,
      "grad_norm": 0.2235344797372818,
      "learning_rate": 8.64321608040201e-06,
      "loss": 0.0253,
      "step": 958
    },
    {
      "epoch": 8.56629213483146,
      "grad_norm": 0.26406291127204895,
      "learning_rate": 8.442211055276381e-06,
      "loss": 0.0258,
      "step": 959
    },
    {
      "epoch": 8.575280898876404,
      "grad_norm": 0.23671191930770874,
      "learning_rate": 8.241206030150754e-06,
      "loss": 0.0243,
      "step": 960
    },
    {
      "epoch": 8.584269662921349,
      "grad_norm": 0.2689703106880188,
      "learning_rate": 8.040201005025125e-06,
      "loss": 0.0255,
      "step": 961
    },
    {
      "epoch": 8.593258426966292,
      "grad_norm": 0.30311158299446106,
      "learning_rate": 7.839195979899498e-06,
      "loss": 0.0271,
      "step": 962
    },
    {
      "epoch": 8.602247191011235,
      "grad_norm": 0.2869846820831299,
      "learning_rate": 7.63819095477387e-06,
      "loss": 0.0267,
      "step": 963
    },
    {
      "epoch": 8.61123595505618,
      "grad_norm": 0.3266974687576294,
      "learning_rate": 7.437185929648241e-06,
      "loss": 0.0285,
      "step": 964
    },
    {
      "epoch": 8.620224719101124,
      "grad_norm": 0.2076135277748108,
      "learning_rate": 7.236180904522613e-06,
      "loss": 0.0237,
      "step": 965
    },
    {
      "epoch": 8.629213483146067,
      "grad_norm": 0.270469605922699,
      "learning_rate": 7.035175879396985e-06,
      "loss": 0.0241,
      "step": 966
    },
    {
      "epoch": 8.63820224719101,
      "grad_norm": 0.2704058289527893,
      "learning_rate": 6.834170854271357e-06,
      "loss": 0.0275,
      "step": 967
    },
    {
      "epoch": 8.647191011235956,
      "grad_norm": 0.21484375,
      "learning_rate": 6.633165829145729e-06,
      "loss": 0.0195,
      "step": 968
    },
    {
      "epoch": 8.656179775280899,
      "grad_norm": 0.2830042541027069,
      "learning_rate": 6.4321608040201015e-06,
      "loss": 0.0258,
      "step": 969
    },
    {
      "epoch": 8.665168539325842,
      "grad_norm": 0.26789236068725586,
      "learning_rate": 6.231155778894473e-06,
      "loss": 0.0264,
      "step": 970
    },
    {
      "epoch": 8.674157303370787,
      "grad_norm": 0.34919533133506775,
      "learning_rate": 6.030150753768844e-06,
      "loss": 0.028,
      "step": 971
    },
    {
      "epoch": 8.68314606741573,
      "grad_norm": 0.20106878876686096,
      "learning_rate": 5.829145728643216e-06,
      "loss": 0.0203,
      "step": 972
    },
    {
      "epoch": 8.692134831460674,
      "grad_norm": 0.27339380979537964,
      "learning_rate": 5.628140703517588e-06,
      "loss": 0.0253,
      "step": 973
    },
    {
      "epoch": 8.701123595505617,
      "grad_norm": 0.2601727545261383,
      "learning_rate": 5.4271356783919595e-06,
      "loss": 0.0251,
      "step": 974
    },
    {
      "epoch": 8.710112359550562,
      "grad_norm": 0.33203473687171936,
      "learning_rate": 5.226130653266332e-06,
      "loss": 0.0256,
      "step": 975
    },
    {
      "epoch": 8.719101123595506,
      "grad_norm": 0.2857777774333954,
      "learning_rate": 5.025125628140704e-06,
      "loss": 0.0254,
      "step": 976
    },
    {
      "epoch": 8.728089887640449,
      "grad_norm": 0.2991282045841217,
      "learning_rate": 4.824120603015075e-06,
      "loss": 0.0278,
      "step": 977
    },
    {
      "epoch": 8.737078651685394,
      "grad_norm": 0.29151660203933716,
      "learning_rate": 4.623115577889447e-06,
      "loss": 0.0277,
      "step": 978
    },
    {
      "epoch": 8.746067415730337,
      "grad_norm": 0.30010709166526794,
      "learning_rate": 4.422110552763819e-06,
      "loss": 0.0261,
      "step": 979
    },
    {
      "epoch": 8.75505617977528,
      "grad_norm": 0.24854226410388947,
      "learning_rate": 4.2211055276381906e-06,
      "loss": 0.0235,
      "step": 980
    },
    {
      "epoch": 8.764044943820224,
      "grad_norm": 0.2871999144554138,
      "learning_rate": 4.020100502512563e-06,
      "loss": 0.0264,
      "step": 981
    },
    {
      "epoch": 8.773033707865169,
      "grad_norm": 0.2349306344985962,
      "learning_rate": 3.819095477386935e-06,
      "loss": 0.0218,
      "step": 982
    },
    {
      "epoch": 8.782022471910112,
      "grad_norm": 0.24486519396305084,
      "learning_rate": 3.6180904522613065e-06,
      "loss": 0.0206,
      "step": 983
    },
    {
      "epoch": 8.791011235955056,
      "grad_norm": 0.3444826602935791,
      "learning_rate": 3.4170854271356786e-06,
      "loss": 0.0258,
      "step": 984
    },
    {
      "epoch": 8.8,
      "grad_norm": 0.32605108618736267,
      "learning_rate": 3.2160804020100507e-06,
      "loss": 0.0302,
      "step": 985
    },
    {
      "epoch": 8.808988764044944,
      "grad_norm": 0.5258952379226685,
      "learning_rate": 3.015075376884422e-06,
      "loss": 0.0273,
      "step": 986
    },
    {
      "epoch": 8.817977528089887,
      "grad_norm": 0.23077918589115143,
      "learning_rate": 2.814070351758794e-06,
      "loss": 0.0234,
      "step": 987
    },
    {
      "epoch": 8.82696629213483,
      "grad_norm": 0.26049068570137024,
      "learning_rate": 2.613065326633166e-06,
      "loss": 0.0251,
      "step": 988
    },
    {
      "epoch": 8.835955056179776,
      "grad_norm": 0.2814459204673767,
      "learning_rate": 2.4120603015075375e-06,
      "loss": 0.0274,
      "step": 989
    },
    {
      "epoch": 8.844943820224719,
      "grad_norm": 0.28811731934547424,
      "learning_rate": 2.2110552763819096e-06,
      "loss": 0.0278,
      "step": 990
    },
    {
      "epoch": 8.853932584269662,
      "grad_norm": 0.31750252842903137,
      "learning_rate": 2.0100502512562813e-06,
      "loss": 0.0292,
      "step": 991
    },
    {
      "epoch": 8.862921348314607,
      "grad_norm": 0.383592426776886,
      "learning_rate": 1.8090452261306533e-06,
      "loss": 0.0304,
      "step": 992
    },
    {
      "epoch": 8.87191011235955,
      "grad_norm": 0.2771124839782715,
      "learning_rate": 1.6080402010050254e-06,
      "loss": 0.0259,
      "step": 993
    },
    {
      "epoch": 8.880898876404494,
      "grad_norm": 0.2241499423980713,
      "learning_rate": 1.407035175879397e-06,
      "loss": 0.0241,
      "step": 994
    },
    {
      "epoch": 8.889887640449437,
      "grad_norm": 0.23871122300624847,
      "learning_rate": 1.2060301507537688e-06,
      "loss": 0.0207,
      "step": 995
    },
    {
      "epoch": 8.898876404494382,
      "grad_norm": 0.30455490946769714,
      "learning_rate": 1.0050251256281407e-06,
      "loss": 0.0234,
      "step": 996
    },
    {
      "epoch": 8.907865168539326,
      "grad_norm": 0.25007298588752747,
      "learning_rate": 8.040201005025127e-07,
      "loss": 0.0238,
      "step": 997
    },
    {
      "epoch": 8.916853932584269,
      "grad_norm": 0.29162871837615967,
      "learning_rate": 6.030150753768844e-07,
      "loss": 0.0272,
      "step": 998
    },
    {
      "epoch": 8.925842696629214,
      "grad_norm": 0.2700066864490509,
      "learning_rate": 4.0201005025125634e-07,
      "loss": 0.0231,
      "step": 999
    },
    {
      "epoch": 8.934831460674157,
      "grad_norm": 0.33910810947418213,
      "learning_rate": 2.0100502512562817e-07,
      "loss": 0.0308,
      "step": 1000
    }
  ],
  "logging_steps": 1,
  "max_steps": 1000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 9,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 8.4570860005632e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
